{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 尝试层次self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.init as weight_init\n",
    "import numpy as np\n",
    "from time import time\n",
    "import sys\n",
    "import data\n",
    "from metrics import Metrics\n",
    "DEVICE = torch.device('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_HRED():\n",
    "    conf = {\n",
    "    'maxlen':20, # maximum utterance length\n",
    "    'diaglen':10, # how many utterance kept in the context window\n",
    "    'n_head':5,\n",
    "    'n_self':80, # self attention dimention\n",
    "# Model Arguments\n",
    "    'emb_size':200, # size of word embeddings\n",
    "    'n_hidden':300,\n",
    "    'n_hidden_utter_encode':300, # number of hidden units of utterance encoder\n",
    "    'n_hidden_context_encode':300,\n",
    "    'n_hidden_decode':300,\n",
    "    'n_layers':1, # number of layers\n",
    "    'noise_radius':0.2, # stdev of noise for autoencoder (regularizer)\n",
    "    'lambda_gp':10, # Gradient penalty lambda hyperparameter.\n",
    "    'temp':1.0, # softmax temperature (lower --> more discrete)\n",
    "    'dropout':0.5, # dropout applied to layers (0 = no dropout)\n",
    "\n",
    "# Training Arguments\n",
    "    'batch_size':128,\n",
    "    'epochs':100, # maximum number of epochs\n",
    "\n",
    "    'lr':0.001, # autoencoder learning rate\n",
    "    'beta1':0.9, # beta1 for adam\n",
    "    'clip':1.0,  # gradient clipping, max norm\n",
    "    'gan_clamp':0.01,  # WGAN clamp (Do not use clamp when you apply gradient penelty             \n",
    "    }\n",
    "    return conf \n",
    "\n",
    "config = config_HRED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gData(data):\n",
    "    tensor=data\n",
    "    if isinstance(data, np.ndarray):\n",
    "        tensor = torch.from_numpy(data)\n",
    "    tensor=tensor.to(DEVICE)\n",
    "    return tensor\n",
    "def gVar(data):\n",
    "    return gData(data)\n",
    "def print_flush(data, args=None):\n",
    "    if args == None:\n",
    "        print(data)\n",
    "    else:\n",
    "        print(data, args)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def indexes2sent(indexes, vocab, eos_tok, ignore_tok=0): \n",
    "    '''indexes: numpy array'''\n",
    "    def revert_sent(indexes, ivocab, eos_tok, ignore_tok=0):\n",
    "        toks=[]\n",
    "        length=0\n",
    "        indexes=filter(lambda i: i!=ignore_tok, indexes)\n",
    "        for idx in indexes:\n",
    "            toks.append(ivocab[idx])\n",
    "            length+=1\n",
    "            if idx == eos_tok:\n",
    "                break\n",
    "        return ' '.join(toks), length\n",
    "    \n",
    "    ivocab = {v: k for k, v in vocab.items()}\n",
    "    if indexes.ndim==1:# one sentence\n",
    "        return revert_sent(indexes, ivocab, eos_tok, ignore_tok)\n",
    "    else:# dim>1\n",
    "        sentences=[] # a batch of sentences\n",
    "        lens=[]\n",
    "        for inds in indexes:\n",
    "            sentence, length = revert_sent(inds, ivocab, eos_tok, ignore_tok)\n",
    "            sentences.append(sentence)\n",
    "            lens.append(length)\n",
    "        return sentences, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        attn = torch.bmm(q, k.transpose(1, 2))\n",
    "        attn = attn / self.temperature\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -np.inf)\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v)\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5))\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model)\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "\n",
    "        sz_b, len_q, _ = q.size()\n",
    "        sz_b, len_k, _ = k.size()\n",
    "        sz_b, len_v, _ = v.size()\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k) # (n*b) x lq x dk\n",
    "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k) # (n*b) x lk x dk\n",
    "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v) # (n*b) x lv x dv\n",
    "\n",
    "#         mask = mask.repeat(n_head, 1, 1) # (n*b) x .. x ..\n",
    "        output, attn = self.attention(q, k, v)\n",
    "\n",
    "        output = output.view(n_head, sz_b, len_q, d_v)\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1) # b x lq x (n*dv)\n",
    "\n",
    "        output = self.dropout(self.fc(output))\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedder, input_size, hidden_size, bidirectional, n_layers, noise_radius=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.noise_radius=noise_radius\n",
    "        self.n_layers = n_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        assert type(self.bidirectional)==bool\n",
    "        self.embedding = embedder\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, n_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for w in self.rnn.parameters(): \n",
    "            if w.dim()>1:\n",
    "                weight_init.orthogonal_(w)\n",
    "                \n",
    "    def store_grad_norm(self, grad):\n",
    "        norm = torch.norm(grad, 2, 1)\n",
    "        self.grad_norm = norm.detach().data.mean()\n",
    "        return grad\n",
    "    \n",
    "    def forward(self, inputs, input_lens=None, noise=False): \n",
    "        if self.embedding is not None:\n",
    "            inputs=self.embedding(inputs) \n",
    "        \n",
    "        batch_size, seq_len, emb_size=inputs.size()\n",
    "        inputs=F.dropout(inputs, 0.5, self.training)\n",
    "        \n",
    "        if input_lens is not None:\n",
    "            input_lens_sorted, indices = input_lens.sort(descending=True)\n",
    "            inputs_sorted = inputs.index_select(0, indices)        \n",
    "            inputs = pack_padded_sequence(inputs_sorted, input_lens_sorted.data.tolist(), batch_first=True)\n",
    "            \n",
    "        init_hidden = gVar(torch.zeros(self.n_layers*(1+self.bidirectional), batch_size, self.hidden_size))\n",
    "        hids, h_n = self.rnn(inputs, init_hidden) \n",
    "        if input_lens is not None:\n",
    "            _, inv_indices = indices.sort()\n",
    "            hids, lens = pad_packed_sequence(hids, batch_first=True)     \n",
    "            hids = hids.index_select(0, inv_indices)\n",
    "            h_n = h_n.index_select(1, inv_indices)\n",
    "        h_n = h_n.view(self.n_layers, (1+self.bidirectional), batch_size, self.hidden_size)\n",
    "        h_n = h_n[-1]\n",
    "        enc = h_n.transpose(1,0).contiguous().view(batch_size,-1) \n",
    "        hids = hids.transpose(1,0).contiguous().view(batch_size,-1) \n",
    "\n",
    "        if noise and self.noise_radius > 0:\n",
    "            gauss_noise = gVar(torch.normal(means=torch.zeros(enc.size()),std=self.noise_radius))\n",
    "            enc = enc + gauss_noise\n",
    "        return enc, hids\n",
    "    \n",
    "class ContextEncoder(nn.Module):\n",
    "    def __init__(self, utt_encoder, input_size, hidden_size, n_head, n_self,\n",
    "                 n_layers=1, noise_radius=0.2):\n",
    "        super(ContextEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.noise_radius=noise_radius\n",
    "        self.n_layers = n_layers\n",
    "        self.self_attention = MultiHeadAttention(n_head, hidden_size, n_self, n_self)\n",
    "        self.utt_encoder=utt_encoder\n",
    "#         self.linear = nn.Linear(input_size, hidden_size)\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for w in self.rnn.parameters(): # initialize the gate weights with orthogonal\n",
    "            if w.dim()>1:\n",
    "                weight_init.orthogonal_(w)\n",
    "    \n",
    "    def store_grad_norm(self, grad):\n",
    "        norm = torch.norm(grad, 2, 1)\n",
    "        self.grad_norm = norm.detach().data.mean()\n",
    "        return grad\n",
    "\n",
    "    def forward(self, context, context_lens, utt_lens, floors, noise=False): \n",
    "        batch_size, max_context_len, max_utt_len = context.size()\n",
    "        utts=context.view(-1, max_utt_len)\n",
    "        utt_lens=utt_lens.view(-1)\n",
    "        utt_encs,_ = self.utt_encoder(utts, utt_lens) \n",
    "        utt_encs = utt_encs.view(batch_size, max_context_len, -1)\n",
    "        floor_one_hot = gVar(torch.zeros(floors.numel(), 2))\n",
    "        floor_one_hot.data.scatter_(1, floors.view(-1, 1), 1)\n",
    "        floor_one_hot = floor_one_hot.view(-1, max_context_len, 2)\n",
    "        utt_floor_encs = torch.cat([utt_encs, floor_one_hot], 2)\n",
    "#         utt_floor_encs = self.linear(utt_floor_encs)\n",
    "#         utt_floor_encs=F.dropout(utt_floor_encs, 0.25, self.training)\n",
    "#         final_out = self.linear(maxpool)\n",
    "        context_lens_sorted, indices = context_lens.sort(descending=True)\n",
    "        utt_floor_encs = utt_floor_encs.index_select(0, indices)\n",
    "        utt_floor_encs = pack_padded_sequence(utt_floor_encs, context_lens_sorted.data.tolist(), batch_first=True)\n",
    "        init_hidden = gVar(torch.zeros(1, batch_size, self.hidden_size))\n",
    "        hids, h_n = self.rnn(utt_floor_encs, init_hidden)\n",
    "        hids, lens = pad_packed_sequence(hids, batch_first=True)\n",
    "        _, inv_indices = indices.sort()\n",
    "        hids = hids.index_select(0, inv_indices)\n",
    "#         h_n = h_n.index_select(1, inv_indices)\n",
    "#         enc = h_n.transpose(1,0).contiguous().view(batch_size, -1)\n",
    "        self_attention_outputs = self.self_attention(hids, hids, hids)\n",
    "        maxpool = F.max_pool2d(self_attention_outputs.unsqueeze_(1), (max_context_len, 1)).squeeze()\n",
    "#         if noise and self.noise_radius > 0:\n",
    "#             gauss_noise = gVar(torch.normal(means=torch.zeros(enc.size()),std=self.noise_radius))\n",
    "#             enc = enc + gauss_noise\n",
    "        return maxpool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedder, input_size, hidden_size, vocab_size, n_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.input_size= input_size \n",
    "        self.hidden_size = hidden_size \n",
    "        self.vocab_size = vocab_size \n",
    "        self.embedding = embedder\n",
    "        self.linear = nn.Linear(602, hidden_size)\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        for w in self.rnn.parameters():\n",
    "            if w.dim()>1:\n",
    "                weight_init.orthogonal_(w)\n",
    "        self.out.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out.bias.data.fill_(0)\n",
    "    \n",
    "    def forward(self, init_hidden, context=None, inputs=None, lens=None):\n",
    "        batch_size, maxlen = inputs.size()\n",
    "        if self.embedding is not None:\n",
    "            inputs = self.embedding(inputs)\n",
    "        if context is not None:\n",
    "            repeated_context = context.unsqueeze(1).repeat(1, maxlen, 1)\n",
    "            inputs = torch.cat([inputs, repeated_context], 2)\n",
    "#         inputs = F.dropout(inputs, 0.5, self.training)  ß\n",
    "#         init_hidden = self.linear(init_hidden)\n",
    "        hids, h_n = self.rnn(inputs, init_hidden.unsqueeze(0))        \n",
    "        decoded = self.out(hids.contiguous().view(-1, self.hidden_size))# reshape before linear over vocab\n",
    "        decoded = decoded.view(batch_size, maxlen, self.vocab_size)\n",
    "        return decoded\n",
    "    \n",
    "    def sampling(self, init_hidden, context, maxlen, SOS_tok, EOS_tok, mode='greedy'):\n",
    "        init_hidden.unsqueeze_(0)\n",
    "        batch_size=init_hidden.size(0)\n",
    "#         init_hidden = self.linear(init_hidden)\n",
    "        decoded_words = np.zeros((batch_size, maxlen), dtype=np.int)\n",
    "        sample_lens=np.zeros(batch_size, dtype=np.int)         \n",
    "        \n",
    "        decoder_input = gVar(torch.LongTensor([[SOS_tok]*batch_size]).view(batch_size,1))\n",
    "        decoder_input = self.embedding(decoder_input) if self.embedding is not None else decoder_input \n",
    "        decoder_input = torch.cat([decoder_input, context.unsqueeze(1)],2) if context is not None else decoder_input\n",
    "#         init_hidden = self.linear(init_hidden)\n",
    "        decoder_hidden = init_hidden.unsqueeze(0)\n",
    "        for di in range(maxlen):\n",
    "            decoder_output, decoder_hidden = self.rnn(decoder_input, decoder_hidden)\n",
    "            decoder_output=self.out(decoder_output)\n",
    "            if mode=='greedy':\n",
    "                topi = decoder_output[:,-1].max(1, keepdim=True)[1]\n",
    "            elif mode=='sample':\n",
    "                topi = torch.multinomial(F.softmax(decoder_output[:,-1], dim=1), 1)                    \n",
    "            decoder_input = self.embedding(topi) if self.embedding is not None else topi\n",
    "            decoder_input = torch.cat([decoder_input, context.unsqueeze(1)],2) if context is not None else decoder_input\n",
    "            ni = topi.squeeze().data.cpu().numpy() \n",
    "            decoded_words[:,di]=ni\n",
    "                      \n",
    "        for i in range(batch_size):\n",
    "            for word in decoded_words[i]:\n",
    "                if word == EOS_tok:\n",
    "                    break\n",
    "                sample_lens[i]=sample_lens[i]+1\n",
    "        return decoded_words, sample_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HRED(nn.Module):\n",
    "    def __init__(self, config, vocab_size, PAD_token=0):\n",
    "        super(HRED, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.maxlen=config['maxlen']\n",
    "        self.clip = config['clip']\n",
    "        self.lambda_gp = config['lambda_gp']\n",
    "        self.temp=config['temp']\n",
    "        \n",
    "        self.embedder= nn.Embedding(vocab_size, config['emb_size'], padding_idx=PAD_token)\n",
    "        self.utt_encoder = Encoder(self.embedder, config['emb_size'], config['n_hidden_utter_encode'], \n",
    "                                   True, config['n_layers'], config['noise_radius']) \n",
    "        self.context_encoder = ContextEncoder(self.utt_encoder, config['n_hidden_utter_encode']*2+2, config['n_hidden_context_encode'], \n",
    "                                              config['n_head'], config['n_self'], 1, config['noise_radius']) \n",
    "        self.decoder = Decoder(self.embedder, config['emb_size'], config['n_hidden_decode'], \n",
    "                               vocab_size, n_layers=1)\n",
    "        \n",
    "    def forward(self, context, context_lens, utt_lens, floors, response, res_lens):\n",
    "        c = self.context_encoder(context, context_lens, utt_lens, floors)\n",
    "#         x,_ = self.utt_encoder(response[:,1:], res_lens-1)      \n",
    "        output = self.decoder(c, None, response[:,:-1], (res_lens-1))  \n",
    "        flattened_output = output.view(-1, self.vocab_size) \n",
    "        \n",
    "        dec_target = response[:,1:].contiguous().view(-1)\n",
    "        mask = dec_target.gt(0) # [(batch_sz*seq_len)]\n",
    "        masked_target = dec_target.masked_select(mask) # \n",
    "        output_mask = mask.unsqueeze(1).expand(mask.size(0), self.vocab_size)# [(batch_sz*seq_len) x n_tokens]\n",
    "        masked_output = flattened_output.masked_select(output_mask).view(-1, self.vocab_size)\n",
    "\n",
    "        return masked_target, masked_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max utt len 296, mean utt len 16.48\n",
      "Max utt len 174, mean utt len 16.37\n",
      "Max utt len 214, mean utt len 16.68\n",
      "Load corpus with train size 2, valid size 2, test size 2 raw vocab size 17716 vocab size 10000 at cut_off 2 OOV rate 0.006757\n",
      "<d> index 21\n",
      "<sil> index -1\n",
      "word2vec cannot cover 0.032194 vocab\n",
      "Done loading corpus\n",
      "Max len 36 and min len 3 and avg len 8.840439\n",
      "Max len 32 and min len 3 and avg len 9.069000\n",
      "Max len 27 and min len 3 and avg len 8.740000\n"
     ]
    }
   ],
   "source": [
    "corpus = getattr(data, 'DailyDial'+'Corpus')('../datasets/DailyDial/', wordvec_path='../datasets/'+'glove.twitter.27B.200d.txt', wordvec_dim=config['emb_size'])\n",
    "dials = corpus.get_dialogs()\n",
    "metas = corpus.get_metas()\n",
    "vocab = corpus.ivocab\n",
    "ivocab = corpus.vocab\n",
    "n_tokens = len(ivocab)\n",
    "train_dial, valid_dial, test_dial = dials.get(\"train\"), dials.get(\"valid\"), dials.get(\"test\")\n",
    "train_meta, valid_meta, test_meta = metas.get(\"train\"), metas.get(\"valid\"), metas.get(\"test\")\n",
    "train_loader = getattr(data, 'DailyDial'+'DataLoader')(\"Train\", train_dial, train_meta, config['maxlen'])\n",
    "valid_loader = getattr(data, 'DailyDial'+'DataLoader')(\"Valid\", valid_dial, valid_meta, config['maxlen'])\n",
    "test_loader = getattr(data, 'DailyDial'+'DataLoader')(\"Test\", test_dial, test_meta, config['maxlen'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(context, context_lens, utt_lens, floors, response, res_lens):\n",
    "    model.train()\n",
    "    target, outputs = model(context, context_lens, utt_lens, floors, response, res_lens)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, target)\n",
    "    batch_loss = loss.item()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return batch_loss\n",
    "        \n",
    "def evaluate_batch(context, context_lens, utt_lens, floors, response, res_lens):\n",
    "    model.eval()\n",
    "    target, outputs = model(context, context_lens, utt_lens, floors, response, res_lens)\n",
    "    loss = criterion(outputs, target)\n",
    "    return loss.item()    \n",
    "\n",
    "def valid(valid_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            batch = valid_loader.next_batch()\n",
    "            if batch is None: # end of epoch\n",
    "                break\n",
    "            context, context_lens, utt_lens, floors,_,_,_,response,res_lens,_ = batch\n",
    "            context, utt_lens = context[:,:,1:], utt_lens-1 # remove the sos token in the context and reduce the context length\n",
    "            context, context_lens, utt_lens, floors, response, res_lens\\\n",
    "                    = gVar(context), gVar(context_lens), gVar(utt_lens), gData(floors), gVar(response), gVar(res_lens)\n",
    "            target, outputs = model(context, context_lens, utt_lens, floors, response, res_lens)\n",
    "            loss_batch = criterion(outputs, target)\n",
    "            total_loss += float(loss_batch.item())\n",
    "        return total_loss / valid_loader.num_batch\n",
    "\n",
    "def sample(context, context_lens, utt_lens, floors, repeat, SOS_tok, EOS_tok):    \n",
    "    model.eval()\n",
    "    c = model.context_encoder(context, context_lens, utt_lens, floors)\n",
    "    c_repeated = c.expand(repeat, -1)\n",
    "#     prior_z = self.sample_code_prior(c_repeated)    \n",
    "    sample_words, sample_lens= model.decoder.sampling(c_repeated, \n",
    "                                                     None, config['maxlen'], SOS_tok, EOS_tok, \"greedy\") \n",
    "    return sample_words, sample_lens \n",
    "    \n",
    "    \n",
    "def evaluate(model, metrics, test_loader, vocab, ivocab, repeat):\n",
    "    \n",
    "    recall_bleus, prec_bleus, bows_extrema, bows_avg, bows_greedy, intra_dist1s, intra_dist2s, avg_lens, inter_dist1s, inter_dist2s\\\n",
    "        = [], [], [], [], [], [], [], [], [], []\n",
    "    local_t = 0\n",
    "    while True:\n",
    "        batch = test_loader.next_batch()\n",
    "        if batch is None:\n",
    "            break\n",
    "        local_t += 1 \n",
    "        context, context_lens, utt_lens, floors,_,_,_,response,res_lens,_ = batch   \n",
    "        context, utt_lens = context[:,:,1:], utt_lens-1 # remove the sos token in the context and reduce the context length\n",
    "#         f_eval.write(\"Batch %d \\n\" % (local_t))# print the context\n",
    "        start = np.maximum(0, context_lens[0]-5)\n",
    "        for t_id in range(start, context.shape[1], 1):\n",
    "            context_str = indexes2sent(context[0, t_id], vocab, vocab[\"</s>\"], 0)\n",
    "#             f_eval.write(\"Context %d-%d: %s\\n\" % (t_id, floors[0, t_id], context_str))\n",
    "        # print the true outputs    \n",
    "        ref_str, _ = indexes2sent(response[0], vocab, vocab[\"</s>\"], vocab[\"<s>\"])\n",
    "        ref_tokens = ref_str.split(' ')\n",
    "#         f_eval.write(\"Target >> %s\\n\" % (ref_str.replace(\" ' \", \"'\")))\n",
    "        \n",
    "        context, context_lens, utt_lens, floors = gVar(context), gVar(context_lens), gVar(utt_lens), gData(floors)\n",
    "        sample_words, sample_lens = sample(context, context_lens, utt_lens, floors, repeat, vocab[\"<s>\"], vocab[\"</s>\"])\n",
    "        # nparray: [repeat x seq_len]\n",
    "        pred_sents, _ = indexes2sent(sample_words, vocab, vocab[\"</s>\"], 0)\n",
    "        pred_tokens = [sent.split(' ') for sent in pred_sents]\n",
    "#         for r_id, pred_sent in enumerate(pred_sents):\n",
    "#             f_eval.write(\"Sample %d >> %s\\n\" % (r_id, pred_sent.replace(\" ' \", \"'\")))\n",
    "        max_bleu, avg_bleu = metrics.sim_bleu(pred_tokens, ref_tokens)\n",
    "        recall_bleus.append(max_bleu)\n",
    "        prec_bleus.append(avg_bleu)\n",
    "        \n",
    "        bow_extrema, bow_avg, bow_greedy = metrics.sim_bow(sample_words, sample_lens, response[:,1:], res_lens-2)\n",
    "        bows_extrema.append(bow_extrema)\n",
    "        bows_avg.append(bow_avg)\n",
    "        bows_greedy.append(bow_greedy)\n",
    "        \n",
    "        intra_dist1, intra_dist2, inter_dist1, inter_dist2 = metrics.div_distinct(sample_words, sample_lens)\n",
    "        intra_dist1s.append(intra_dist1)\n",
    "        intra_dist2s.append(intra_dist2)\n",
    "        avg_lens.append(np.mean(sample_lens))\n",
    "        inter_dist1s.append(inter_dist1)\n",
    "        inter_dist2s.append(inter_dist2)\n",
    "        break\n",
    "    recall_bleu = float(np.mean(recall_bleus))\n",
    "    prec_bleu = float(np.mean(prec_bleus))\n",
    "    f1 = 2*(prec_bleu*recall_bleu) / (prec_bleu+recall_bleu+10e-12)\n",
    "    bow_extrema = float(np.mean(bows_extrema))\n",
    "    bow_avg = float(np.mean(bows_avg))\n",
    "    bow_greedy=float(np.mean(bows_greedy))\n",
    "    intra_dist1=float(np.mean(intra_dist1s))\n",
    "    intra_dist2=float(np.mean(intra_dist2s))\n",
    "    avg_len=float(np.mean(avg_lens))\n",
    "    inter_dist1=float(np.mean(inter_dist1s))\n",
    "    inter_dist2=float(np.mean(inter_dist2s))\n",
    "    report = \"Avg recall BLEU %f, avg precision BLEU %f, F1 %f\\n, bow_extrema %f, bow_avg %f, bow_greedy %f\\n,\\\n",
    "    intra_dist1 %f, intra_dist2 %f, avg_len %f, inter_dist1 %f, inter_dist2 %f\"% (recall_bleu, prec_bleu, f1, bow_extrema, \n",
    "                             bow_avg, bow_greedy, intra_dist1, intra_dist2, avg_len, inter_dist1, inter_dist2)\n",
    "    print(report)\n",
    "    print(' time: %.1f s'%(time()-epoch_begin))\n",
    "#     f_eval.write(report + \"\\n\")\n",
    "    print(\"Done testing\")\n",
    "    \n",
    "    return recall_bleu, prec_bleu, bow_extrema, bow_avg, bow_greedy, intra_dist1, intra_dist2, avg_len, inter_dist1, inter_dist2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word2vec\n"
     ]
    }
   ],
   "source": [
    "metrics=Metrics(corpus.word2vec)\n",
    "model = HRED(config, n_tokens)\n",
    "if corpus.word2vec is not None:\n",
    "    print(\"Loaded word2vec\")\n",
    "    model.embedder.weight.data.copy_(torch.from_numpy(corpus.word2vec))\n",
    "    model.embedder.weight.data[0].fill_(0)\n",
    "model.to(DEVICE)\n",
    "parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(parameters, lr=config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Train begins with 609 batches with 110 left over samples\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ContextEncoder' object has no attribute 'rnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-22658085d384>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutt_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_lens\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;34m=\u001b[0m \u001b[0mgVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutt_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mbatch_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-ac471a72b55d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(context, context_lens, utt_lens, floors, response, res_lens)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutt_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutt_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/liangjiahui/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-9b51590b5d4b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, context, context_lens, utt_lens, floors, response, res_lens)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutt_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutt_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#         x,_ = self.utt_encoder(response[:,1:], res_lens-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mres_lens\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/liangjiahui/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-df5071be9509>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, context, context_lens, utt_lens, floors, noise)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mutt_floor_encs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt_floor_encs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_lens_sorted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0minit_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mhids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt_floor_encs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mhids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m#         _, inv_indices = indices.sort()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/liangjiahui/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    533\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 535\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ContextEncoder' object has no attribute 'rnn'"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "print_every = 100\n",
    "best_state = None\n",
    "max_metric = 0\n",
    "for epoch in range(15):\n",
    "    print('Epoch: ', epoch+1)\n",
    "    train_loader.epoch_init(128, config['diaglen'], 1, shuffle=True)\n",
    "    n_iters=train_loader.num_batch\n",
    "    total_loss = 0.0\n",
    "    epoch_begin = time()\n",
    "    batch_count = 0\n",
    "    batch_begin_time = time()\n",
    "    while True:\n",
    "        batch = train_loader.next_batch()\n",
    "        if batch is None: # end of epoch\n",
    "            break\n",
    "        context, context_lens, utt_lens, floors,_,_,_,response,res_lens,_ = batch\n",
    "        context, utt_lens = context[:,:,1:], utt_lens-1 # remove the sos token in the context and reduce the context length\n",
    "        context, context_lens, utt_lens, floors, response, res_lens\\\n",
    "                = gVar(context), gVar(context_lens), gVar(utt_lens), gData(floors), gVar(response), gVar(res_lens)\n",
    "        loss_batch = train(context, context_lens, utt_lens, floors, response, res_lens)\n",
    "        total_loss += float(loss_batch)\n",
    "        batch_count += 1\n",
    "        if batch_count % print_every == 0:\n",
    "            print_flush('[%d %d] loss: %.6f time: %.1f s' %\n",
    "                  (epoch + 1, batch_count, np.exp(total_loss / print_every), time() - batch_begin_time))\n",
    "            total_loss = 0.0\n",
    "            batch_begin_time = time()\n",
    "#     scheduler.step()\n",
    "    print_flush(\"Evaluating....\")\n",
    "#     valid_loader.epoch_init(32, config['diaglen'], 1, shuffle=False)\n",
    "#     loss_valid = valid(valid_loader)\n",
    "#     valid_result.append(F1)\n",
    "#     print_flush('*'*60)\n",
    "#     print_flush('[epoch %d]. loss: %.6f time: %.1f s'%(epoch+1, np.exp(loss_valid), time()-epoch_begin))\n",
    "#     print_flush('*'*60)\n",
    "    print_flush(\"testing....\")\n",
    "    test_loader.epoch_init(1, config['diaglen'], 1, shuffle=False)\n",
    "#     loss_valid = valid(test_loader)\n",
    "#     print_flush('*'*60)\n",
    "#     print_flush('[epoch %d]. loss: %.6f time: %.1f s'%(epoch+1, np.exp(loss_valid), time()-epoch_begin))\n",
    "#     print_flush('*'*60)\n",
    "    recall_bleu, prec_bleu, bow_extrema, bow_avg, bow_greedy, intra_dist1, intra_dist2, avg_len, inter_dist1, inter_dist2\\\n",
    "     =evaluate(model, metrics, test_loader, vocab, ivocab, repeat=10)\n",
    "    epoch_begin = time()\n",
    "#     if F1 > max_metric:\n",
    "#         best_state = model.state_dict()\n",
    "#         max_metric = F1\n",
    "#         print_flush(\"save model...\")\n",
    "#         torch.save(best_state, '../datasets/models/baseline_LSTM.pth')\n",
    "#     epoch_begin = time()\n",
    "#     if training_termination(valid_result):\n",
    "#         print_flush(\"early stop at [%d] epoch!\" % (epoch+1))\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与dialog_doublegan 采用相同的数据集大小\n",
    "def valid_small(valid_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_valid_batch = 0\n",
    "    valid_count = 0\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            batch = valid_loader.next_batch()\n",
    "            if batch is None or total_valid_batch >= 1500: # end of epoch\n",
    "                break\n",
    "            total_valid_batch += 20\n",
    "            valid_count += 1\n",
    "            context, context_lens, utt_lens, floors,_,_,_,response,res_lens,_ = batch\n",
    "            context, utt_lens = context[:,:,1:], utt_lens-1 # remove the sos token in the context and reduce the context length\n",
    "            context, context_lens, utt_lens, floors, response, res_lens\\\n",
    "                    = gVar(context), gVar(context_lens), gVar(utt_lens), gData(floors), gVar(response), gVar(res_lens)\n",
    "            target, outputs = model(context, context_lens, utt_lens, floors, response, res_lens)\n",
    "            loss_batch = criterion(outputs, target)\n",
    "            total_loss += float(loss_batch.item())\n",
    "        return total_loss / valid_count    \n",
    "    \n",
    "def sample(context, context_lens, utt_lens, floors, repeat, SOS_tok, EOS_tok):    \n",
    "    model.eval()\n",
    "    c = model.context_encoder(context, context_lens, utt_lens, floors)\n",
    "#     c_repeated = c.expand(repeat, -1)\n",
    "    sample_words, sample_lens= model.decoder.sampling(c, None, config['maxlen'], SOS_tok, EOS_tok, \"greedy\")\n",
    "    return sample_words, sample_lens \n",
    "\n",
    "def evaluate(model, metrics, test_loader, vocab, ivocab, repeat):\n",
    "    recall_bleus, prec_bleus, bows_extrema, bows_avg, bows_greedy, intra_dist1s, intra_dist2s, avg_lens, inter_dist1s, inter_dist2s\\\n",
    "        = [], [], [], [], [], [], [], [], [], []\n",
    "    local_t = 0\n",
    "    test_loader.epoch_init(1, config['diaglen'], 1, shuffle=False)\n",
    "    valid_count = 0\n",
    "    begin_time = time()\n",
    "    all_generated_sentences = []\n",
    "    all_generated_lens = []\n",
    "    while True:\n",
    "        batch = test_loader.next_batch()\n",
    "        if batch is None:\n",
    "#         if batch is None or valid_count >= 400:\n",
    "            break\n",
    "        valid_count += 1\n",
    "        local_t += 1 \n",
    "        context, context_lens, utt_lens, floors,_,_,_,response,res_lens,_ = batch   \n",
    "        context, utt_lens = context[:,:,1:], utt_lens-1 # remove the sos token in the context and reduce the context length\n",
    "#         f_eval.write(\"Batch %d \\n\" % (local_t))# print the context\n",
    "        start = np.maximum(0, context_lens[0]-5)\n",
    "        for t_id in range(start, context.shape[1], 1):\n",
    "            context_str = indexes2sent(context[0, t_id], vocab, vocab[\"</s>\"], 0)\n",
    "#             f_eval.write(\"Context %d-%d: %s\\n\" % (t_id, floors[0, t_id], context_str))\n",
    "        # print the true outputs    \n",
    "        ref_str, _ = indexes2sent(response[0], vocab, vocab[\"</s>\"], vocab[\"<s>\"])\n",
    "        ref_tokens = ref_str.split(' ')\n",
    "#         f_eval.write(\"Target >> %s\\n\" % (ref_str.replace(\" ' \", \"'\")))\n",
    "        context, context_lens, utt_lens, floors = gVar(context), gVar(context_lens), gVar(utt_lens), gData(floors)\n",
    "        sample_words, sample_lens = sample(context, context_lens, utt_lens, floors, repeat, vocab[\"<s>\"], vocab[\"</s>\"])\n",
    "        # 存储所有生成的回复，用来计算div\n",
    "        all_generated_sentences.append(sample_words[0].tolist())\n",
    "        all_generated_lens.append(sample_lens[0].tolist())\n",
    "        # nparray: [repeat x seq_len]\n",
    "        pred_sents, _ = indexes2sent(sample_words, vocab, vocab[\"</s>\"], 0)\n",
    "        if valid_count % 300 == 0:\n",
    "            print('true response: ', ref_str)\n",
    "            print('generate response: ', pred_sents[0])\n",
    "        pred_tokens = [sent.split(' ') for sent in pred_sents]\n",
    "#         for r_id, pred_sent in enumerate(pred_sents):\n",
    "#             f_eval.write(\"Sample %d >> %s\\n\" % (r_id, pred_sent.replace(\" ' \", \"'\")))\n",
    "        max_bleu, avg_bleu = metrics.sim_bleu(pred_tokens, ref_tokens)\n",
    "        recall_bleus.append(max_bleu)\n",
    "        prec_bleus.append(avg_bleu)\n",
    "        bow_extrema, bow_avg, bow_greedy = metrics.sim_bow(sample_words, sample_lens, response[:,1:], res_lens-2)\n",
    "        bows_extrema.append(bow_extrema)\n",
    "        bows_avg.append(bow_avg)\n",
    "        bows_greedy.append(bow_greedy)\n",
    "#         intra_dist1, intra_dist2, inter_dist1, inter_dist2 = metrics.div_distinct(sample_words, sample_lens-1)\n",
    "#         intra_dist1s.append(intra_dist1)\n",
    "#         intra_dist2s.append(intra_dist2)\n",
    "        avg_lens.append(np.mean(sample_lens))\n",
    "#         inter_dist1s.append(inter_dist1)\n",
    "#         inter_dist2s.append(inter_dist2)\n",
    "    recall_bleu = float(np.mean(recall_bleus))\n",
    "    prec_bleu = float(np.mean(prec_bleus))\n",
    "    f1 = 2*(prec_bleu*recall_bleu) / (prec_bleu+recall_bleu+10e-12)\n",
    "    bow_extrema = float(np.mean(bows_extrema))\n",
    "    bow_avg = float(np.mean(bows_avg))\n",
    "    bow_greedy=float(np.mean(bows_greedy))\n",
    "#     intra_dist1=float(np.mean(intra_dist1s))\n",
    "#     intra_dist2=float(np.mean(intra_dist2s))\n",
    "    avg_len=float(np.mean(avg_lens))\n",
    "    all_generated_sentences = np.array(all_generated_sentences)\n",
    "    all_generated_lens = np.array(all_generated_lens)\n",
    "#     print(all_generated_sentences[:5])\n",
    "#     print(all_generated_lens[:5])\n",
    "    intra_dist1, intra_dist2, inter_dist1, inter_dist2 = metrics.div_distinct(all_generated_sentences, all_generated_lens)\n",
    "#     inter_dist1=float(np.mean(inter_dist1s))\n",
    "#     inter_dist2=float(np.mean(inter_dist2s))\n",
    "    report = \"Avg recall BLEU %f, bow_extrema %f, bow_avg %f, bow_greedy %f, inter_dist1 %f, inter_dist2 %f avg_len %f\" \\\n",
    "    % (recall_bleu, bow_extrema, bow_avg, bow_greedy, inter_dist1, inter_dist2, avg_len)\n",
    "    print(report)\n",
    "    print(' time: %.1f s'%(time()-begin_time))\n",
    "#     f_eval.write(report + \"\\n\")\n",
    "    print(\"Done testing\")\n",
    "    return recall_bleu, bow_extrema, bow_avg, bow_greedy, inter_dist1, inter_dist2, avg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word2vec\n",
      "Epoch:  1\n",
      "Train begins with 2393 batches with 14 left over samples\n",
      "[1 50] loss: 405.543142 time: 1.4 s\n",
      "[1 100] loss: 146.169661 time: 1.4 s\n",
      "[1 150] loss: 116.324097 time: 1.4 s\n",
      "[1 200] loss: 94.348775 time: 1.3 s\n",
      "[1 250] loss: 90.524827 time: 1.3 s\n",
      "[1 300] loss: 74.203323 time: 1.3 s\n",
      "[1 350] loss: 69.581943 time: 1.3 s\n",
      "[1 400] loss: 64.959710 time: 1.3 s\n",
      "[1 450] loss: 66.453394 time: 1.4 s\n",
      "[1 500] loss: 63.519073 time: 1.3 s\n",
      "[1 550] loss: 52.460902 time: 1.3 s\n",
      "[1 600] loss: 48.193870 time: 1.3 s\n",
      "[1 650] loss: 46.085603 time: 1.3 s\n",
      "[1 700] loss: 42.894215 time: 1.3 s\n",
      "[1 750] loss: 42.297767 time: 1.3 s\n",
      "[1 800] loss: 42.600828 time: 1.3 s\n",
      "[1 850] loss: 45.933147 time: 1.3 s\n",
      "[1 900] loss: 42.998136 time: 1.3 s\n",
      "[1 950] loss: 50.568986 time: 1.2 s\n",
      "[1 1000] loss: 51.189504 time: 1.3 s\n",
      "[1 1050] loss: 49.047964 time: 1.3 s\n",
      "[1 1100] loss: 49.000170 time: 1.3 s\n",
      "[1 1150] loss: 47.199214 time: 1.3 s\n",
      "[1 1200] loss: 44.592601 time: 1.3 s\n",
      "[1 1250] loss: 42.769137 time: 1.2 s\n",
      "[1 1300] loss: 41.583287 time: 1.3 s\n",
      "[1 1350] loss: 46.320539 time: 1.2 s\n",
      "[1 1400] loss: 43.444718 time: 1.2 s\n",
      "[1 1450] loss: 42.293655 time: 1.2 s\n",
      "[1 1500] loss: 38.603250 time: 1.2 s\n",
      "[1 1550] loss: 42.252898 time: 1.2 s\n",
      "[1 1600] loss: 39.527563 time: 1.2 s\n",
      "[1 1650] loss: 40.262430 time: 1.2 s\n",
      "[1 1700] loss: 38.011630 time: 1.2 s\n",
      "[1 1750] loss: 39.442767 time: 1.2 s\n",
      "[1 1800] loss: 36.354694 time: 1.1 s\n",
      "[1 1850] loss: 36.309056 time: 1.1 s\n",
      "[1 1900] loss: 42.315310 time: 1.1 s\n",
      "[1 1950] loss: 41.765561 time: 1.1 s\n",
      "[1 2000] loss: 37.505374 time: 1.1 s\n",
      "[1 2050] loss: 34.627514 time: 1.1 s\n",
      "[1 2100] loss: 34.636750 time: 1.1 s\n",
      "[1 2150] loss: 30.339415 time: 1.0 s\n",
      "[1 2200] loss: 34.378458 time: 1.0 s\n",
      "[1 2250] loss: 31.872187 time: 1.0 s\n",
      "[1 2300] loss: 29.107220 time: 1.1 s\n",
      "[1 2350] loss: 32.206281 time: 1.0 s\n",
      "Evaluating....\n",
      "Valid begins with 367 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 1]. loss: 34.825547 time: 59.1 s\n",
      "************************************************************\n",
      "Test begins with 6740 batches with 0 left over samples\n",
      "true response:  i ' m sorry , but mr . johnson is out at the moment . can i take </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m sorry . </s>\n",
      "true response:  nonsmoking , please . </s>\n",
      "generate response:  i ' m sorry . i ' ll have to go to the airport . </s>\n",
      "true response:  well , i wanted to let you know that i ' ve put in my notice . </s>\n",
      "generate response:  i ' m sorry . i ' m not sure . </s>\n",
      "true response:  can i return the car in san francisco ? </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . </s>\n",
      "true response:  you will need to pay late fees on these books . </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m not sure . </s>\n",
      "true response:  that sounds nice . </s>\n",
      "generate response:  i ' m sorry . i ' ll have to go to the office . </s>\n",
      "true response:  i can ’ t wait to vote . </s>\n",
      "generate response:  yes , i ' m sorry . i ' m sorry . </s>\n",
      "true response:  it doesn ' t matter . it happens to everyone . </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . </s>\n",
      "true response:  well , it ' s specially made of a platinum <unk> , sir , and the <unk> will </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . </s>\n",
      "true response:  no , that won ’ t be necessary . </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m sorry . </s>\n",
      "true response:  let me give you a little advice . you know what the <unk> are going to be , </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m not sure . </s>\n",
      "true response:  well , it looks like his encouragement paid off rebecca . so how about extracurricular activities at university </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m not sure . </s>\n",
      "true response:  i love dogs . they have been used as guards for centuries . nowadays , they are often </s>\n",
      "generate response:  i think you ' ll be able to make a good job . </s>\n",
      "true response:  <unk> , yes . there ’ s a big cavity . </s>\n",
      "generate response:  i ' m sorry . </s>\n",
      "true response:  sorry , he is out . </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m sorry . </s>\n",
      "true response:  good morning . i ’ m here to see <unk> davis , the human resources manager . </s>\n",
      "generate response:  yes , i ' m looking for a suit . </s>\n",
      "true response:  yes , stones . </s>\n",
      "generate response:  i ' m afraid i ' m not sure . </s>\n",
      "true response:  a :: tim , please . please be seated . </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . </s>\n",
      "true response:  we ' d like to invite you for our dress party tomorrow evening , are you free ? </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . </s>\n",
      "true response:  look , that bamboo <unk> are <unk> . let ' s go and play there . </s>\n",
      "generate response:  i ' m going to the post office . </s>\n",
      "true response:  christine , i know you ’ re new here and there ’ s a lot to learn , </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m sorry . </s>\n",
      "true response:  it looks like the shipping company did this . </s>\n",
      "generate response:  i ' m sorry . i ' m not sure . </s>\n",
      "Avg recall BLEU 0.276401, bow_extrema 0.492737, bow_avg 0.877620, bow_greedy 0.790038, inter_dist1 0.000897, inter_dist2 0.001794 avg_len 11.418398\n",
      " time: 140.4 s\n",
      "Done testing\n",
      "Epoch:  2\n",
      "Train begins with 2393 batches with 14 left over samples\n",
      "[2 50] loss: 35.518357 time: 1.4 s\n",
      "[2 100] loss: 31.255915 time: 1.4 s\n",
      "[2 150] loss: 31.344015 time: 1.4 s\n",
      "[2 200] loss: 31.219201 time: 1.4 s\n",
      "[2 250] loss: 33.800062 time: 1.3 s\n",
      "[2 300] loss: 29.734501 time: 1.4 s\n",
      "[2 350] loss: 29.993572 time: 1.3 s\n",
      "[2 400] loss: 29.443859 time: 1.3 s\n",
      "[2 450] loss: 30.901721 time: 1.3 s\n",
      "[2 500] loss: 30.760980 time: 1.3 s\n",
      "[2 550] loss: 26.167421 time: 1.3 s\n",
      "[2 600] loss: 24.662166 time: 1.3 s\n",
      "[2 650] loss: 24.078427 time: 1.3 s\n",
      "[2 700] loss: 23.083263 time: 1.3 s\n",
      "[2 750] loss: 23.027885 time: 1.3 s\n",
      "[2 800] loss: 23.405989 time: 1.3 s\n",
      "[2 850] loss: 25.676343 time: 1.3 s\n",
      "[2 900] loss: 23.939349 time: 1.3 s\n",
      "[2 950] loss: 28.573888 time: 1.3 s\n",
      "[2 1000] loss: 27.692976 time: 1.3 s\n",
      "[2 1050] loss: 27.142369 time: 1.3 s\n",
      "[2 1100] loss: 28.252028 time: 1.3 s\n",
      "[2 1150] loss: 27.449799 time: 1.2 s\n",
      "[2 1200] loss: 25.885166 time: 1.2 s\n",
      "[2 1250] loss: 25.753434 time: 1.2 s\n",
      "[2 1300] loss: 25.057406 time: 1.2 s\n",
      "[2 1350] loss: 28.006468 time: 1.2 s\n",
      "[2 1400] loss: 26.088022 time: 1.2 s\n",
      "[2 1450] loss: 24.714722 time: 1.2 s\n",
      "[2 1500] loss: 23.718170 time: 1.2 s\n",
      "[2 1550] loss: 25.983919 time: 1.2 s\n",
      "[2 1600] loss: 25.182738 time: 1.2 s\n",
      "[2 1650] loss: 25.798443 time: 1.2 s\n",
      "[2 1700] loss: 24.842849 time: 1.2 s\n",
      "[2 1750] loss: 25.261420 time: 1.2 s\n",
      "[2 1800] loss: 23.591146 time: 1.1 s\n",
      "[2 1850] loss: 23.005341 time: 1.2 s\n",
      "[2 1900] loss: 27.308568 time: 1.1 s\n",
      "[2 1950] loss: 27.109889 time: 1.1 s\n",
      "[2 2000] loss: 24.279533 time: 1.1 s\n",
      "[2 2050] loss: 22.853875 time: 1.1 s\n",
      "[2 2100] loss: 23.450384 time: 1.1 s\n",
      "[2 2150] loss: 20.497844 time: 1.1 s\n",
      "[2 2200] loss: 22.919282 time: 1.1 s\n",
      "[2 2250] loss: 21.579673 time: 1.0 s\n",
      "[2 2300] loss: 19.855069 time: 1.0 s\n",
      "[2 2350] loss: 21.726528 time: 1.0 s\n",
      "Evaluating....\n",
      "Valid begins with 367 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 2]. loss: 28.485213 time: 59.0 s\n",
      "************************************************************\n",
      "Test begins with 6740 batches with 0 left over samples\n",
      "true response:  i ' m sorry , but mr . johnson is out at the moment . can i take </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m sorry . i ' m sorry .\n",
      "true response:  nonsmoking , please . </s>\n",
      "generate response:  yes , i ' ll have to be able to make it . </s>\n",
      "true response:  well , i wanted to let you know that i ' ve put in my notice . </s>\n",
      "generate response:  i ' m going to the pub tonight . </s>\n",
      "true response:  can i return the car in san francisco ? </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m not sure . </s>\n",
      "true response:  you will need to pay late fees on these books . </s>\n",
      "generate response:  i hope you can help you . </s>\n",
      "true response:  that sounds nice . </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m not sure . </s>\n",
      "true response:  i can ’ t wait to vote . </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m not sure . </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  it doesn ' t matter . it happens to everyone . </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m not sure . </s>\n",
      "true response:  well , it ' s specially made of a platinum <unk> , sir , and the <unk> will </s>\n",
      "generate response:  i ' m afraid i ' m not sure . </s>\n",
      "true response:  no , that won ’ t be necessary . </s>\n",
      "generate response:  thanks a lot . </s>\n",
      "true response:  let me give you a little advice . you know what the <unk> are going to be , </s>\n",
      "generate response:  i ' m sorry . i ' m not sure . i ' m not sure . </s>\n",
      "true response:  well , it looks like his encouragement paid off rebecca . so how about extracurricular activities at university </s>\n",
      "generate response:  i ’ m sorry . i ’ m not sure . i ’ m not sure . </s>\n",
      "true response:  i love dogs . they have been used as guards for centuries . nowadays , they are often </s>\n",
      "generate response:  i think you ' ll be able to make a good job . </s>\n",
      "true response:  <unk> , yes . there ’ s a big cavity . </s>\n",
      "generate response:  i ' m sorry . i ' m not sure . </s>\n",
      "true response:  sorry , he is out . </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m sorry . i ' m sorry .\n",
      "true response:  good morning . i ’ m here to see <unk> davis , the human resources manager . </s>\n",
      "generate response:  yes , i ' d like to have a suit . </s>\n",
      "true response:  yes , stones . </s>\n",
      "generate response:  yes , i think the government will be settled . </s>\n",
      "true response:  a :: tim , please . please be seated . </s>\n",
      "generate response:  thanks for your help . </s>\n",
      "true response:  we ' d like to invite you for our dress party tomorrow evening , are you free ? </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m sorry . i ' m not sure\n",
      "true response:  look , that bamboo <unk> are <unk> . let ' s go and play there . </s>\n",
      "generate response:  i ' m going to the pub tonight . </s>\n",
      "true response:  christine , i know you ’ re new here and there ’ s a lot to learn , </s>\n",
      "generate response:  i ’ m sorry . i ’ m sorry . i ’ m sorry . i ’ m sorry .\n",
      "true response:  it looks like the shipping company did this . </s>\n",
      "generate response:  i ' m not sure . i ' m not sure . </s>\n",
      "Avg recall BLEU 0.271592, bow_extrema 0.498808, bow_avg 0.892005, bow_greedy 0.787252, inter_dist1 0.002034, inter_dist2 0.005154 avg_len 12.255341\n",
      " time: 159.9 s\n",
      "Done testing\n",
      "Epoch:  3\n",
      "Train begins with 2393 batches with 14 left over samples\n",
      "[3 50] loss: 24.340314 time: 1.3 s\n",
      "[3 100] loss: 22.213187 time: 1.2 s\n",
      "[3 150] loss: 21.773184 time: 1.3 s\n",
      "[3 200] loss: 22.082783 time: 1.3 s\n",
      "[3 250] loss: 24.504328 time: 1.3 s\n",
      "[3 300] loss: 21.279787 time: 1.3 s\n",
      "[3 350] loss: 21.887894 time: 1.3 s\n",
      "[3 400] loss: 21.184218 time: 1.2 s\n",
      "[3 450] loss: 22.360401 time: 1.2 s\n",
      "[3 500] loss: 22.413073 time: 1.2 s\n",
      "[3 550] loss: 19.280897 time: 1.2 s\n",
      "[3 600] loss: 17.918329 time: 1.2 s\n",
      "[3 650] loss: 17.155980 time: 1.2 s\n",
      "[3 700] loss: 16.707601 time: 1.3 s\n",
      "[3 750] loss: 16.917932 time: 1.2 s\n",
      "[3 800] loss: 17.131324 time: 1.2 s\n",
      "[3 850] loss: 18.865467 time: 1.2 s\n",
      "[3 900] loss: 17.203869 time: 1.2 s\n",
      "[3 950] loss: 21.281952 time: 1.2 s\n",
      "[3 1000] loss: 20.107180 time: 1.2 s\n",
      "[3 1050] loss: 19.675734 time: 1.2 s\n",
      "[3 1100] loss: 20.843478 time: 1.2 s\n",
      "[3 1150] loss: 20.360983 time: 1.2 s\n",
      "[3 1200] loss: 19.099175 time: 1.2 s\n",
      "[3 1250] loss: 19.381664 time: 1.2 s\n",
      "[3 1300] loss: 18.799289 time: 1.2 s\n",
      "[3 1350] loss: 20.998235 time: 1.1 s\n",
      "[3 1400] loss: 19.523904 time: 1.1 s\n",
      "[3 1450] loss: 18.026635 time: 1.1 s\n",
      "[3 1500] loss: 17.942000 time: 1.1 s\n",
      "[3 1550] loss: 19.459182 time: 1.1 s\n",
      "[3 1600] loss: 19.222100 time: 1.1 s\n",
      "[3 1650] loss: 19.410446 time: 1.1 s\n",
      "[3 1700] loss: 19.061817 time: 1.1 s\n",
      "[3 1750] loss: 19.277713 time: 1.1 s\n",
      "[3 1800] loss: 18.238803 time: 1.1 s\n",
      "[3 1850] loss: 17.441805 time: 1.1 s\n",
      "[3 1900] loss: 20.647535 time: 1.0 s\n",
      "[3 1950] loss: 20.657182 time: 1.0 s\n",
      "[3 2000] loss: 18.649986 time: 1.0 s\n",
      "[3 2050] loss: 17.725015 time: 1.1 s\n",
      "[3 2100] loss: 18.190921 time: 1.0 s\n",
      "[3 2150] loss: 16.067645 time: 1.1 s\n",
      "[3 2200] loss: 17.661846 time: 1.0 s\n",
      "[3 2250] loss: 16.912053 time: 1.0 s\n",
      "[3 2300] loss: 15.660597 time: 1.0 s\n",
      "[3 2350] loss: 16.777859 time: 1.0 s\n",
      "Evaluating....\n",
      "Valid begins with 367 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 3]. loss: 26.162046 time: 55.7 s\n",
      "************************************************************\n",
      "Test begins with 6740 batches with 0 left over samples\n",
      "true response:  i ' m sorry , but mr . johnson is out at the moment . can i take </s>\n",
      "generate response:  yes , i ' m sorry . i ' m sorry . i ' m sorry . </s>\n",
      "true response:  nonsmoking , please . </s>\n",
      "generate response:  yes , i ' d like to have a look at the menu . </s>\n",
      "true response:  well , i wanted to let you know that i ' ve put in my notice . </s>\n",
      "generate response:  i ' m going to the pub tonight . </s>\n",
      "true response:  can i return the car in san francisco ? </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m sorry . i ' m sorry </s>\n",
      "true response:  you will need to pay late fees on these books . </s>\n",
      "generate response:  i ’ ll have to go . </s>\n",
      "true response:  that sounds nice . </s>\n",
      "generate response:  thanks . </s>\n",
      "true response:  i can ’ t wait to vote . </s>\n",
      "generate response:  yes , i ' m sorry . i ' m sorry . i ' m not sure . </s>\n",
      "true response:  it doesn ' t matter . it happens to everyone . </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m sorry . i ' m </s>\n",
      "true response:  well , it ' s specially made of a platinum <unk> , sir , and the <unk> will </s>\n",
      "generate response:  it ' s a bit stiff . it ' s a bit stiff . </s>\n",
      "true response:  no , that won ’ t be necessary . </s>\n",
      "generate response:  thank you . i ’ ll have to check out . </s>\n",
      "true response:  let me give you a little advice . you know what the <unk> are going to be , </s>\n",
      "generate response:  i ' m sorry . i ' m not sure . i ' m sure you ' ll </s>\n",
      "true response:  well , it looks like his encouragement paid off rebecca . so how about extracurricular activities at university </s>\n",
      "generate response:  you are right . i ’ ll have to go to the office . </s>\n",
      "true response:  i love dogs . they have been used as guards for centuries . nowadays , they are often </s>\n",
      "generate response:  i think you ' ll be better . </s>\n",
      "true response:  <unk> , yes . there ’ s a big cavity . </s>\n",
      "generate response:  i ' m sorry . i ' m afraid i ' m not sure . </s>\n",
      "true response:  sorry , he is out . </s>\n",
      "generate response:  i ’ m sorry . i ’ m sorry . i ’ m sorry . i ’ m sorry </s>\n",
      "true response:  good morning . i ’ m here to see <unk> davis , the human resources manager . </s>\n",
      "generate response:  yes , i ' d like to have a suit . </s>\n",
      "true response:  yes , stones . </s>\n",
      "generate response:  yes , it is . i think the government should have done a lot of money . </s>\n",
      "true response:  a :: tim , please . please be seated . </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m not sure . </s>\n",
      "true response:  we ' d like to invite you for our dress party tomorrow evening , are you free ? </s>\n",
      "generate response:  i ' ll have to go . </s>\n",
      "true response:  look , that bamboo <unk> are <unk> . let ' s go and play there . </s>\n",
      "generate response:  i ' m sorry . i ' m late . </s>\n",
      "true response:  christine , i know you ’ re new here and there ’ s a lot to learn , </s>\n",
      "generate response:  i ’ m sorry . i ’ m sorry . i ’ m sorry . i ’ ve got </s>\n",
      "true response:  it looks like the shipping company did this . </s>\n",
      "generate response:  i ' m sorry . i ' m not sure . </s>\n",
      "Avg recall BLEU 0.256750, bow_extrema 0.504507, bow_avg 0.883952, bow_greedy 0.796542, inter_dist1 0.004694, inter_dist2 0.014051 avg_len 10.777745\n",
      " time: 165.3 s\n",
      "Done testing\n",
      "Epoch:  4\n",
      "Train begins with 2393 batches with 14 left over samples\n",
      "[4 50] loss: 18.753660 time: 1.4 s\n",
      "[4 100] loss: 17.745051 time: 1.3 s\n",
      "[4 150] loss: 17.056512 time: 1.4 s\n",
      "[4 200] loss: 17.179152 time: 1.3 s\n",
      "[4 250] loss: 19.385588 time: 1.3 s\n",
      "[4 300] loss: 16.891978 time: 1.4 s\n",
      "[4 350] loss: 17.541257 time: 1.3 s\n",
      "[4 400] loss: 16.581970 time: 1.3 s\n",
      "[4 450] loss: 17.658950 time: 1.1 s\n",
      "[4 500] loss: 17.923818 time: 1.3 s\n",
      "[4 550] loss: 15.512227 time: 1.3 s\n",
      "[4 600] loss: 14.257656 time: 1.3 s\n",
      "[4 650] loss: 13.499054 time: 1.3 s\n",
      "[4 700] loss: 13.306419 time: 1.1 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 750] loss: 13.653819 time: 1.3 s\n",
      "[4 800] loss: 13.639791 time: 1.3 s\n",
      "[4 850] loss: 15.119535 time: 1.1 s\n",
      "[4 900] loss: 13.558507 time: 1.3 s\n",
      "[4 950] loss: 16.836984 time: 1.2 s\n",
      "[4 1000] loss: 15.739160 time: 1.3 s\n",
      "[4 1050] loss: 15.593383 time: 1.2 s\n",
      "[4 1100] loss: 16.648988 time: 1.1 s\n",
      "[4 1150] loss: 16.295123 time: 1.2 s\n",
      "[4 1200] loss: 15.301578 time: 1.0 s\n",
      "[4 1250] loss: 15.811700 time: 1.3 s\n",
      "[4 1300] loss: 15.399139 time: 1.2 s\n",
      "[4 1350] loss: 16.784759 time: 1.2 s\n",
      "[4 1400] loss: 15.718522 time: 1.1 s\n",
      "[4 1450] loss: 14.225892 time: 1.0 s\n",
      "[4 1500] loss: 14.575527 time: 1.2 s\n",
      "[4 1550] loss: 15.749287 time: 1.1 s\n",
      "[4 1600] loss: 15.731176 time: 1.0 s\n",
      "[4 1650] loss: 15.885844 time: 1.2 s\n",
      "[4 1700] loss: 15.718367 time: 1.1 s\n",
      "[4 1750] loss: 15.748219 time: 1.1 s\n",
      "[4 1800] loss: 14.918579 time: 1.2 s\n",
      "[4 1850] loss: 14.161859 time: 1.2 s\n",
      "[4 1900] loss: 16.761000 time: 0.9 s\n",
      "[4 1950] loss: 16.723910 time: 1.1 s\n",
      "[4 2000] loss: 15.300872 time: 1.0 s\n",
      "[4 2050] loss: 14.607748 time: 1.1 s\n",
      "[4 2100] loss: 15.149628 time: 1.1 s\n",
      "[4 2150] loss: 13.356886 time: 1.0 s\n",
      "[4 2200] loss: 14.575413 time: 1.0 s\n",
      "[4 2250] loss: 14.049749 time: 1.0 s\n",
      "[4 2300] loss: 13.177332 time: 1.1 s\n",
      "[4 2350] loss: 13.906631 time: 1.0 s\n",
      "Evaluating....\n",
      "Valid begins with 367 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 4]. loss: 25.639098 time: 56.9 s\n",
      "************************************************************\n",
      "Test begins with 6740 batches with 0 left over samples\n",
      "true response:  i ' m sorry , but mr . johnson is out at the moment . can i take </s>\n",
      "generate response:  yes , i ' m sorry . i ' m sorry . i ' m not sure . </s>\n",
      "true response:  nonsmoking , please . </s>\n",
      "generate response:  i ' m sorry . i ' m afraid i can ' t . </s>\n",
      "true response:  well , i wanted to let you know that i ' ve put in my notice . </s>\n",
      "generate response:  i ' m going to the pub tonight . </s>\n",
      "true response:  can i return the car in san francisco ? </s>\n",
      "generate response:  thanks . i ' ll have to go . </s>\n",
      "true response:  you will need to pay late fees on these books . </s>\n",
      "generate response:  i can ’ t agree with you . </s>\n",
      "true response:  that sounds nice . </s>\n",
      "generate response:  thanks . </s>\n",
      "true response:  i can ’ t wait to vote . </s>\n",
      "generate response:  yes , i ’ m looking for a suit . </s>\n",
      "true response:  it doesn ' t matter . it happens to everyone . </s>\n",
      "generate response:  i ' m sorry . i ' m sorry . i ' m not sure . </s>\n",
      "true response:  well , it ' s specially made of a platinum <unk> , sir , and the <unk> will </s>\n",
      "generate response:  you can take a taxi . </s>\n",
      "true response:  no , that won ’ t be necessary . </s>\n",
      "generate response:  thanks . i ’ ll have to check out . </s>\n",
      "true response:  let me give you a little advice . you know what the <unk> are going to be , </s>\n",
      "generate response:  i ' m sorry . i ' m not sure . i ' m sure you ' ll </s>\n",
      "true response:  well , it looks like his encouragement paid off rebecca . so how about extracurricular activities at university </s>\n",
      "generate response:  you are right . i ’ ll be glad to help you . </s>\n",
      "true response:  i love dogs . they have been used as guards for centuries . nowadays , they are often </s>\n",
      "generate response:  i agree . i think the government should have done a lot of problems in the leadership . </s>\n",
      "true response:  <unk> , yes . there ’ s a big cavity . </s>\n",
      "generate response:  i ' m sorry . i ' m afraid i can ' t . </s>\n",
      "true response:  sorry , he is out . </s>\n",
      "generate response:  i ' m sorry , but i ' m not sure . </s>\n",
      "true response:  good morning . i ’ m here to see <unk> davis , the human resources manager . </s>\n",
      "generate response:  yes , i ' d like to buy a suit . </s>\n",
      "true response:  yes , stones . </s>\n",
      "generate response:  yes , it is . </s>\n",
      "true response:  a :: tim , please . please be seated . </s>\n",
      "generate response:  i ' m sorry . i ' m late . </s>\n",
      "true response:  we ' d like to invite you for our dress party tomorrow evening , are you free ? </s>\n",
      "generate response:  i ' ll have to go . </s>\n",
      "true response:  look , that bamboo <unk> are <unk> . let ' s go and play there . </s>\n",
      "generate response:  i ' m sorry . i ' m late . </s>\n",
      "true response:  christine , i know you ’ re new here and there ’ s a lot to learn , </s>\n",
      "generate response:  thanks , frank . i ’ m sorry . i ’ m not sure . </s>\n",
      "true response:  it looks like the shipping company did this . </s>\n",
      "generate response:  i ' m sorry . i ' m not sure . </s>\n",
      "Avg recall BLEU 0.247471, bow_extrema 0.503434, bow_avg 0.887314, bow_greedy 0.796097, inter_dist1 0.007523, inter_dist2 0.023094 avg_len 10.077745\n",
      " time: 135.4 s\n",
      "Done testing\n",
      "Epoch:  5\n",
      "Train begins with 2393 batches with 14 left over samples\n",
      "[5 50] loss: 15.507632 time: 1.3 s\n",
      "[5 100] loss: 14.330481 time: 1.2 s\n",
      "[5 150] loss: 14.061667 time: 1.4 s\n",
      "[5 200] loss: 14.039018 time: 1.1 s\n",
      "[5 250] loss: 16.142786 time: 1.3 s\n",
      "[5 300] loss: 14.080903 time: 1.3 s\n",
      "[5 350] loss: 14.831941 time: 1.3 s\n",
      "[5 400] loss: 13.759722 time: 1.3 s\n",
      "[5 450] loss: 14.586861 time: 1.3 s\n",
      "[5 500] loss: 14.692124 time: 1.3 s\n",
      "[5 550] loss: 13.081407 time: 1.3 s\n",
      "[5 600] loss: 11.911649 time: 1.1 s\n",
      "[5 650] loss: 11.243615 time: 1.3 s\n",
      "[5 700] loss: 11.205393 time: 1.3 s\n",
      "[5 750] loss: 11.627939 time: 1.1 s\n",
      "[5 800] loss: 11.406589 time: 1.3 s\n",
      "[5 850] loss: 12.599514 time: 1.3 s\n",
      "[5 900] loss: 11.356033 time: 1.3 s\n",
      "[5 950] loss: 14.128458 time: 1.2 s\n",
      "[5 1000] loss: 13.141700 time: 1.2 s\n",
      "[5 1050] loss: 12.875739 time: 1.2 s\n",
      "[5 1100] loss: 13.937333 time: 1.3 s\n",
      "[5 1150] loss: 13.612458 time: 1.3 s\n",
      "[5 1200] loss: 12.861262 time: 1.3 s\n",
      "[5 1250] loss: 13.392966 time: 1.2 s\n",
      "[5 1300] loss: 13.108012 time: 1.3 s\n",
      "[5 1350] loss: 14.125765 time: 1.2 s\n",
      "[5 1400] loss: 13.192485 time: 1.2 s\n",
      "[5 1450] loss: 11.906652 time: 1.2 s\n",
      "[5 1500] loss: 12.435810 time: 1.2 s\n",
      "[5 1550] loss: 13.372296 time: 1.2 s\n",
      "[5 1600] loss: 13.488914 time: 1.2 s\n",
      "[5 1650] loss: 13.403633 time: 1.2 s\n",
      "[5 1700] loss: 13.426323 time: 1.1 s\n",
      "[5 1750] loss: 13.338540 time: 1.1 s\n",
      "[5 1800] loss: 12.718211 time: 1.1 s\n",
      "[5 1850] loss: 11.973029 time: 1.2 s\n",
      "[5 1900] loss: 14.140398 time: 1.2 s\n",
      "[5 1950] loss: 14.134549 time: 1.1 s\n",
      "[5 2000] loss: 13.015658 time: 1.1 s\n",
      "[5 2050] loss: 12.404901 time: 1.0 s\n",
      "[5 2100] loss: 12.863602 time: 1.1 s\n",
      "[5 2150] loss: 11.496694 time: 1.1 s\n",
      "[5 2200] loss: 12.416585 time: 1.0 s\n",
      "[5 2250] loss: 12.097294 time: 1.0 s\n",
      "[5 2300] loss: 11.402479 time: 1.0 s\n",
      "[5 2350] loss: 11.929640 time: 1.1 s\n",
      "Evaluating....\n",
      "Valid begins with 367 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 5]. loss: 25.704557 time: 57.5 s\n",
      "************************************************************\n",
      "Test begins with 6740 batches with 0 left over samples\n",
      "true response:  i ' m sorry , but mr . johnson is out at the moment . can i take </s>\n",
      "generate response:  yes , mr . smith . </s>\n",
      "true response:  nonsmoking , please . </s>\n",
      "generate response:  yes , i ' d like to have a tomato juice and juice . </s>\n",
      "true response:  well , i wanted to let you know that i ' ve put in my notice . </s>\n",
      "generate response:  i ' m going to the pub tonight . </s>\n",
      "true response:  can i return the car in san francisco ? </s>\n",
      "generate response:  i ' m sorry . i ' m afraid i ' ll have to check . </s>\n",
      "true response:  you will need to pay late fees on these books . </s>\n",
      "generate response:  i ’ ll have to go . </s>\n",
      "true response:  that sounds nice . </s>\n",
      "generate response:  thanks . </s>\n",
      "true response:  i can ’ t wait to vote . </s>\n",
      "generate response:  i ’ m sorry . i didn ’ t realize that . </s>\n",
      "true response:  it doesn ' t matter . it happens to everyone . </s>\n",
      "generate response:  i ' m sorry . i didn ' t realize that . </s>\n",
      "true response:  well , it ' s specially made of a platinum <unk> , sir , and the <unk> will </s>\n",
      "generate response:  you can keep it . </s>\n",
      "true response:  no , that won ’ t be necessary . </s>\n",
      "generate response:  thank you . i ’ ll have to check my schedule . </s>\n",
      "true response:  let me give you a little advice . you know what the <unk> are going to be , </s>\n",
      "generate response:  i ' m sorry . i don ' t think you ' ll have to work . </s>\n",
      "true response:  well , it looks like his encouragement paid off rebecca . so how about extracurricular activities at university </s>\n",
      "generate response:  you ’ ll see you at the end of the week . </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  i love dogs . they have been used as guards for centuries . nowadays , they are often </s>\n",
      "generate response:  i agree . i think you ’ re beating around the bush with the air so much </s>\n",
      "true response:  <unk> , yes . there ’ s a big cavity . </s>\n",
      "generate response:  i ' m sorry . i didn ' t realize that . </s>\n",
      "true response:  sorry , he is out . </s>\n",
      "generate response:  i ’ m sorry , but i ’ m not booked up . </s>\n",
      "true response:  good morning . i ’ m here to see <unk> davis , the human resources manager . </s>\n",
      "generate response:  yes . i ' d like to buy a suit . </s>\n",
      "true response:  yes , stones . </s>\n",
      "generate response:  yes , of course . but i think they are a good idea . </s>\n",
      "true response:  a :: tim , please . please be seated . </s>\n",
      "generate response:  i ' m phoning about your work . </s>\n",
      "true response:  we ' d like to invite you for our dress party tomorrow evening , are you free ? </s>\n",
      "generate response:  ok . i ' ll take it . </s>\n",
      "true response:  look , that bamboo <unk> are <unk> . let ' s go and play there . </s>\n",
      "generate response:  i ' m tired . </s>\n",
      "true response:  christine , i know you ’ re new here and there ’ s a lot to learn , </s>\n",
      "generate response:  what ’ s the problem ? </s>\n",
      "true response:  it looks like the shipping company did this . </s>\n",
      "generate response:  i ' m sorry . i ' m not sure . </s>\n",
      "Avg recall BLEU 0.231616, bow_extrema 0.491392, bow_avg 0.890517, bow_greedy 0.795926, inter_dist1 0.010938, inter_dist2 0.035314 avg_len 9.440653\n",
      " time: 137.0 s\n",
      "Done testing\n",
      "Epoch:  6\n",
      "Train begins with 2393 batches with 14 left over samples\n",
      "[6 50] loss: 13.256149 time: 1.4 s\n",
      "[6 100] loss: 12.226384 time: 1.4 s\n",
      "[6 150] loss: 12.050318 time: 1.4 s\n",
      "[6 200] loss: 12.005772 time: 1.4 s\n",
      "[6 250] loss: 14.059879 time: 1.4 s\n",
      "[6 300] loss: 12.360161 time: 1.4 s\n",
      "[6 350] loss: 12.612714 time: 1.3 s\n",
      "[6 400] loss: 11.813021 time: 1.3 s\n",
      "[6 450] loss: 12.461241 time: 1.3 s\n",
      "[6 500] loss: 12.501902 time: 1.3 s\n",
      "[6 550] loss: 11.469822 time: 1.3 s\n",
      "[6 600] loss: 10.372259 time: 1.3 s\n",
      "[6 650] loss: 9.778236 time: 1.2 s\n",
      "[6 700] loss: 9.746503 time: 1.3 s\n",
      "[6 750] loss: 10.156980 time: 1.3 s\n",
      "[6 800] loss: 9.947915 time: 1.3 s\n",
      "[6 850] loss: 10.902126 time: 1.3 s\n",
      "[6 900] loss: 9.830090 time: 1.3 s\n",
      "[6 950] loss: 12.205090 time: 1.2 s\n",
      "[6 1000] loss: 11.118593 time: 1.3 s\n",
      "[6 1050] loss: 11.066335 time: 1.3 s\n",
      "[6 1100] loss: 12.042612 time: 1.3 s\n",
      "[6 1150] loss: 11.806177 time: 1.2 s\n",
      "[6 1200] loss: 11.081720 time: 1.3 s\n",
      "[6 1250] loss: 11.692514 time: 1.2 s\n",
      "[6 1300] loss: 11.494204 time: 1.2 s\n",
      "[6 1350] loss: 12.225511 time: 1.2 s\n",
      "[6 1400] loss: 11.455821 time: 1.2 s\n",
      "[6 1450] loss: 10.240820 time: 1.2 s\n",
      "[6 1500] loss: 10.878992 time: 1.2 s\n",
      "[6 1550] loss: 11.855499 time: 1.2 s\n",
      "[6 1600] loss: 11.878695 time: 1.2 s\n",
      "[6 1650] loss: 11.745371 time: 1.2 s\n",
      "[6 1700] loss: 11.827982 time: 1.2 s\n",
      "[6 1750] loss: 11.692867 time: 1.1 s\n",
      "[6 1800] loss: 11.166965 time: 1.1 s\n",
      "[6 1850] loss: 10.464705 time: 1.2 s\n",
      "[6 1900] loss: 12.346508 time: 1.2 s\n",
      "[6 1950] loss: 12.267854 time: 1.1 s\n",
      "[6 2000] loss: 11.372304 time: 1.1 s\n",
      "[6 2050] loss: 10.833662 time: 1.1 s\n",
      "[6 2100] loss: 11.318894 time: 1.0 s\n",
      "[6 2150] loss: 10.176073 time: 1.1 s\n",
      "[6 2200] loss: 10.865712 time: 1.1 s\n",
      "[6 2250] loss: 10.638450 time: 1.1 s\n",
      "[6 2300] loss: 10.117424 time: 1.1 s\n",
      "[6 2350] loss: 10.503863 time: 1.0 s\n",
      "Evaluating....\n",
      "Valid begins with 367 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 6]. loss: 25.759937 time: 58.8 s\n",
      "************************************************************\n",
      "Test begins with 6740 batches with 0 left over samples\n",
      "true response:  i ' m sorry , but mr . johnson is out at the moment . can i take </s>\n",
      "generate response:  yes , i ' m sorry . i ' m sorry . i ' m not sure . </s>\n",
      "true response:  nonsmoking , please . </s>\n",
      "generate response:  yes , i ' ll have a look . </s>\n",
      "true response:  well , i wanted to let you know that i ' ve put in my notice . </s>\n",
      "generate response:  i ' m going to the pub tonight . </s>\n",
      "true response:  can i return the car in san francisco ? </s>\n",
      "generate response:  i ' m sorry . i ' m not sure . i ' m sure that i can find a\n",
      "true response:  you will need to pay late fees on these books . </s>\n",
      "generate response:  i ’ ll take it . </s>\n",
      "true response:  that sounds nice . </s>\n",
      "generate response:  thanks . </s>\n",
      "true response:  i can ’ t wait to vote . </s>\n",
      "generate response:  i ’ m sorry . i didn ’ t realize that . </s>\n",
      "true response:  it doesn ' t matter . it happens to everyone . </s>\n",
      "generate response:  i ' m sorry . i didn ' t realize that . </s>\n",
      "true response:  well , it ' s specially made of a platinum <unk> , sir , and the <unk> will </s>\n",
      "generate response:  it ' s $ 160 . </s>\n",
      "true response:  no , that won ’ t be necessary . </s>\n",
      "generate response:  thank you . i ’ ll have to check the schedule . </s>\n",
      "true response:  let me give you a little advice . you know what the <unk> are going to be , </s>\n",
      "generate response:  why ? </s>\n",
      "true response:  well , it looks like his encouragement paid off rebecca . so how about extracurricular activities at university </s>\n",
      "generate response:  you ’ ll see the doctor ’ s meeting . we ’ ll have to postpone the meeting . </s>\n",
      "true response:  i love dogs . they have been used as guards for centuries . nowadays , they are often </s>\n",
      "generate response:  i agree . i think the government should have to be a good time . </s>\n",
      "true response:  <unk> , yes . there ’ s a big cavity . </s>\n",
      "generate response:  i ' m sorry . i didn ' t realize that . </s>\n",
      "true response:  sorry , he is out . </s>\n",
      "generate response:  i ’ m sorry , mr . emory . i ’ m sorry . i ’ m not sure .\n",
      "true response:  good morning . i ’ m here to see <unk> davis , the human resources manager . </s>\n",
      "generate response:  yes . i ' d like to buy a sweater . </s>\n",
      "true response:  yes , stones . </s>\n",
      "generate response:  yes . the air is to the distance . i can ’ t find it . </s>\n",
      "true response:  a :: tim , please . please be seated . </s>\n",
      "generate response:  i ' m phoning about your work . </s>\n",
      "true response:  we ' d like to invite you for our dress party tomorrow evening , are you free ? </s>\n",
      "generate response:  would you please give me a call ? </s>\n",
      "true response:  look , that bamboo <unk> are <unk> . let ' s go and play there . </s>\n",
      "generate response:  i ' m tired . </s>\n",
      "true response:  christine , i know you ’ re new here and there ’ s a lot to learn , </s>\n",
      "generate response:  oh , that ’ s nice . i ’ m sure i ’ ll be able to </s>\n",
      "true response:  it looks like the shipping company did this . </s>\n",
      "generate response:  i ' m sorry . i ' m not sure . </s>\n",
      "Avg recall BLEU 0.234846, bow_extrema 0.491586, bow_avg 0.892962, bow_greedy 0.798409, inter_dist1 0.014283, inter_dist2 0.048152 avg_len 9.556677\n",
      " time: 200.9 s\n",
      "Done testing\n",
      "Epoch:  7\n",
      "Train begins with 2393 batches with 14 left over samples\n",
      "[7 50] loss: 11.514059 time: 1.4 s\n",
      "[7 100] loss: 10.662991 time: 1.4 s\n",
      "[7 150] loss: 10.633447 time: 1.4 s\n",
      "[7 200] loss: 10.513540 time: 1.3 s\n",
      "[7 250] loss: 12.308266 time: 1.3 s\n",
      "[7 300] loss: 10.737996 time: 1.4 s\n",
      "[7 350] loss: 11.066145 time: 1.3 s\n",
      "[7 400] loss: 10.402893 time: 1.3 s\n",
      "[7 450] loss: 10.951566 time: 1.3 s\n",
      "[7 500] loss: 10.909933 time: 1.3 s\n",
      "[7 550] loss: 10.130437 time: 1.3 s\n",
      "[7 600] loss: 9.219494 time: 1.3 s\n",
      "[7 650] loss: 8.720093 time: 1.3 s\n",
      "[7 700] loss: 8.709709 time: 1.3 s\n",
      "[7 750] loss: 9.101501 time: 1.2 s\n",
      "[7 800] loss: 8.906991 time: 1.3 s\n",
      "[7 850] loss: 9.668923 time: 1.3 s\n",
      "[7 900] loss: 8.772246 time: 1.3 s\n",
      "[7 950] loss: 10.855531 time: 1.3 s\n",
      "[7 1000] loss: 9.735051 time: 1.3 s\n",
      "[7 1050] loss: 9.770546 time: 1.3 s\n",
      "[7 1100] loss: 10.727532 time: 1.3 s\n",
      "[7 1150] loss: 10.556271 time: 1.2 s\n",
      "[7 1200] loss: 9.897699 time: 1.2 s\n",
      "[7 1250] loss: 10.436288 time: 1.2 s\n",
      "[7 1300] loss: 10.258172 time: 1.2 s\n",
      "[7 1350] loss: 10.934076 time: 1.2 s\n",
      "[7 1400] loss: 10.171819 time: 1.2 s\n",
      "[7 1450] loss: 9.150559 time: 1.2 s\n",
      "[7 1500] loss: 9.955829 time: 1.2 s\n",
      "[7 1550] loss: 10.571735 time: 1.2 s\n",
      "[7 1600] loss: 10.573081 time: 1.2 s\n",
      "[7 1650] loss: 10.542152 time: 1.1 s\n",
      "[7 1700] loss: 10.657741 time: 1.1 s\n",
      "[7 1750] loss: 10.430324 time: 1.2 s\n",
      "[7 1800] loss: 10.034575 time: 1.1 s\n",
      "[7 1850] loss: 9.361339 time: 1.1 s\n",
      "[7 1900] loss: 11.032604 time: 1.1 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 1950] loss: 10.950815 time: 1.1 s\n",
      "[7 2000] loss: 10.172276 time: 1.1 s\n",
      "[7 2050] loss: 9.740996 time: 1.1 s\n",
      "[7 2100] loss: 10.171589 time: 1.1 s\n",
      "[7 2150] loss: 9.203370 time: 1.0 s\n",
      "[7 2200] loss: 9.808709 time: 1.0 s\n",
      "[7 2250] loss: 9.611292 time: 1.0 s\n",
      "[7 2300] loss: 9.199462 time: 1.0 s\n",
      "[7 2350] loss: 9.430979 time: 1.0 s\n",
      "Evaluating....\n",
      "Valid begins with 367 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 7]. loss: 26.197943 time: 58.5 s\n",
      "************************************************************\n",
      "Test begins with 6740 batches with 0 left over samples\n",
      "true response:  i ' m sorry , but mr . johnson is out at the moment . can i take </s>\n",
      "generate response:  yes , mr . smith . </s>\n",
      "true response:  nonsmoking , please . </s>\n",
      "generate response:  yes , i ' ll have a look . </s>\n",
      "true response:  well , i wanted to let you know that i ' ve put in my notice . </s>\n",
      "generate response:  i ' m going to the pub tonight . </s>\n",
      "true response:  can i return the car in san francisco ? </s>\n",
      "generate response:  thanks . i ' ll have to check the schedule . </s>\n",
      "true response:  you will need to pay late fees on these books . </s>\n",
      "generate response:  i ’ ll take it . </s>\n",
      "true response:  that sounds nice . </s>\n",
      "generate response:  thanks , vivian . </s>\n",
      "true response:  i can ’ t wait to vote . </s>\n",
      "generate response:  i ’ m sorry . i didn ’ t know you . </s>\n",
      "true response:  it doesn ' t matter . it happens to everyone . </s>\n",
      "generate response:  i ' m sorry . i didn ' t catch the nine o ' clock . </s>\n",
      "true response:  well , it ' s specially made of a platinum <unk> , sir , and the <unk> will </s>\n",
      "generate response:  it ' s $ 160 . </s>\n",
      "true response:  no , that won ’ t be necessary . </s>\n",
      "generate response:  thanks . i ’ ll have to check the schedule . </s>\n",
      "true response:  let me give you a little advice . you know what the <unk> are going to be , </s>\n",
      "generate response:  i think you ' ll have a lot of work on your mind . </s>\n",
      "true response:  well , it looks like his encouragement paid off rebecca . so how about extracurricular activities at university </s>\n",
      "generate response:  i ’ m sorry . i didn ’ t realize that . </s>\n",
      "true response:  i love dogs . they have been used as guards for centuries . nowadays , they are often </s>\n",
      "generate response:  i agree . i think the government should have to face the face of <unk> . </s>\n",
      "true response:  <unk> , yes . there ’ s a big cavity . </s>\n",
      "generate response:  it ' s a bit far . it ' s a bit too tight . </s>\n",
      "true response:  sorry , he is out . </s>\n",
      "generate response:  this is paula blake . </s>\n",
      "true response:  good morning . i ’ m here to see <unk> davis , the human resources manager . </s>\n",
      "generate response:  yes . i ' d like to buy a sweater . </s>\n",
      "true response:  yes , stones . </s>\n",
      "generate response:  yes , i think so . but i think they ’ ll be a good taste for </s>\n",
      "true response:  a :: tim , please . please be seated . </s>\n",
      "generate response:  happy birthday , alice . i ' m afraid i ' m going to drop in shape . </s>\n",
      "true response:  we ' d like to invite you for our dress party tomorrow evening , are you free ? </s>\n",
      "generate response:  ok , i ' ll go to the party tonight . </s>\n",
      "true response:  look , that bamboo <unk> are <unk> . let ' s go and play there . </s>\n",
      "generate response:  i ' m tired of my job . </s>\n",
      "true response:  christine , i know you ’ re new here and there ’ s a lot to learn , </s>\n",
      "generate response:  oh , that ’ s nice . i ’ ll take it . </s>\n",
      "true response:  it looks like the shipping company did this . </s>\n",
      "generate response:  i ' m sorry . i didn ' t realize it . </s>\n",
      "Avg recall BLEU 0.247535, bow_extrema 0.490326, bow_avg 0.900729, bow_greedy 0.794546, inter_dist1 0.016238, inter_dist2 0.056781 avg_len 9.722107\n",
      " time: 136.7 s\n",
      "Done testing\n",
      "Epoch:  8\n",
      "Train begins with 2393 batches with 14 left over samples\n",
      "[8 50] loss: 10.383088 time: 1.4 s\n",
      "[8 100] loss: 9.620617 time: 1.4 s\n",
      "[8 150] loss: 9.578775 time: 1.4 s\n",
      "[8 200] loss: 9.748071 time: 1.3 s\n",
      "[8 250] loss: 11.414378 time: 1.3 s\n",
      "[8 300] loss: 9.736240 time: 1.4 s\n",
      "[8 350] loss: 10.014456 time: 1.3 s\n",
      "[8 400] loss: 9.418436 time: 1.3 s\n",
      "[8 450] loss: 9.898895 time: 1.3 s\n",
      "[8 500] loss: 9.834066 time: 1.3 s\n",
      "[8 550] loss: 9.264712 time: 1.3 s\n",
      "[8 600] loss: 8.523991 time: 1.3 s\n",
      "[8 650] loss: 8.003193 time: 1.2 s\n",
      "[8 700] loss: 8.005899 time: 1.2 s\n",
      "[8 750] loss: 8.365958 time: 1.2 s\n",
      "[8 800] loss: 8.140740 time: 1.3 s\n",
      "[8 850] loss: 8.794538 time: 1.3 s\n",
      "[8 900] loss: 8.061486 time: 1.3 s\n",
      "[8 950] loss: 9.884674 time: 1.3 s\n",
      "[8 1000] loss: 8.987522 time: 1.2 s\n",
      "[8 1050] loss: 8.942012 time: 1.2 s\n",
      "[8 1100] loss: 9.836393 time: 1.2 s\n",
      "[8 1150] loss: 9.548719 time: 1.2 s\n",
      "[8 1200] loss: 8.995005 time: 1.2 s\n",
      "[8 1250] loss: 9.502292 time: 1.2 s\n",
      "[8 1300] loss: 9.385093 time: 1.3 s\n",
      "[8 1350] loss: 9.997313 time: 1.2 s\n",
      "[8 1400] loss: 9.317012 time: 1.2 s\n",
      "[8 1450] loss: 8.277459 time: 1.2 s\n",
      "[8 1500] loss: 8.984979 time: 1.2 s\n",
      "[8 1550] loss: 9.615779 time: 1.1 s\n",
      "[8 1600] loss: 9.633667 time: 1.2 s\n",
      "[8 1650] loss: 9.528607 time: 1.2 s\n",
      "[8 1700] loss: 9.789946 time: 1.2 s\n",
      "[8 1750] loss: 9.566776 time: 1.1 s\n",
      "[8 1800] loss: 9.221106 time: 1.1 s\n",
      "[8 1850] loss: 8.555627 time: 1.1 s\n",
      "[8 1900] loss: 10.054379 time: 1.1 s\n",
      "[8 1950] loss: 10.009031 time: 1.1 s\n",
      "[8 2000] loss: 9.308388 time: 1.1 s\n",
      "[8 2050] loss: 8.885508 time: 1.1 s\n",
      "[8 2100] loss: 9.277911 time: 1.0 s\n",
      "[8 2150] loss: 8.497319 time: 1.0 s\n",
      "[8 2200] loss: 8.919178 time: 1.0 s\n",
      "[8 2250] loss: 8.856728 time: 1.1 s\n",
      "[8 2300] loss: 8.467590 time: 1.0 s\n",
      "[8 2350] loss: 8.664487 time: 1.0 s\n",
      "Evaluating....\n",
      "Valid begins with 367 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 8]. loss: 27.083207 time: 58.5 s\n",
      "************************************************************\n",
      "Test begins with 6740 batches with 0 left over samples\n",
      "true response:  i ' m sorry , but mr . johnson is out at the moment . can i take </s>\n",
      "generate response:  yes , mr . smith . </s>\n",
      "true response:  nonsmoking , please . </s>\n",
      "generate response:  yes , please . </s>\n",
      "true response:  well , i wanted to let you know that i ' ve put in my notice . </s>\n",
      "generate response:  i ' m going to quit my job . </s>\n",
      "true response:  can i return the car in san francisco ? </s>\n",
      "generate response:  i ' m sorry . i didn ' t pay attention . i ' m buying a </s>\n",
      "true response:  you will need to pay late fees on these books . </s>\n",
      "generate response:  i ’ ll take it . </s>\n",
      "true response:  that sounds nice . </s>\n",
      "generate response:  thanks , i ' ll have to go . </s>\n",
      "true response:  i can ’ t wait to vote . </s>\n",
      "generate response:  i ’ m sorry . i didn ’ t catch the nine o ’ clock . </s>\n",
      "true response:  it doesn ' t matter . it happens to everyone . </s>\n",
      "generate response:  i ' m sorry , but i ' m sure you ' ll have to promise . </s>\n",
      "true response:  well , it ' s specially made of a platinum <unk> , sir , and the <unk> will </s>\n",
      "generate response:  it ' s $ 160 . </s>\n",
      "true response:  no , that won ’ t be necessary . </s>\n",
      "generate response:  thanks . </s>\n",
      "true response:  let me give you a little advice . you know what the <unk> are going to be , </s>\n",
      "generate response:  i don ' t think you ' ll have to face the face . </s>\n",
      "true response:  well , it looks like his encouragement paid off rebecca . so how about extracurricular activities at university </s>\n",
      "generate response:  you ’ re right . i ’ m sure you ’ ll have a good time . </s>\n",
      "true response:  i love dogs . they have been used as guards for centuries . nowadays , they are often </s>\n",
      "generate response:  i think you ’ re beating around the bush . </s>\n",
      "true response:  <unk> , yes . there ’ s a big cavity . </s>\n",
      "generate response:  how about this one ? </s>\n",
      "true response:  sorry , he is out . </s>\n",
      "generate response:  he ’ s out of the office . he has a girlfriend . </s>\n",
      "true response:  good morning . i ’ m here to see <unk> davis , the human resources manager . </s>\n",
      "generate response:  yes . i ' d like to book a table for 2 . </s>\n",
      "true response:  yes , stones . </s>\n",
      "generate response:  yes , but i doubt it . </s>\n",
      "true response:  a :: tim , please . please be seated . </s>\n",
      "generate response:  i ' m phoning about the job . </s>\n",
      "true response:  we ' d like to invite you for our dress party tomorrow evening , are you free ? </s>\n",
      "generate response:  you ' ll have to go . </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  look , that bamboo <unk> are <unk> . let ' s go and play there . </s>\n",
      "generate response:  i ' m going to the pub tonight . </s>\n",
      "true response:  christine , i know you ’ re new here and there ’ s a lot to learn , </s>\n",
      "generate response:  oh , that ’ s really unbearable . </s>\n",
      "true response:  it looks like the shipping company did this . </s>\n",
      "generate response:  i ' m sorry , but i ' m not sure . </s>\n",
      "Avg recall BLEU 0.251015, bow_extrema 0.498272, bow_avg 0.904194, bow_greedy 0.797956, inter_dist1 0.016894, inter_dist2 0.060250 avg_len 9.941543\n",
      " time: 137.2 s\n",
      "Done testing\n",
      "Epoch:  9\n",
      "Train begins with 2393 batches with 14 left over samples\n",
      "[9 50] loss: 9.519946 time: 1.4 s\n",
      "[9 100] loss: 8.996950 time: 1.3 s\n",
      "[9 150] loss: 9.067976 time: 1.3 s\n",
      "[9 200] loss: 8.890422 time: 1.4 s\n",
      "[9 250] loss: 10.235870 time: 1.3 s\n",
      "[9 300] loss: 8.906806 time: 1.4 s\n",
      "[9 350] loss: 9.239548 time: 1.3 s\n",
      "[9 400] loss: 8.668331 time: 1.3 s\n",
      "[9 450] loss: 9.069580 time: 1.4 s\n",
      "[9 500] loss: 8.999891 time: 1.3 s\n",
      "[9 550] loss: 8.568866 time: 1.3 s\n",
      "[9 600] loss: 7.865196 time: 1.3 s\n",
      "[9 650] loss: 7.423778 time: 1.3 s\n",
      "[9 700] loss: 7.347478 time: 1.3 s\n",
      "[9 750] loss: 7.796157 time: 1.2 s\n",
      "[9 800] loss: 7.541363 time: 1.3 s\n",
      "[9 850] loss: 8.195587 time: 1.3 s\n",
      "[9 900] loss: 7.504948 time: 1.3 s\n",
      "[9 950] loss: 9.102717 time: 1.3 s\n",
      "[9 1000] loss: 8.264312 time: 1.2 s\n",
      "[9 1050] loss: 8.225968 time: 1.2 s\n",
      "[9 1100] loss: 9.051116 time: 1.2 s\n",
      "[9 1150] loss: 8.827686 time: 1.2 s\n",
      "[9 1200] loss: 8.319561 time: 1.2 s\n",
      "[9 1250] loss: 8.790779 time: 1.2 s\n",
      "[9 1300] loss: 8.681945 time: 1.3 s\n",
      "[9 1350] loss: 9.263363 time: 1.2 s\n",
      "[9 1400] loss: 8.685366 time: 1.2 s\n",
      "[9 1450] loss: 7.636237 time: 1.2 s\n",
      "[9 1500] loss: 8.347250 time: 1.2 s\n",
      "[9 1550] loss: 8.861118 time: 1.2 s\n",
      "[9 1600] loss: 8.886399 time: 1.1 s\n",
      "[9 1650] loss: 8.840015 time: 1.1 s\n",
      "[9 1700] loss: 9.082252 time: 1.2 s\n",
      "[9 1750] loss: 8.871755 time: 1.2 s\n",
      "[9 1800] loss: 8.595883 time: 1.2 s\n",
      "[9 1850] loss: 7.962820 time: 1.1 s\n",
      "[9 1900] loss: 9.271112 time: 1.1 s\n",
      "[9 1950] loss: 9.145852 time: 1.0 s\n",
      "[9 2000] loss: 8.606786 time: 1.1 s\n",
      "[9 2050] loss: 8.251614 time: 1.1 s\n",
      "[9 2100] loss: 8.663972 time: 1.1 s\n",
      "[9 2150] loss: 7.974733 time: 1.0 s\n",
      "[9 2200] loss: 8.316153 time: 1.0 s\n",
      "[9 2250] loss: 8.225663 time: 1.0 s\n",
      "[9 2300] loss: 7.905291 time: 1.0 s\n",
      "[9 2350] loss: 8.038229 time: 1.0 s\n",
      "Evaluating....\n",
      "Valid begins with 367 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 9]. loss: 27.496889 time: 58.6 s\n",
      "************************************************************\n",
      "Test begins with 6740 batches with 0 left over samples\n",
      "true response:  i ' m sorry , but mr . johnson is out at the moment . can i take </s>\n",
      "generate response:  yes , i ' m sorry . i ' ve got a problem with you . </s>\n",
      "true response:  nonsmoking , please . </s>\n",
      "generate response:  yes , please . </s>\n",
      "true response:  well , i wanted to let you know that i ' ve put in my notice . </s>\n",
      "generate response:  i ' m going to the pub tonight . </s>\n",
      "true response:  can i return the car in san francisco ? </s>\n",
      "generate response:  thanks a lot . </s>\n",
      "true response:  you will need to pay late fees on these books . </s>\n",
      "generate response:  i ’ m sorry . i didn ’ t know that . but you see , it was </s>\n",
      "true response:  that sounds nice . </s>\n",
      "generate response:  thanks a lot . </s>\n",
      "true response:  i can ’ t wait to vote . </s>\n",
      "generate response:  i ’ m afraid i can ’ t . i ’ m sure you ’ re late . </s>\n",
      "true response:  it doesn ' t matter . it happens to everyone . </s>\n",
      "generate response:  i ' m sorry . i didn ' t realize that . </s>\n",
      "true response:  well , it ' s specially made of a platinum <unk> , sir , and the <unk> will </s>\n",
      "generate response:  it ' s a <unk> . </s>\n",
      "true response:  no , that won ’ t be necessary . </s>\n",
      "generate response:  ok , thank you . </s>\n",
      "true response:  let me give you a little advice . you know what the <unk> are going to be , </s>\n",
      "generate response:  why ? </s>\n",
      "true response:  well , it looks like his encouragement paid off rebecca . so how about extracurricular activities at university </s>\n",
      "generate response:  i ’ m sorry . i didn ’ t realize that . but i don ’ t </s>\n",
      "true response:  i love dogs . they have been used as guards for centuries . nowadays , they are often </s>\n",
      "generate response:  i think you are a good dancer . i think it ' s a sheer designer . </s>\n",
      "true response:  <unk> , yes . there ’ s a big cavity . </s>\n",
      "generate response:  oh , that ' s a good idea . </s>\n",
      "true response:  sorry , he is out . </s>\n",
      "generate response:  i ’ m sorry , he ’ s out . </s>\n",
      "true response:  good morning . i ’ m here to see <unk> davis , the human resources manager . </s>\n",
      "generate response:  yes . i ' d like to make a reservation to los angeles on sep . 19th . </s>\n",
      "true response:  yes , stones . </s>\n",
      "generate response:  i think they are a good dancer . </s>\n",
      "true response:  a :: tim , please . please be seated . </s>\n",
      "generate response:  i ' m phoning about it . </s>\n",
      "true response:  we ' d like to invite you for our dress party tomorrow evening , are you free ? </s>\n",
      "generate response:  ok , i ' ll go home . </s>\n",
      "true response:  look , that bamboo <unk> are <unk> . let ' s go and play there . </s>\n",
      "generate response:  i ' m going to the pub tonight . </s>\n",
      "true response:  christine , i know you ’ re new here and there ’ s a lot to learn , </s>\n",
      "generate response:  that ’ s a good idea . i ’ m sure you ’ ll have a good </s>\n",
      "true response:  it looks like the shipping company did this . </s>\n",
      "generate response:  i ' m sorry . i ' m not sure . </s>\n",
      "Avg recall BLEU 0.256347, bow_extrema 0.497604, bow_avg 0.905423, bow_greedy 0.796370, inter_dist1 0.018257, inter_dist2 0.066712 avg_len 10.182938\n",
      " time: 194.4 s\n",
      "Done testing\n",
      "Epoch:  10\n",
      "Train begins with 2393 batches with 14 left over samples\n",
      "[10 50] loss: 8.860748 time: 1.3 s\n",
      "[10 100] loss: 8.298804 time: 1.3 s\n",
      "[10 150] loss: 8.474808 time: 1.3 s\n",
      "[10 200] loss: 8.280015 time: 1.3 s\n",
      "[10 250] loss: 9.509963 time: 1.2 s\n",
      "[10 300] loss: 8.285617 time: 1.3 s\n",
      "[10 350] loss: 8.608525 time: 1.3 s\n",
      "[10 400] loss: 8.124917 time: 1.3 s\n",
      "[10 450] loss: 8.419596 time: 1.2 s\n",
      "[10 500] loss: 8.362064 time: 1.2 s\n",
      "[10 550] loss: 8.002342 time: 1.2 s\n",
      "[10 600] loss: 7.369409 time: 1.2 s\n",
      "[10 650] loss: 7.036002 time: 1.2 s\n",
      "[10 700] loss: 6.941717 time: 1.2 s\n",
      "[10 750] loss: 7.358238 time: 1.2 s\n",
      "[10 800] loss: 7.104552 time: 1.2 s\n",
      "[10 850] loss: 7.710799 time: 1.2 s\n",
      "[10 900] loss: 7.040130 time: 1.2 s\n",
      "[10 950] loss: 8.471759 time: 1.2 s\n",
      "[10 1000] loss: 7.651846 time: 1.2 s\n",
      "[10 1050] loss: 7.626677 time: 1.1 s\n",
      "[10 1100] loss: 8.435189 time: 1.3 s\n",
      "[10 1150] loss: 8.271054 time: 1.2 s\n",
      "[10 1200] loss: 7.797826 time: 1.2 s\n",
      "[10 1250] loss: 8.210612 time: 1.2 s\n",
      "[10 1300] loss: 8.188209 time: 1.2 s\n",
      "[10 1350] loss: 8.598250 time: 1.2 s\n",
      "[10 1400] loss: 8.141479 time: 1.2 s\n",
      "[10 1450] loss: 7.155428 time: 1.2 s\n",
      "[10 1500] loss: 7.826657 time: 1.2 s\n",
      "[10 1550] loss: 8.243433 time: 1.2 s\n",
      "[10 1600] loss: 8.314953 time: 1.2 s\n",
      "[10 1650] loss: 8.234868 time: 1.2 s\n",
      "[10 1700] loss: 8.515177 time: 1.1 s\n",
      "[10 1750] loss: 8.362766 time: 1.1 s\n",
      "[10 1800] loss: 8.033909 time: 1.1 s\n",
      "[10 1850] loss: 7.457763 time: 1.2 s\n",
      "[10 1900] loss: 8.683533 time: 1.1 s\n",
      "[10 1950] loss: 8.510348 time: 1.1 s\n",
      "[10 2000] loss: 8.078644 time: 1.1 s\n",
      "[10 2050] loss: 7.775921 time: 1.1 s\n",
      "[10 2100] loss: 8.152692 time: 1.1 s\n",
      "[10 2150] loss: 7.479812 time: 1.1 s\n",
      "[10 2200] loss: 7.847163 time: 1.1 s\n",
      "[10 2250] loss: 7.775097 time: 1.0 s\n",
      "[10 2300] loss: 7.512008 time: 1.0 s\n",
      "[10 2350] loss: 7.574523 time: 1.0 s\n",
      "Evaluating....\n",
      "Valid begins with 367 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 10]. loss: 28.474029 time: 57.1 s\n",
      "************************************************************\n",
      "Test begins with 6740 batches with 0 left over samples\n",
      "true response:  i ' m sorry , but mr . johnson is out at the moment . can i take </s>\n",
      "generate response:  i ' m sorry . i ' ve got to call you . </s>\n",
      "true response:  nonsmoking , please . </s>\n",
      "generate response:  i ' ll have a look . </s>\n",
      "true response:  well , i wanted to let you know that i ' ve put in my notice . </s>\n",
      "generate response:  i ' m going to the pub tonight . </s>\n",
      "true response:  can i return the car in san francisco ? </s>\n",
      "generate response:  i ' m sorry . i didn ' t mean to . but i think i ' ll </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  you will need to pay late fees on these books . </s>\n",
      "generate response:  i ’ ll take it . </s>\n",
      "true response:  that sounds nice . </s>\n",
      "generate response:  thanks a lot . </s>\n",
      "true response:  i can ’ t wait to vote . </s>\n",
      "generate response:  i ’ m sorry . i ’ ve got to go . </s>\n",
      "true response:  it doesn ' t matter . it happens to everyone . </s>\n",
      "generate response:  i ' m sorry . i didn ' t know you . </s>\n",
      "true response:  well , it ' s specially made of a platinum <unk> , sir , and the <unk> will </s>\n",
      "generate response:  it ' s a <unk> . </s>\n",
      "true response:  no , that won ’ t be necessary . </s>\n",
      "generate response:  thank you . i ’ ll have a look . </s>\n",
      "true response:  let me give you a little advice . you know what the <unk> are going to be , </s>\n",
      "generate response:  why ? </s>\n",
      "true response:  well , it looks like his encouragement paid off rebecca . so how about extracurricular activities at university </s>\n",
      "generate response:  i ’ m sorry . i ’ ve got to go now . </s>\n",
      "true response:  i love dogs . they have been used as guards for centuries . nowadays , they are often </s>\n",
      "generate response:  i think you ' ll have a good life to make a perfect choice . </s>\n",
      "true response:  <unk> , yes . there ’ s a big cavity . </s>\n",
      "generate response:  i ' m sorry . i didn ' t mean to . but i think you ' d </s>\n",
      "true response:  sorry , he is out . </s>\n",
      "generate response:  this is kevin . </s>\n",
      "true response:  good morning . i ’ m here to see <unk> davis , the human resources manager . </s>\n",
      "generate response:  yes . i ' d like to buy a sweater . </s>\n",
      "true response:  yes , stones . </s>\n",
      "generate response:  yes , of course . i think they are a bit anxious for them . </s>\n",
      "true response:  a :: tim , please . please be seated . </s>\n",
      "generate response:  happy birthday , mary . </s>\n",
      "true response:  we ' d like to invite you for our dress party tomorrow evening , are you free ? </s>\n",
      "generate response:  how about a salesman ? </s>\n",
      "true response:  look , that bamboo <unk> are <unk> . let ' s go and play there . </s>\n",
      "generate response:  i ' m sorry . i ' ve got to run . </s>\n",
      "true response:  christine , i know you ’ re new here and there ’ s a lot to learn , </s>\n",
      "generate response:  oh , that ’ s nice to hear that . i ’ m sure that ’ s what </s>\n",
      "true response:  it looks like the shipping company did this . </s>\n",
      "generate response:  i ' m sorry , but i ' ve got to run . </s>\n",
      "Avg recall BLEU 0.260683, bow_extrema 0.492004, bow_avg 0.902088, bow_greedy 0.792491, inter_dist1 0.018835, inter_dist2 0.070476 avg_len 10.555638\n",
      " time: 137.7 s\n",
      "Done testing\n"
     ]
    }
   ],
   "source": [
    "metrics = Metrics(corpus.word2vec)\n",
    "model = HRED(config, n_tokens)\n",
    "if corpus.word2vec is not None:\n",
    "    print(\"Loaded word2vec\")\n",
    "    model.embedder.weight.data.copy_(torch.from_numpy(corpus.word2vec))\n",
    "    model.embedder.weight.data[0].fill_(0)\n",
    "model.to(DEVICE)\n",
    "parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(parameters, lr=config['lr'])\n",
    "\n",
    "model.zero_grad()\n",
    "print_every = 50\n",
    "best_state = None\n",
    "max_metric = 0\n",
    "for epoch in range(10):\n",
    "    print('Epoch: ', epoch+1)\n",
    "    train_loader.epoch_init(32, config['diaglen'], 1, shuffle=False)\n",
    "    n_iters=train_loader.num_batch\n",
    "    total_loss = 0.0\n",
    "    epoch_begin = time()\n",
    "    batch_count = 0\n",
    "    batch_begin_time = time()\n",
    "    total_train_batch = 0 # 记录训练的样本数量\n",
    "    total_valid_batch = 0 # 记录测试的样本数量\n",
    "    # 分别用来记录训练时候，生成器最顶层的梯度，最底层的梯度以及判别器最顶层的梯度\n",
    "    train_grad_G_top_layer = []\n",
    "    train_grad_G_bottom_layer = []\n",
    "    train_grad_D_top_layer = []\n",
    "    while True:\n",
    "        loss_records=[]\n",
    "        batch = train_loader.next_batch()\n",
    "        total_train_batch += 32\n",
    "        if batch is None:\n",
    "#         if batch is None or total_train_batch >= 1000: # end of epoch\n",
    "            break\n",
    "        context, context_lens, utt_lens, floors,_,_,_,response,res_lens,_ = batch\n",
    "        context, utt_lens = context[:,:,1:], utt_lens-1 # remove the sos token in the context and reduce the context length\n",
    "        context, context_lens, utt_lens, floors, response, res_lens\\\n",
    "                = gVar(context), gVar(context_lens), gVar(utt_lens), gData(floors), gVar(response), gVar(res_lens)\n",
    "        loss_batch = train(context, context_lens, utt_lens, floors, response, res_lens)\n",
    "        total_loss += float(loss_batch)\n",
    "        batch_count += 1\n",
    "        if batch_count % print_every == 0:\n",
    "            print_flush('[%d %d] loss: %.6f time: %.1f s' %\n",
    "                  (epoch + 1, batch_count, np.exp(total_loss / print_every), time() - batch_begin_time))\n",
    "            total_loss = 0.0\n",
    "            batch_begin_time = time()\n",
    "#         train_grad_G_top_layer.append(torch.mean(model.decoder.rnn.weight_hh_l0.grad))\n",
    "#         train_grad_G_bottom_layer.append(torch.mean(model.generator.embedding.weight.grad))\n",
    "#         train_grad_G_bottom_layer.append(torch.mean(model.decoder.rnn.weight_hh_l0.grad))\n",
    "#     plot_gradient(train_grad_G_top_layer, 'G top layer')\n",
    "#     plot_gradient(train_grad_G_bottom_layer, 'G bottom layer')\n",
    "    print_flush(\"Evaluating....\")\n",
    "    valid_loader.epoch_init(20, config['diaglen'], 1, shuffle=False)\n",
    "    loss_valid = valid_small(valid_loader)\n",
    "#     valid_result.append(F1)\n",
    "    print_flush('*'*60)\n",
    "    print_flush('[epoch %d]. loss: %.6f time: %.1f s'%(epoch+1, np.exp(loss_valid), time()-epoch_begin))\n",
    "    print_flush('*'*60)\n",
    "#     print_flush(\"testing....\")\n",
    "#     test_loader.epoch_init(1, config['diaglen'], 1, shuffle=False)\n",
    "#     loss_valid = valid(test_loader)\n",
    "#     print_flush('*'*60)\n",
    "#     print_flush('[epoch %d]. loss: %.6f time: %.1f s'%(epoch+1, np.exp(loss_valid), time()-epoch_begin))\n",
    "#     print_flush('*'*60)\n",
    "    recall_bleu, bow_extrema, bow_avg, bow_greedy, inter_dist1, inter_dist2, avg_len\\\n",
    "     =evaluate(model, metrics, test_loader, vocab, ivocab, repeat=10)\n",
    "    epoch_begin = time()\n",
    "#     if F1 > max_metric:\n",
    "#         best_state = model.state_dict()\n",
    "#         max_metric = F1\n",
    "#         print_flush(\"save model...\")\n",
    "#         torch.save(best_state, '../datasets/models/baseline_LSTM.pth')\n",
    "#     epoch_begin = time()\n",
    "#     if training_termination(valid_result):\n",
    "#         print_flush(\"early stop at [%d] epoch!\" % (epoch+1))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (liangjiahui)",
   "language": "python",
   "name": "liangjiahui"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
