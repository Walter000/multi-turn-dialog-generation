{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.init as weight_init\n",
    "import numpy as np\n",
    "from time import time\n",
    "import data\n",
    "import sys\n",
    "from metrics import Metrics\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "DEVICE = torch.device('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_HRED():\n",
    "    conf = {\n",
    "    'maxlen':30, # maximum utterance length\n",
    "    'diaglen':10, # how many utterance kept in the context window\n",
    "\n",
    "# Model Arguments\n",
    "    'emb_size':200, # size of word embeddings\n",
    "    'n_hidden':300, # number of hidden units per layer\n",
    "    'n_layers':1, # number of layers\n",
    "    'noise_radius':0.2, # stdev of noise for autoencoder (regularizer)\n",
    "    'z_size':200, # dimension of z # 300 performs worse\n",
    "    'lambda_gp':10, # Gradient penalty lambda hyperparameter.\n",
    "    'temp':1.0, # softmax temperature (lower --> more discrete)\n",
    "    'dropout':0.5, # dropout applied to layers (0 = no dropout)\n",
    "\n",
    "# Training Arguments\n",
    "    'batch_size':32,\n",
    "    'epochs':100, # maximum number of epochs\n",
    "    'min_epochs':2, # minimum number of epochs to train for\n",
    "\n",
    "    'n_iters_d':5, # number of discriminator iterations in training\n",
    "    'lr_ae':1.0, # autoencoder learning rate\n",
    "    'lr_gan_g':5e-05, # generator learning rate\n",
    "    'lr_gan_d':1e-05, # critic/discriminator learning rate\n",
    "    'beta1':0.9, # beta1 for adam\n",
    "    'clip':1.0,  # gradient clipping, max norm\n",
    "    'gan_clamp':0.01,  # WGAN clamp (Do not use clamp when you apply gradient penelty             \n",
    "    }\n",
    "    return conf \n",
    "\n",
    "config = config_HRED()\n",
    "random.seed(1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gData(data):\n",
    "    tensor=data\n",
    "    if isinstance(data, np.ndarray):\n",
    "        tensor = torch.from_numpy(data)\n",
    "    tensor=tensor.to(DEVICE)\n",
    "    return tensor\n",
    "def gVar(data):\n",
    "    return gData(data)\n",
    "def print_flush(data, args=None):\n",
    "    if args == None:\n",
    "        print(data)\n",
    "    else:\n",
    "        print(data, args)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def indexes2sent(indexes, vocab, eos_tok, ignore_tok=0): \n",
    "    '''indexes: numpy array'''\n",
    "    def revert_sent(indexes, ivocab, eos_tok, ignore_tok=0):\n",
    "        toks=[]\n",
    "        length=0\n",
    "        indexes=filter(lambda i: i!=ignore_tok, indexes)\n",
    "        for idx in indexes:\n",
    "            toks.append(ivocab[idx])\n",
    "            length+=1\n",
    "            if idx == eos_tok:\n",
    "                break\n",
    "        return ' '.join(toks), length\n",
    "    \n",
    "    ivocab = {v: k for k, v in vocab.items()}\n",
    "    if indexes.ndim==1:# one sentence\n",
    "        return revert_sent(indexes, ivocab, eos_tok, ignore_tok)\n",
    "    else:# dim>1\n",
    "        sentences=[] # a batch of sentences\n",
    "        lens=[]\n",
    "        for inds in indexes:\n",
    "            sentence, length = revert_sent(inds, ivocab, eos_tok, ignore_tok)\n",
    "            sentences.append(sentence)\n",
    "            lens.append(length)\n",
    "        return sentences, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedder, input_size, hidden_size, bidirectional, n_layers, noise_radius=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.noise_radius=noise_radius\n",
    "        self.n_layers = n_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        assert type(self.bidirectional)==bool\n",
    "        self.embedding = embedder\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, n_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for w in self.rnn.parameters(): \n",
    "            if w.dim()>1:\n",
    "                weight_init.orthogonal_(w)\n",
    "                \n",
    "    def store_grad_norm(self, grad):\n",
    "        norm = torch.norm(grad, 2, 1)\n",
    "        self.grad_norm = norm.detach().data.mean()\n",
    "        return grad\n",
    "    \n",
    "    def forward(self, inputs, input_lens=None, noise=False): \n",
    "        if self.embedding is not None:\n",
    "            inputs=self.embedding(inputs) \n",
    "        \n",
    "        batch_size, seq_len, emb_size=inputs.size()\n",
    "#         inputs=F.dropout(inputs, 0.5, self.training)\n",
    "        \n",
    "        if input_lens is not None:\n",
    "            input_lens_sorted, indices = input_lens.sort(descending=True)\n",
    "            inputs_sorted = inputs.index_select(0, indices)        \n",
    "            inputs = pack_padded_sequence(inputs_sorted, input_lens_sorted.data.tolist(), batch_first=True)\n",
    "            \n",
    "        init_hidden = gVar(torch.zeros(self.n_layers*(1+self.bidirectional), batch_size, self.hidden_size))\n",
    "        hids, h_n = self.rnn(inputs, init_hidden) \n",
    "        if input_lens is not None:\n",
    "            _, inv_indices = indices.sort()\n",
    "            hids, lens = pad_packed_sequence(hids, batch_first=True)     \n",
    "            hids = hids.index_select(0, inv_indices)\n",
    "            h_n = h_n.index_select(1, inv_indices)\n",
    "        h_n = h_n.view(self.n_layers, (1+self.bidirectional), batch_size, self.hidden_size)\n",
    "        h_n = h_n[-1]\n",
    "        enc = h_n.transpose(1,0).contiguous().view(batch_size,-1) \n",
    "#         if noise and self.noise_radius > 0:\n",
    "#             gauss_noise = gVar(torch.normal(means=torch.zeros(enc.size()),std=self.noise_radius))\n",
    "#             enc = enc + gauss_noise\n",
    "            \n",
    "        return enc, hids\n",
    "    \n",
    "class ContextEncoder(nn.Module):\n",
    "    def __init__(self, utt_encoder, input_size, hidden_size, n_layers=1, noise_radius=0.2):\n",
    "        super(ContextEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.noise_radius=noise_radius\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.utt_encoder=utt_encoder\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for w in self.rnn.parameters(): # initialize the gate weights with orthogonal\n",
    "            if w.dim()>1:\n",
    "                weight_init.orthogonal_(w)\n",
    "    \n",
    "    def store_grad_norm(self, grad):\n",
    "        norm = torch.norm(grad, 2, 1)\n",
    "        self.grad_norm = norm.detach().data.mean()\n",
    "        return grad\n",
    "\n",
    "    def forward(self, context, context_lens, utt_lens, floors, noise=False): \n",
    "        batch_size, max_context_len, max_utt_len = context.size()\n",
    "        utts=context.view(-1, max_utt_len) \n",
    "        utt_lens=utt_lens.view(-1)\n",
    "        utt_encs,_ = self.utt_encoder(utts, utt_lens) \n",
    "        utt_encs = utt_encs.view(batch_size, max_context_len, -1)\n",
    "        floor_one_hot = gVar(torch.zeros(floors.numel(), 2))\n",
    "        floor_one_hot.data.scatter_(1, floors.view(-1, 1), 1)\n",
    "        floor_one_hot = floor_one_hot.view(-1, max_context_len, 2)\n",
    "        utt_floor_encs = torch.cat([utt_encs, floor_one_hot], 2) \n",
    "        \n",
    "#         utt_floor_encs=F.dropout(utt_floor_encs, 0.25, self.training)\n",
    "        context_lens_sorted, indices = context_lens.sort(descending=True)\n",
    "        utt_floor_encs = utt_floor_encs.index_select(0, indices)\n",
    "        utt_floor_encs = pack_padded_sequence(utt_floor_encs, context_lens_sorted.data.tolist(), batch_first=True)\n",
    "        \n",
    "        init_hidden=gVar(torch.zeros(1, batch_size, self.hidden_size))\n",
    "        hids, h_n = self.rnn(utt_floor_encs, init_hidden)\n",
    "        \n",
    "        _, inv_indices = indices.sort()\n",
    "        h_n = h_n.index_select(1, inv_indices)  \n",
    "        enc = h_n.transpose(1,0).contiguous().view(batch_size, -1)\n",
    "\n",
    "#         if noise and self.noise_radius > 0:\n",
    "#             gauss_noise = gVar(torch.normal(means=torch.zeros(enc.size()),std=self.noise_radius))\n",
    "#             enc = enc + gauss_noise\n",
    "        return enc\n",
    "class Variation(nn.Module):\n",
    "    def __init__(self, input_size, z_size):\n",
    "        super(Variation, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.z_size=z_size   \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, z_size),\n",
    "            nn.BatchNorm1d(z_size, eps=1e-05, momentum=0.1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(z_size, z_size),\n",
    "            nn.BatchNorm1d(z_size, eps=1e-05, momentum=0.1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.context_to_mu=nn.Linear(z_size, z_size) # activation???\n",
    "        self.context_to_logsigma=nn.Linear(z_size, z_size) \n",
    "        \n",
    "        self.fc.apply(self.init_weights)\n",
    "        self.init_weights(self.context_to_mu)\n",
    "        self.init_weights(self.context_to_logsigma)\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):        \n",
    "            m.weight.data.uniform_(-0.02, 0.02)\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, context):\n",
    "        batch_size,_=context.size()\n",
    "        context = self.fc(context)\n",
    "        mu=self.context_to_mu(context)\n",
    "        logsigma = self.context_to_logsigma(context) \n",
    "        std = torch.exp(0.5 * logsigma)\n",
    "        \n",
    "        epsilon = gVar(torch.randn([batch_size, self.z_size]))\n",
    "        z = epsilon * std + mu  \n",
    "        return z, mu, logsigma \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedder, input_size, hidden_size, vocab_size, n_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.input_size= input_size \n",
    "        self.hidden_size = hidden_size \n",
    "        self.vocab_size = vocab_size \n",
    "        self.embedding = embedder\n",
    "#         self.linear = nn.Linear(500, hidden_size)\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        for w in self.rnn.parameters():\n",
    "            if w.dim()>1:\n",
    "                weight_init.orthogonal_(w)\n",
    "        self.out.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out.bias.data.fill_(0)\n",
    "    \n",
    "    def forward(self, init_hidden, context=None, inputs=None, lens=None):\n",
    "        batch_size, maxlen = inputs.size()\n",
    "        if self.embedding is not None:\n",
    "            inputs = self.embedding(inputs)\n",
    "        if context is not None:\n",
    "            repeated_context = context.unsqueeze(1).repeat(1, maxlen, 1)\n",
    "            inputs = torch.cat([inputs, repeated_context], 2)\n",
    "#         inputs = F.dropout(inputs, 0.5, self.training)  ß\n",
    "#         init_hidden = self.linear(init_hidden)\n",
    "        hids, h_n = self.rnn(inputs, init_hidden.unsqueeze(0))\n",
    "        decoded = self.out(hids.contiguous().view(-1, self.hidden_size))# reshape before linear over vocab\n",
    "        decoded = decoded.view(batch_size, maxlen, self.vocab_size)\n",
    "        return decoded\n",
    "    \n",
    "    def sampling(self, init_hidden, context, maxlen, SOS_tok, EOS_tok, mode='greedy'):\n",
    "        batch_size=init_hidden.size(0)\n",
    "#         init_hidden = self.linear(init_hidden)\n",
    "        decoded_words = np.zeros((batch_size, maxlen), dtype=np.int)\n",
    "        sample_lens=np.zeros(batch_size, dtype=np.int)         \n",
    "        decoder_input = gVar(torch.LongTensor([[SOS_tok]*batch_size]).view(batch_size,1))\n",
    "        decoder_input = self.embedding(decoder_input) if self.embedding is not None else decoder_input \n",
    "        decoder_input = torch.cat([decoder_input, context.unsqueeze(1)],2) if context is not None else decoder_input\n",
    "        decoder_hidden = init_hidden.unsqueeze(0).contiguous()\n",
    "        for di in range(maxlen):\n",
    "            decoder_output, decoder_hidden = self.rnn(decoder_input, decoder_hidden)\n",
    "            decoder_output=self.out(decoder_output)\n",
    "            if mode=='greedy':\n",
    "                topi = decoder_output[:,-1].max(1, keepdim=True)[1] \n",
    "            elif mode=='sample':\n",
    "                topi = torch.multinomial(F.softmax(decoder_output[:,-1], dim=1), 1)                    \n",
    "            decoder_input = self.embedding(topi) if self.embedding is not None else topi\n",
    "            decoder_input = torch.cat([decoder_input, context.unsqueeze(1)],2) if context is not None else decoder_input\n",
    "            ni = topi.squeeze().data.cpu().numpy() \n",
    "            decoded_words[:,di]=ni\n",
    "                      \n",
    "        for i in range(batch_size):\n",
    "            for word in decoded_words[i]:\n",
    "                if word == EOS_tok:\n",
    "                    break\n",
    "                sample_lens[i]=sample_lens[i]+1\n",
    "        return decoded_words, sample_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = gData(torch.FloatTensor([1]))\n",
    "minus_one = one * -1    \n",
    "class DialogWAE(nn.Module):\n",
    "    def __init__(self, config, vocab_size, PAD_token=0):\n",
    "        super(DialogWAE, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.maxlen=config['maxlen']\n",
    "        self.clip = config['clip']\n",
    "        self.lambda_gp = config['lambda_gp']\n",
    "        self.temp=config['temp']\n",
    "        \n",
    "        self.embedder= nn.Embedding(vocab_size, config['emb_size'], padding_idx=PAD_token)\n",
    "        self.utt_encoder = Encoder(self.embedder, config['emb_size'], config['n_hidden'], \n",
    "                                   True, config['n_layers'], config['noise_radius']) \n",
    "        self.context_encoder = ContextEncoder(self.utt_encoder, config['n_hidden']*2+2, config['n_hidden'], 1, config['noise_radius']) \n",
    "        self.prior_net = Variation(config['n_hidden'], config['z_size']) # p(e|c)\n",
    "        self.post_net = Variation(config['n_hidden']*3, config['z_size']) # q(e|c,x)\n",
    "        \n",
    "        self.post_generator = nn.Sequential( \n",
    "            nn.Linear(config['z_size'], config['z_size']),\n",
    "            nn.BatchNorm1d(config['z_size'], eps=1e-05, momentum=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config['z_size'], config['z_size']),\n",
    "            nn.BatchNorm1d(config['z_size'], eps=1e-05, momentum=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config['z_size'], config['z_size'])\n",
    "        )\n",
    "        self.post_generator.apply(self.init_weights)\n",
    "                                                                              \n",
    "        self.prior_generator = nn.Sequential( \n",
    "            nn.Linear(config['z_size'], config['z_size']),\n",
    "            nn.BatchNorm1d(config['z_size'], eps=1e-05, momentum=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config['z_size'], config['z_size']),\n",
    "            nn.BatchNorm1d(config['z_size'], eps=1e-05, momentum=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config['z_size'], config['z_size'])\n",
    "        ) \n",
    "        self.prior_generator.apply(self.init_weights)\n",
    "                                                                                             \n",
    "        self.decoder = Decoder(self.embedder, config['emb_size'], config['n_hidden']+config['z_size'], \n",
    "                               vocab_size, n_layers=1) \n",
    "        \n",
    "        self.discriminator = nn.Sequential(  \n",
    "            nn.Linear(config['n_hidden']+config['z_size'], config['n_hidden']*2),\n",
    "            nn.BatchNorm1d(config['n_hidden']*2, eps=1e-05, momentum=0.1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(config['n_hidden']*2, config['n_hidden']*2),\n",
    "            nn.BatchNorm1d(config['n_hidden']*2, eps=1e-05, momentum=0.1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(config['n_hidden']*2, 1),\n",
    "        )\n",
    "        self.discriminator.apply(self.init_weights)\n",
    "        \n",
    "           \n",
    "        self.optimizer_AE = optim.SGD(list(self.context_encoder.parameters())\n",
    "                                      +list(self.post_net.parameters())\n",
    "                                      +list(self.post_generator.parameters())\n",
    "                                      +list(self.decoder.parameters()),lr=config['lr_ae'])\n",
    "        self.optimizer_G = optim.RMSprop(list(self.post_net.parameters())\n",
    "                                      +list(self.post_generator.parameters())\n",
    "                                      +list(self.prior_net.parameters())\n",
    "                                      +list(self.prior_generator.parameters()), lr=config['lr_gan_g'])\n",
    "        self.optimizer_D = optim.RMSprop(self.discriminator.parameters(), lr=config['lr_gan_d'])\n",
    "        \n",
    "        self.lr_scheduler_AE = optim.lr_scheduler.StepLR(self.optimizer_AE, step_size = 10, gamma=0.6)\n",
    "        \n",
    "        self.criterion_ce = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):        \n",
    "            m.weight.data.uniform_(-0.02, 0.02)\n",
    "            m.bias.data.fill_(0)\n",
    "            \n",
    "    def sample_code_post(self, x, c):\n",
    "        e, _, _ = self.post_net(torch.cat((x, c),1))\n",
    "        z = self.post_generator(e)\n",
    "        return z\n",
    "   \n",
    "    def sample_code_prior(self, c):\n",
    "        e, _, _ = self.prior_net(c)\n",
    "        z = self.prior_generator(e)\n",
    "        return z    \n",
    "    \n",
    "    def train_AE(self, context, context_lens, utt_lens, floors, response, res_lens):\n",
    "        self.context_encoder.train()\n",
    "        self.decoder.train()\n",
    "        c = self.context_encoder(context, context_lens, utt_lens, floors)\n",
    "        x,_ = self.utt_encoder(response[:,1:], res_lens-1)      \n",
    "        z = self.sample_code_post(x, c)\n",
    "        output = self.decoder(torch.cat((z, c),1), None, response[:,:-1], (res_lens-1))  \n",
    "        flattened_output = output.view(-1, self.vocab_size) \n",
    "        \n",
    "        dec_target = response[:,1:].contiguous().view(-1)\n",
    "        mask = dec_target.gt(0) # [(batch_sz*seq_len)]\n",
    "        masked_target = dec_target.masked_select(mask) # \n",
    "        output_mask = mask.unsqueeze(1).expand(mask.size(0), self.vocab_size)# [(batch_sz*seq_len) x n_tokens]\n",
    "        masked_output = flattened_output.masked_select(output_mask).view(-1, self.vocab_size)\n",
    "        \n",
    "        self.optimizer_AE.zero_grad()\n",
    "        loss = self.criterion_ce(masked_output/self.temp, masked_target)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(list(self.context_encoder.parameters())+list(self.decoder.parameters()), self.clip)\n",
    "        self.optimizer_AE.step()\n",
    "\n",
    "        return [('train_loss_AE', loss.item())]\n",
    "    \n",
    "    def train_G(self, context, context_lens, utt_lens, floors, response, res_lens): \n",
    "        self.context_encoder.eval()\n",
    "        self.optimizer_G.zero_grad()\n",
    "        \n",
    "        for p in self.discriminator.parameters():\n",
    "            p.requires_grad = False  \n",
    "        \n",
    "        c = self.context_encoder(context, context_lens, utt_lens, floors)\n",
    "        # -----------------posterior samples ---------------------------\n",
    "        x,_ = self.utt_encoder(response[:,1:], res_lens-1)\n",
    "        z_post= self.sample_code_post(x.detach(), c.detach())\n",
    "        errG_post = torch.mean(self.discriminator(torch.cat((z_post, c.detach()),1) ))\n",
    "        errG_post.backward(minus_one) \n",
    "    \n",
    "        # ----------------- prior samples ---------------------------\n",
    "        prior_z = self.sample_code_prior(c.detach()) \n",
    "        errG_prior = torch.mean(self.discriminator(torch.cat((prior_z, c.detach()),1)))\n",
    "        errG_prior.backward(one) \n",
    "    \n",
    "        self.optimizer_G.step()\n",
    "        \n",
    "        for p in self.discriminator.parameters():\n",
    "            p.requires_grad = True  \n",
    "        \n",
    "        costG = errG_prior - errG_post\n",
    "        return [('train_loss_G', costG.item())]\n",
    "    \n",
    "    def train_D(self, context, context_lens, utt_lens, floors, response, res_lens):\n",
    "        self.context_encoder.eval()\n",
    "        self.discriminator.train()\n",
    "        \n",
    "        self.optimizer_D.zero_grad()\n",
    "        \n",
    "        batch_size=context.size(0)\n",
    "\n",
    "        c = self.context_encoder(context, context_lens, utt_lens, floors)\n",
    "        x,_ = self.utt_encoder(response[:,1:], res_lens-1)\n",
    "        post_z = self.sample_code_post(x, c)\n",
    "        errD_post = torch.mean(self.discriminator(torch.cat((post_z.detach(), c.detach()),1)))\n",
    "        errD_post.backward(one)\n",
    " \n",
    "        prior_z = self.sample_code_prior(c) \n",
    "        errD_prior = torch.mean(self.discriminator(torch.cat((prior_z.detach(), c.detach()),1)))\n",
    "        errD_prior.backward(minus_one) \n",
    "    \n",
    "        alpha = gData(torch.rand(batch_size, 1))\n",
    "        alpha = alpha.expand(prior_z.size())\n",
    "        interpolates = alpha * prior_z.data + ((1 - alpha) * post_z.data)\n",
    "        interpolates = Variable(interpolates, requires_grad=True)\n",
    "        d_input=torch.cat((interpolates, c.detach()),1)\n",
    "        disc_interpolates = torch.mean(self.discriminator(d_input))\n",
    "        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                               grad_outputs=gData(torch.ones(disc_interpolates.size())),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "        gradient_penalty = ((gradients.contiguous().view(gradients.size(0),-1).norm(2,dim=1)-1)**2).mean()*self.lambda_gp\n",
    "        gradient_penalty.backward()\n",
    "    \n",
    "        self.optimizer_D.step()\n",
    "        costD = -(errD_prior - errD_post) + gradient_penalty\n",
    "        return [('train_loss_D', costD.item())]   \n",
    "    \n",
    "    def valid(self, context, context_lens, utt_lens, floors, response, res_lens):\n",
    "        self.context_encoder.eval()      \n",
    "        self.discriminator.eval()\n",
    "        self.decoder.eval()\n",
    "        \n",
    "        c = self.context_encoder(context, context_lens, utt_lens, floors)\n",
    "        x,_ = self.utt_encoder(response[:,1:], res_lens-1)\n",
    "        post_z = self.sample_code_post(x, c)\n",
    "        prior_z = self.sample_code_prior(c)\n",
    "        errD_post = torch.mean(self.discriminator(torch.cat((post_z, c),1)))\n",
    "        errD_prior = torch.mean(self.discriminator(torch.cat((prior_z, c),1)))\n",
    "        costD = -(errD_prior - errD_post)\n",
    "        costG = -costD \n",
    "        \n",
    "        dec_target = response[:,1:].contiguous().view(-1)\n",
    "        mask = dec_target.gt(0) # [(batch_sz*seq_len)]\n",
    "        masked_target = dec_target.masked_select(mask) \n",
    "        output_mask = mask.unsqueeze(1).expand(mask.size(0), self.vocab_size)\n",
    "        output = self.decoder(torch.cat((post_z, c),1), None, response[:,:-1], (res_lens-1)) \n",
    "        flattened_output = output.view(-1, self.vocab_size) \n",
    "        masked_output = flattened_output.masked_select(output_mask).view(-1, self.vocab_size)\n",
    "        lossAE = self.criterion_ce(masked_output/self.temp, masked_target)\n",
    "        return [('valid_loss_AE', lossAE.item()),('valid_loss_G', costG.item()), ('valid_loss_D', costD.item())]\n",
    "        \n",
    "    def sample(self, context, context_lens, utt_lens, floors, repeat, SOS_tok, EOS_tok):    \n",
    "        self.prior_net.eval()\n",
    "        self.prior_generator.eval()\n",
    "        self.context_encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        \n",
    "        c = self.context_encoder(context, context_lens, utt_lens, floors)\n",
    "#         c_repeated = c.expand(repeat, -1)\n",
    "        prior_z = self.sample_code_prior(c)\n",
    "        sample_words, sample_lens= self.decoder.sampling(torch.cat((prior_z, c),1), \n",
    "                                                         None, self.maxlen, SOS_tok, EOS_tok, \"greedy\") \n",
    "        return sample_words, sample_lens \n",
    "      \n",
    "    def adjust_lr(self):\n",
    "        self.lr_scheduler_AE.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max utt len 96, mean utt len 14.69\n",
      "Max utt len 75, mean utt len 15.06\n",
      "Max utt len 74, mean utt len 15.39\n",
      "Load corpus with train size 3, valid size 3, test size 3 raw vocab size 24497 vocab size 10000 at cut_off 4 OOV rate 0.008035\n",
      "<d> index 143\n",
      "<sil> index -1\n",
      "67 topics in train data\n",
      "['statement-non-opinion', 'acknowledge_(backchannel)', 'statement-opinion', 'abandoned_or_turn-exit/uninterpretable', 'yes-no-question', 'agree/accept', 'appreciation', 'wh-question', 'backchannel_in_question_form', 'yes_answers', 'conventional-closing', 'response_acknowledgement', 'open-question', 'no_answers', 'affirmative_non-yes_answers', 'declarative_yes-no-question', 'summarize/reformulate', 'other', 'action-directive', 'rhetorical-questions', 'conventional-opening', 'collaborative_completion', 'signal-non-understanding', 'or-clause', 'hold_before_answer/agreement', 'quotation', 'negative_non-no_answers', 'self-talk', 'apology', 'dispreferred_answers', 'offers,_options_commits', 'other_answers', 'reject', 'repeat-phrase', 'non-verbal', 'declarative_wh-question', 'thanking', 'hedge', 'maybe/accept-part', '3rd-party-talk', 'downplayer', 'tag-question']\n",
      "42 dialog acts in train data\n",
      "word2vec cannot cover 0.006599 vocab\n",
      "Done loading corpus\n",
      "Max len 265 and min len 10 and avg len 90.737910\n",
      "Max len 191 and min len 34 and avg len 88.083333\n",
      "Max len 207 and min len 14 and avg len 90.403226\n"
     ]
    }
   ],
   "source": [
    "corpus = getattr(data, 'SWDA'+'Corpus')('../datasets/SWDA/', wordvec_path='../datasets/'+'glove.twitter.27B.200d.txt', wordvec_dim=config['emb_size'])\n",
    "dials = corpus.get_dialogs()\n",
    "metas = corpus.get_metas()\n",
    "vocab = corpus.ivocab\n",
    "ivocab = corpus.vocab\n",
    "n_tokens = len(ivocab)\n",
    "train_dial, valid_dial, test_dial = dials.get(\"train\"), dials.get(\"valid\"), dials.get(\"test\")\n",
    "train_meta, valid_meta, test_meta = metas.get(\"train\"), metas.get(\"valid\"), metas.get(\"test\")\n",
    "train_loader = getattr(data, 'SWDA'+'DataLoader')(\"Train\", train_dial, train_meta, config['maxlen'])\n",
    "valid_loader = getattr(data, 'SWDA'+'DataLoader')(\"Valid\", valid_dial, valid_meta, config['maxlen'])\n",
    "test_loader = getattr(data, 'SWDA'+'DataLoader')(\"Test\", test_dial, test_meta, config['maxlen'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max utt len 296, mean utt len 16.48\n",
      "Max utt len 174, mean utt len 16.37\n",
      "Max utt len 214, mean utt len 16.68\n",
      "Load corpus with train size 2, valid size 2, test size 2 raw vocab size 17716 vocab size 10000 at cut_off 2 OOV rate 0.006757\n",
      "<d> index 21\n",
      "<sil> index -1\n",
      "word2vec cannot cover 0.032194 vocab\n",
      "Done loading corpus\n",
      "Max len 36 and min len 3 and avg len 8.840439\n",
      "Max len 32 and min len 3 and avg len 9.069000\n",
      "Max len 27 and min len 3 and avg len 8.740000\n"
     ]
    }
   ],
   "source": [
    "corpus = getattr(data, 'DailyDial'+'Corpus')('../datasets/DailyDial/', wordvec_path='../datasets/'+'glove.twitter.27B.200d.txt', wordvec_dim=config['emb_size'])\n",
    "dials = corpus.get_dialogs()\n",
    "metas = corpus.get_metas()\n",
    "vocab = corpus.ivocab\n",
    "ivocab = corpus.vocab\n",
    "n_tokens = len(ivocab)\n",
    "train_dial, valid_dial, test_dial = dials.get(\"train\"), dials.get(\"valid\"), dials.get(\"test\")\n",
    "train_meta, valid_meta, test_meta = metas.get(\"train\"), metas.get(\"valid\"), metas.get(\"test\")\n",
    "train_loader = getattr(data, 'DailyDial'+'DataLoader')(\"Train\", train_dial, train_meta, config['maxlen'])\n",
    "valid_loader = getattr(data, 'DailyDial'+'DataLoader')(\"Valid\", valid_dial, valid_meta, config['maxlen'])\n",
    "test_loader = getattr(data, 'DailyDial'+'DataLoader')(\"Test\", test_dial, test_meta, config['maxlen'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与dialog_doublegan 采用相同的数据集大小\n",
    "def valid_small(valid_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_valid_batch = 0\n",
    "    valid_count = 0\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            batch = valid_loader.next_batch()\n",
    "            if batch is None or total_valid_batch >= 1500: # end of epoch\n",
    "                break\n",
    "            total_valid_batch += 20\n",
    "            valid_count += 1\n",
    "            context, context_lens, utt_lens, floors,_,_,_,response,res_lens,_ = batch\n",
    "            context, utt_lens = context[:,:,1:], utt_lens-1 # remove the sos token in the context and reduce the context length\n",
    "            context, context_lens, utt_lens, floors, response, res_lens\\\n",
    "                    = gVar(context), gVar(context_lens), gVar(utt_lens), gData(floors), gVar(response), gVar(res_lens)\n",
    "            target, outputs = model(context, context_lens, utt_lens, floors, response, res_lens)\n",
    "            loss_batch = criterion(outputs, target)\n",
    "            total_loss += float(loss_batch.item())\n",
    "        return total_loss / valid_count    \n",
    "    \n",
    "def sample(context, context_lens, utt_lens, floors, repeat, SOS_tok, EOS_tok):    \n",
    "    model.eval()\n",
    "    c = model.context_encoder(context, context_lens, utt_lens, floors)\n",
    "#     c_repeated = c.expand(repeat, -1)\n",
    "    sample_words, sample_lens= model.decoder.sampling(c, None, config['maxlen'], SOS_tok, EOS_tok, \"greedy\")\n",
    "    return sample_words, sample_lens \n",
    "\n",
    "def evaluate(model, metrics, test_loader, vocab, ivocab, f_eval, repeat):\n",
    "    recall_bleus, prec_bleus, bows_extrema, bows_avg, bows_greedy, intra_dist1s, intra_dist2s, avg_lens, inter_dist1s, inter_dist2s\\\n",
    "        = [], [], [], [], [], [], [], [], [], []\n",
    "    bleu1_4s = []\n",
    "    local_t = 0\n",
    "    test_loader.epoch_init(1, config['diaglen'], 1, shuffle=False)\n",
    "    valid_count = 0\n",
    "    begin_time = time()\n",
    "    all_generated_sentences = []\n",
    "    all_generated_lens = []\n",
    "    while True:\n",
    "        batch = test_loader.next_batch()\n",
    "        if batch is None:\n",
    "#         if batch is None or valid_count >= 400:\n",
    "            break\n",
    "        valid_count += 1\n",
    "        local_t += 1 \n",
    "        context, context_lens, utt_lens, floors,_,_,_,response,res_lens,_ = batch   \n",
    "        context, utt_lens = context[:,:,1:], utt_lens-1 # remove the sos token in the context and reduce the context length\n",
    "        f_eval.write(\"Batch %d \\n\" % (local_t))# print the context\n",
    "        start = np.maximum(0, context_lens[0]-5)\n",
    "        for t_id in range(start, context.shape[1], 1):\n",
    "            context_str = indexes2sent(context[0, t_id], vocab, vocab[\"</s>\"], 0)\n",
    "            f_eval.write(\"Context %d-%d: %s\\n\" % (t_id, floors[0, t_id], context_str))\n",
    "        # print the true outputs    \n",
    "        ref_str, _ = indexes2sent(response[0], vocab, vocab[\"</s>\"], vocab[\"<s>\"])\n",
    "        ref_tokens = ref_str.split(' ')\n",
    "        f_eval.write(\"Target >> %s\\n\" % (ref_str.replace(\" ' \", \"'\")))\n",
    "        context, context_lens, utt_lens, floors = gVar(context), gVar(context_lens), gVar(utt_lens), gData(floors)\n",
    "        sample_words, sample_lens = model.sample(context, context_lens, utt_lens, floors, repeat, vocab[\"<s>\"], vocab[\"</s>\"])\n",
    "        # 存储所有生成的回复，用来计算div\n",
    "        all_generated_sentences.append(sample_words[0].tolist())\n",
    "        all_generated_lens.append(sample_lens[0].tolist())\n",
    "        # nparray: [repeat x seq_len]\n",
    "        pred_sents, _ = indexes2sent(sample_words, vocab, vocab[\"</s>\"], 0)\n",
    "        if valid_count % 300 == 0:\n",
    "            print('true response: ', ref_str)\n",
    "            print('generate response: ', pred_sents[0])\n",
    "        pred_tokens = [sent.split(' ') for sent in pred_sents]\n",
    "        for r_id, pred_sent in enumerate(pred_sents):\n",
    "            f_eval.write(\"Generate >> %s\\n\" % (pred_sent.replace(\" ' \", \"'\")))\n",
    "        max_bleu, avg_bleu = metrics.sim_bleu(pred_tokens, ref_tokens)\n",
    "        recall_bleus.append(max_bleu)\n",
    "        prec_bleus.append(avg_bleu)\n",
    "        bleu1_4s.append(metrics.sim_bleu1_4(pred_tokens[0], ref_tokens))\n",
    "        bow_extrema, bow_avg, bow_greedy = metrics.sim_bow(sample_words, sample_lens, response[:,1:], res_lens-2)\n",
    "        bows_extrema.append(bow_extrema)\n",
    "        bows_avg.append(bow_avg)\n",
    "        bows_greedy.append(bow_greedy)\n",
    "#         intra_dist1, intra_dist2, inter_dist1, inter_dist2 = metrics.div_distinct(sample_words, sample_lens-1)\n",
    "#         intra_dist1s.append(intra_dist1)\n",
    "#         intra_dist2s.append(intra_dist2)\n",
    "        avg_lens.append(np.mean(sample_lens))\n",
    "#         inter_dist1s.append(inter_dist1)\n",
    "#         inter_dist2s.append(inter_dist2)\n",
    "        f_eval.write(\"\\n\")\n",
    "    recall_bleu = float(np.mean(recall_bleus))\n",
    "    prec_bleu = float(np.mean(prec_bleus))\n",
    "    f1 = 2*(prec_bleu*recall_bleu) / (prec_bleu+recall_bleu+10e-12)\n",
    "    bleu1_4 = np.mean(bleu1_4s, 0)\n",
    "    bow_extrema = float(np.mean(bows_extrema))\n",
    "    bow_avg = float(np.mean(bows_avg))\n",
    "    bow_greedy=float(np.mean(bows_greedy))\n",
    "#     intra_dist1=float(np.mean(intra_dist1s))\n",
    "#     intra_dist2=float(np.mean(intra_dist2s))\n",
    "    avg_len=float(np.mean(avg_lens))\n",
    "    all_generated_sentences = np.array(all_generated_sentences)\n",
    "    all_generated_lens = np.array(all_generated_lens)\n",
    "#     print(all_generated_sentences[:5])\n",
    "#     print(all_generated_lens[:5])\n",
    "    intra_dist1, intra_dist2, inter_dist1, inter_dist2 = metrics.div_distinct(all_generated_sentences, all_generated_lens)\n",
    "#     inter_dist1=float(np.mean(inter_dist1s))\n",
    "#     inter_dist2=float(np.mean(inter_dist2s))\n",
    "#     report = \"Avg recall BLEU %f, bow_extrema %f, bow_avg %f, bow_greedy %f, inter_dist1 %f, inter_dist2 %f avg_len %f\" \\\n",
    "#     % (recall_bleu, bow_extrema, bow_avg, bow_greedy, inter_dist1, inter_dist2, avg_len)\n",
    "    report = \"BLEU1 %f, BLEU2 %f, BLEU3 %f, BLEU4 %f, inter_dist1 %f, inter_dist2 %f avg_len %f\" % (bleu1_4[0], bleu1_4[1], bleu1_4[2], bleu1_4[3], inter_dist1, inter_dist2, avg_len)\n",
    "    f_eval.write(report + \"\\n\")\n",
    "    print(report)\n",
    "    print(' time: %.1f s'%(time()-begin_time))\n",
    "    print(\"Done testing\")\n",
    "    return recall_bleu, bow_extrema, bow_avg, bow_greedy, inter_dist1, inter_dist2, avg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word2vec\n",
      "Epoch:  1\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[1 50] loss_ae: 772.952671 loss_g: 0.003260 loss_d: 9.441190 time: 7.1 s\n",
      "[1 100] loss_ae: 281.586226 loss_g: 0.007951 loss_d: 9.201698 time: 6.9 s\n",
      "[1 150] loss_ae: 176.866460 loss_g: 0.021754 loss_d: 9.092923 time: 7.0 s\n",
      "[1 200] loss_ae: 146.034011 loss_g: 0.029456 loss_d: 8.941715 time: 7.0 s\n",
      "[1 250] loss_ae: 127.633686 loss_g: 0.038266 loss_d: 8.804258 time: 7.0 s\n",
      "[1 300] loss_ae: 117.296023 loss_g: 0.084724 loss_d: 8.665235 time: 7.0 s\n",
      "[1 350] loss_ae: 103.915196 loss_g: 0.089025 loss_d: 8.589442 time: 7.0 s\n",
      "[1 400] loss_ae: 91.235519 loss_g: 0.078870 loss_d: 8.492210 time: 7.0 s\n",
      "[1 450] loss_ae: 86.707661 loss_g: 0.069349 loss_d: 8.387099 time: 7.0 s\n",
      "[1 500] loss_ae: 84.711306 loss_g: 0.085879 loss_d: 8.203546 time: 7.0 s\n",
      "[1 550] loss_ae: 84.355394 loss_g: 0.084642 loss_d: 8.040713 time: 7.0 s\n",
      "[1 600] loss_ae: 76.815586 loss_g: 0.099949 loss_d: 7.835270 time: 7.0 s\n",
      "[1 650] loss_ae: 75.378659 loss_g: 0.127281 loss_d: 7.709286 time: 7.0 s\n",
      "[1 700] loss_ae: 72.153581 loss_g: 0.126035 loss_d: 7.505994 time: 6.9 s\n",
      "[1 750] loss_ae: 67.798017 loss_g: 0.159414 loss_d: 7.462734 time: 7.0 s\n",
      "[1 800] loss_ae: 71.642383 loss_g: 0.205728 loss_d: 7.364033 time: 7.0 s\n",
      "[1 850] loss_ae: 70.922345 loss_g: 0.207321 loss_d: 7.266915 time: 7.0 s\n",
      "[1 900] loss_ae: 67.295124 loss_g: 0.186682 loss_d: 7.238027 time: 7.0 s\n",
      "[1 950] loss_ae: 61.311767 loss_g: 0.195381 loss_d: 7.127955 time: 7.0 s\n",
      "[1 1000] loss_ae: 64.882279 loss_g: 0.220248 loss_d: 6.953374 time: 7.0 s\n",
      "[1 1050] loss_ae: 61.577316 loss_g: 0.214222 loss_d: 6.905390 time: 7.0 s\n",
      "[1 1100] loss_ae: 62.639101 loss_g: 0.207480 loss_d: 6.886177 time: 7.0 s\n",
      "[1 1150] loss_ae: 60.569433 loss_g: 0.189316 loss_d: 6.754030 time: 7.0 s\n",
      "[1 1200] loss_ae: 53.222336 loss_g: 0.178742 loss_d: 6.717924 time: 7.2 s\n",
      "[1 1250] loss_ae: 60.874108 loss_g: 0.165005 loss_d: 6.725406 time: 7.0 s\n",
      "Evaluating....\n",
      "Epoch:  2\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[2 50] loss_ae: 56.925897 loss_g: 0.094636 loss_d: 6.643656 time: 7.0 s\n",
      "[2 100] loss_ae: 54.211338 loss_g: 0.082264 loss_d: 6.687222 time: 7.0 s\n",
      "[2 150] loss_ae: 52.457824 loss_g: 0.119308 loss_d: 6.612288 time: 6.9 s\n",
      "[2 200] loss_ae: 58.915600 loss_g: 0.126695 loss_d: 6.587664 time: 7.0 s\n",
      "[2 250] loss_ae: 56.129869 loss_g: 0.182233 loss_d: 6.447257 time: 7.1 s\n",
      "[2 300] loss_ae: 51.570942 loss_g: 0.246111 loss_d: 6.218617 time: 7.1 s\n",
      "[2 350] loss_ae: 53.137189 loss_g: 0.309165 loss_d: 6.211732 time: 7.0 s\n",
      "[2 400] loss_ae: 48.828830 loss_g: 0.348345 loss_d: 6.093193 time: 7.1 s\n",
      "[2 450] loss_ae: 49.373428 loss_g: 0.395365 loss_d: 6.030426 time: 7.0 s\n",
      "[2 500] loss_ae: 51.495709 loss_g: 0.337018 loss_d: 5.989023 time: 7.0 s\n",
      "[2 550] loss_ae: 51.355660 loss_g: 0.352932 loss_d: 6.054303 time: 7.0 s\n",
      "[2 600] loss_ae: 49.942865 loss_g: 0.415759 loss_d: 5.976384 time: 7.0 s\n",
      "[2 650] loss_ae: 55.705143 loss_g: 0.517309 loss_d: 5.698787 time: 7.0 s\n",
      "[2 700] loss_ae: 47.188180 loss_g: 0.533096 loss_d: 5.527480 time: 7.1 s\n",
      "[2 750] loss_ae: 52.543618 loss_g: 0.562111 loss_d: 5.269851 time: 7.1 s\n",
      "[2 800] loss_ae: 45.748557 loss_g: 0.499336 loss_d: 5.485379 time: 7.0 s\n",
      "[2 850] loss_ae: 52.906195 loss_g: 0.521092 loss_d: 5.258669 time: 7.1 s\n",
      "[2 900] loss_ae: 45.207044 loss_g: 0.570797 loss_d: 5.288457 time: 7.0 s\n",
      "[2 950] loss_ae: 46.601896 loss_g: 0.634172 loss_d: 5.053354 time: 7.0 s\n",
      "[2 1000] loss_ae: 47.539595 loss_g: 0.629844 loss_d: 5.071852 time: 7.0 s\n",
      "[2 1050] loss_ae: 47.518010 loss_g: 0.624076 loss_d: 5.010710 time: 7.1 s\n",
      "[2 1100] loss_ae: 46.240644 loss_g: 0.733668 loss_d: 4.777055 time: 7.0 s\n",
      "[2 1150] loss_ae: 49.490415 loss_g: 0.772825 loss_d: 4.650910 time: 7.0 s\n",
      "[2 1200] loss_ae: 44.202630 loss_g: 0.791597 loss_d: 4.629150 time: 6.8 s\n",
      "[2 1250] loss_ae: 45.502908 loss_g: 0.875029 loss_d: 4.395398 time: 6.8 s\n",
      "Evaluating....\n",
      "Epoch:  3\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[3 50] loss_ae: 41.320255 loss_g: 0.981736 loss_d: 4.175373 time: 6.8 s\n",
      "[3 100] loss_ae: 48.452294 loss_g: 1.056528 loss_d: 4.151715 time: 6.8 s\n",
      "[3 150] loss_ae: 46.650001 loss_g: 1.135069 loss_d: 3.913132 time: 6.8 s\n",
      "[3 200] loss_ae: 46.551746 loss_g: 1.157707 loss_d: 3.748518 time: 6.8 s\n",
      "[3 250] loss_ae: 43.921777 loss_g: 1.221852 loss_d: 3.587373 time: 6.8 s\n",
      "[3 300] loss_ae: 45.334111 loss_g: 1.287385 loss_d: 3.488599 time: 6.7 s\n",
      "[3 350] loss_ae: 43.129276 loss_g: 1.242093 loss_d: 3.447218 time: 6.7 s\n",
      "[3 400] loss_ae: 42.277752 loss_g: 1.328047 loss_d: 3.183527 time: 6.8 s\n",
      "[3 450] loss_ae: 44.787692 loss_g: 1.360280 loss_d: 3.148918 time: 6.8 s\n",
      "[3 500] loss_ae: 44.580995 loss_g: 1.424669 loss_d: 3.073979 time: 6.8 s\n",
      "[3 550] loss_ae: 44.251920 loss_g: 1.394026 loss_d: 2.985550 time: 6.7 s\n",
      "[3 600] loss_ae: 40.108987 loss_g: 1.499938 loss_d: 2.775922 time: 6.7 s\n",
      "[3 650] loss_ae: 43.318727 loss_g: 1.630149 loss_d: 2.805777 time: 6.7 s\n",
      "[3 700] loss_ae: 39.611466 loss_g: 1.672398 loss_d: 2.611373 time: 6.7 s\n",
      "[3 750] loss_ae: 43.672614 loss_g: 1.692167 loss_d: 2.481329 time: 6.8 s\n",
      "[3 800] loss_ae: 44.185718 loss_g: 1.834362 loss_d: 2.347278 time: 6.8 s\n",
      "[3 850] loss_ae: 42.302259 loss_g: 1.930050 loss_d: 2.193985 time: 6.7 s\n",
      "[3 900] loss_ae: 41.992351 loss_g: 1.847679 loss_d: 2.475805 time: 6.8 s\n",
      "[3 950] loss_ae: 39.988564 loss_g: 1.825417 loss_d: 2.605280 time: 6.7 s\n",
      "[3 1000] loss_ae: 45.971614 loss_g: 1.974925 loss_d: 2.107554 time: 6.8 s\n",
      "[3 1050] loss_ae: 43.410016 loss_g: 2.286047 loss_d: 1.923517 time: 6.7 s\n",
      "[3 1100] loss_ae: 41.126602 loss_g: 2.472990 loss_d: 1.591030 time: 6.8 s\n",
      "[3 1150] loss_ae: 39.110220 loss_g: 2.338810 loss_d: 1.500622 time: 6.7 s\n",
      "[3 1200] loss_ae: 38.894288 loss_g: 2.307385 loss_d: 1.602392 time: 6.7 s\n",
      "[3 1250] loss_ae: 41.979002 loss_g: 2.522459 loss_d: 1.343611 time: 6.7 s\n",
      "Evaluating....\n",
      "Epoch:  4\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[4 50] loss_ae: 37.456772 loss_g: 2.662748 loss_d: 1.323755 time: 6.8 s\n",
      "[4 100] loss_ae: 37.662571 loss_g: 2.834530 loss_d: 1.228902 time: 6.8 s\n",
      "[4 150] loss_ae: 37.149353 loss_g: 2.980751 loss_d: 1.234704 time: 6.8 s\n",
      "[4 200] loss_ae: 38.501126 loss_g: 2.997268 loss_d: 1.047467 time: 6.8 s\n",
      "[4 250] loss_ae: 41.214279 loss_g: 2.920886 loss_d: 0.869462 time: 6.8 s\n",
      "[4 300] loss_ae: 36.383906 loss_g: 3.000142 loss_d: 0.808984 time: 6.8 s\n",
      "[4 350] loss_ae: 42.592007 loss_g: 3.209792 loss_d: 0.630229 time: 6.9 s\n",
      "[4 400] loss_ae: 41.049442 loss_g: 3.412396 loss_d: 0.400763 time: 6.7 s\n",
      "[4 450] loss_ae: 41.263224 loss_g: 3.528630 loss_d: 0.188875 time: 6.8 s\n",
      "[4 500] loss_ae: 39.301777 loss_g: 3.705305 loss_d: -0.186638 time: 6.8 s\n",
      "[4 550] loss_ae: 40.801693 loss_g: 3.723590 loss_d: -0.142997 time: 6.7 s\n",
      "[4 600] loss_ae: 37.586674 loss_g: 3.762230 loss_d: -0.301713 time: 6.8 s\n",
      "[4 650] loss_ae: 40.172607 loss_g: 3.866721 loss_d: -0.437859 time: 6.8 s\n",
      "[4 700] loss_ae: 40.022090 loss_g: 3.979621 loss_d: -0.606403 time: 6.8 s\n",
      "[4 750] loss_ae: 37.732347 loss_g: 4.030451 loss_d: -0.682811 time: 6.8 s\n",
      "[4 800] loss_ae: 38.787566 loss_g: 4.073235 loss_d: -0.847050 time: 6.8 s\n",
      "[4 850] loss_ae: 39.378515 loss_g: 4.384522 loss_d: -1.116463 time: 6.7 s\n",
      "[4 900] loss_ae: 38.788278 loss_g: 4.411392 loss_d: -1.307960 time: 6.8 s\n",
      "[4 950] loss_ae: 38.226969 loss_g: 4.601721 loss_d: -1.418517 time: 6.8 s\n",
      "[4 1000] loss_ae: 37.192830 loss_g: 4.629324 loss_d: -1.241444 time: 6.8 s\n",
      "[4 1050] loss_ae: 34.395663 loss_g: 4.718402 loss_d: -1.390172 time: 6.8 s\n",
      "[4 1100] loss_ae: 35.333955 loss_g: 5.044971 loss_d: -1.813239 time: 6.8 s\n",
      "[4 1150] loss_ae: 37.706466 loss_g: 5.004139 loss_d: -1.881838 time: 6.7 s\n",
      "[4 1200] loss_ae: 37.306693 loss_g: 5.370234 loss_d: -2.447786 time: 6.7 s\n",
      "[4 1250] loss_ae: 38.539112 loss_g: 5.618990 loss_d: -2.673873 time: 6.9 s\n",
      "Evaluating....\n",
      "Epoch:  5\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[5 50] loss_ae: 36.910595 loss_g: 5.456367 loss_d: -2.600075 time: 6.8 s\n",
      "[5 100] loss_ae: 35.084777 loss_g: 5.560991 loss_d: -2.788498 time: 6.8 s\n",
      "[5 150] loss_ae: 38.072759 loss_g: 5.412018 loss_d: -2.513136 time: 6.8 s\n",
      "[5 200] loss_ae: 34.661004 loss_g: 5.984701 loss_d: -3.075208 time: 6.7 s\n",
      "[5 250] loss_ae: 33.920083 loss_g: 5.898708 loss_d: -3.002269 time: 6.8 s\n",
      "[5 300] loss_ae: 36.627541 loss_g: 6.049582 loss_d: -3.155916 time: 6.8 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 350] loss_ae: 37.859651 loss_g: 6.218454 loss_d: -3.364293 time: 6.8 s\n",
      "[5 400] loss_ae: 36.183868 loss_g: 6.302505 loss_d: -3.560900 time: 6.7 s\n",
      "[5 450] loss_ae: 33.951994 loss_g: 6.429477 loss_d: -3.783187 time: 6.7 s\n",
      "[5 500] loss_ae: 37.703691 loss_g: 6.518563 loss_d: -3.748051 time: 6.7 s\n",
      "[5 550] loss_ae: 35.514817 loss_g: 6.578220 loss_d: -3.797254 time: 6.7 s\n",
      "[5 600] loss_ae: 36.086232 loss_g: 6.717169 loss_d: -3.815160 time: 6.8 s\n",
      "[5 650] loss_ae: 38.782208 loss_g: 7.205209 loss_d: -4.488389 time: 6.8 s\n",
      "[5 700] loss_ae: 37.765013 loss_g: 7.306391 loss_d: -4.605716 time: 6.8 s\n",
      "[5 750] loss_ae: 33.293509 loss_g: 6.983238 loss_d: -4.192238 time: 6.8 s\n",
      "[5 800] loss_ae: 33.331662 loss_g: 7.339334 loss_d: -4.687312 time: 6.7 s\n",
      "[5 850] loss_ae: 35.962166 loss_g: 7.493131 loss_d: -4.819886 time: 6.8 s\n",
      "[5 900] loss_ae: 35.991774 loss_g: 7.853860 loss_d: -5.241279 time: 6.7 s\n",
      "[5 950] loss_ae: 35.134504 loss_g: 8.025310 loss_d: -5.515351 time: 6.7 s\n",
      "[5 1000] loss_ae: 32.883906 loss_g: 8.112808 loss_d: -5.591484 time: 6.8 s\n",
      "[5 1050] loss_ae: 35.499528 loss_g: 8.249332 loss_d: -5.833731 time: 6.8 s\n",
      "[5 1100] loss_ae: 30.891773 loss_g: 8.659098 loss_d: -6.220408 time: 6.8 s\n",
      "[5 1150] loss_ae: 33.499393 loss_g: 8.187059 loss_d: -5.640215 time: 6.8 s\n",
      "[5 1200] loss_ae: 33.509730 loss_g: 8.913698 loss_d: -6.462598 time: 6.7 s\n",
      "[5 1250] loss_ae: 35.961523 loss_g: 9.346794 loss_d: -7.018331 time: 6.8 s\n",
      "Evaluating....\n",
      "Epoch:  6\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[6 50] loss_ae: 33.279212 loss_g: 9.201713 loss_d: -6.944714 time: 6.7 s\n",
      "[6 100] loss_ae: 34.246340 loss_g: 9.609633 loss_d: -7.241163 time: 6.7 s\n",
      "[6 150] loss_ae: 33.685639 loss_g: 9.399211 loss_d: -7.085109 time: 6.7 s\n",
      "[6 200] loss_ae: 32.945649 loss_g: 9.662373 loss_d: -7.316973 time: 6.8 s\n",
      "[6 250] loss_ae: 33.544384 loss_g: 9.824682 loss_d: -7.402526 time: 6.8 s\n",
      "[6 300] loss_ae: 36.546073 loss_g: 10.291336 loss_d: -7.874781 time: 6.7 s\n",
      "[6 350] loss_ae: 34.080669 loss_g: 10.316104 loss_d: -7.888175 time: 6.8 s\n",
      "[6 400] loss_ae: 33.278294 loss_g: 10.386326 loss_d: -8.142599 time: 6.8 s\n",
      "[6 450] loss_ae: 32.163773 loss_g: 10.327911 loss_d: -8.205184 time: 6.7 s\n",
      "[6 500] loss_ae: 31.285620 loss_g: 10.558418 loss_d: -8.426882 time: 6.8 s\n",
      "[6 550] loss_ae: 34.249187 loss_g: 10.736229 loss_d: -8.386123 time: 6.8 s\n",
      "[6 600] loss_ae: 34.832651 loss_g: 11.157491 loss_d: -8.904209 time: 6.9 s\n",
      "[6 650] loss_ae: 32.685235 loss_g: 11.123901 loss_d: -8.913767 time: 6.7 s\n",
      "[6 700] loss_ae: 29.769502 loss_g: 10.866867 loss_d: -8.731813 time: 6.7 s\n",
      "[6 750] loss_ae: 34.532127 loss_g: 11.513595 loss_d: -9.415522 time: 6.8 s\n",
      "[6 800] loss_ae: 32.948066 loss_g: 11.898063 loss_d: -9.652384 time: 6.7 s\n",
      "[6 850] loss_ae: 32.013796 loss_g: 12.563393 loss_d: -10.284157 time: 6.7 s\n",
      "[6 900] loss_ae: 30.378051 loss_g: 12.455787 loss_d: -10.242965 time: 6.9 s\n",
      "[6 950] loss_ae: 30.541240 loss_g: 12.442251 loss_d: -10.250738 time: 6.8 s\n",
      "[6 1000] loss_ae: 30.940902 loss_g: 11.776243 loss_d: -9.748181 time: 6.8 s\n",
      "[6 1050] loss_ae: 30.595078 loss_g: 12.698488 loss_d: -10.633376 time: 6.7 s\n",
      "[6 1100] loss_ae: 32.499851 loss_g: 12.496861 loss_d: -10.369223 time: 6.8 s\n",
      "[6 1150] loss_ae: 32.257506 loss_g: 13.019662 loss_d: -10.821070 time: 6.7 s\n",
      "[6 1200] loss_ae: 33.182757 loss_g: 13.174741 loss_d: -10.944441 time: 6.7 s\n",
      "[6 1250] loss_ae: 30.705546 loss_g: 13.366376 loss_d: -11.193156 time: 6.7 s\n",
      "Evaluating....\n",
      "Epoch:  7\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[7 50] loss_ae: 34.998716 loss_g: 14.188651 loss_d: -12.300698 time: 6.8 s\n",
      "[7 100] loss_ae: 28.998092 loss_g: 13.632123 loss_d: -11.590061 time: 6.7 s\n",
      "[7 150] loss_ae: 29.341963 loss_g: 14.352017 loss_d: -12.348023 time: 6.7 s\n",
      "[7 200] loss_ae: 28.663588 loss_g: 14.238113 loss_d: -12.233961 time: 6.8 s\n",
      "[7 250] loss_ae: 30.070390 loss_g: 14.316415 loss_d: -12.397090 time: 6.8 s\n",
      "[7 300] loss_ae: 30.331785 loss_g: 14.566289 loss_d: -12.651853 time: 6.7 s\n",
      "[7 350] loss_ae: 28.611618 loss_g: 14.407047 loss_d: -12.498075 time: 6.7 s\n",
      "[7 400] loss_ae: 33.436385 loss_g: 15.105433 loss_d: -13.098993 time: 6.7 s\n",
      "[7 450] loss_ae: 32.638837 loss_g: 15.599838 loss_d: -13.708190 time: 6.8 s\n",
      "[7 500] loss_ae: 30.009437 loss_g: 15.883948 loss_d: -13.947234 time: 6.8 s\n",
      "[7 550] loss_ae: 30.036441 loss_g: 16.277996 loss_d: -14.278543 time: 6.7 s\n",
      "[7 600] loss_ae: 32.672490 loss_g: 16.287005 loss_d: -14.312965 time: 6.7 s\n",
      "[7 650] loss_ae: 29.454152 loss_g: 16.547137 loss_d: -14.611638 time: 6.8 s\n",
      "[7 700] loss_ae: 31.280573 loss_g: 16.471517 loss_d: -14.548227 time: 6.8 s\n",
      "[7 750] loss_ae: 29.733039 loss_g: 16.424958 loss_d: -14.395776 time: 6.7 s\n",
      "[7 800] loss_ae: 32.149244 loss_g: 16.915492 loss_d: -14.775087 time: 6.8 s\n",
      "[7 850] loss_ae: 28.181886 loss_g: 17.190444 loss_d: -15.100836 time: 7.8 s\n",
      "[7 900] loss_ae: 27.457708 loss_g: 16.936980 loss_d: -14.960288 time: 16.6 s\n",
      "[7 950] loss_ae: 28.912345 loss_g: 17.647352 loss_d: -15.719715 time: 16.3 s\n",
      "[7 1000] loss_ae: 29.559558 loss_g: 18.195896 loss_d: -16.350606 time: 16.1 s\n",
      "[7 1050] loss_ae: 30.928533 loss_g: 18.801088 loss_d: -16.885519 time: 15.6 s\n",
      "[7 1100] loss_ae: 30.547928 loss_g: 18.744171 loss_d: -16.953233 time: 16.3 s\n",
      "[7 1150] loss_ae: 30.291002 loss_g: 19.241590 loss_d: -17.456605 time: 16.8 s\n",
      "[7 1200] loss_ae: 29.603193 loss_g: 18.785935 loss_d: -17.028309 time: 16.6 s\n",
      "[7 1250] loss_ae: 28.003965 loss_g: 18.710984 loss_d: -16.935498 time: 16.0 s\n",
      "Evaluating....\n",
      "Epoch:  8\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[8 50] loss_ae: 28.779565 loss_g: 19.395764 loss_d: -17.709621 time: 16.3 s\n",
      "[8 100] loss_ae: 27.462190 loss_g: 19.393890 loss_d: -17.788746 time: 16.5 s\n",
      "[8 150] loss_ae: 27.909536 loss_g: 19.307509 loss_d: -17.718894 time: 16.6 s\n",
      "[8 200] loss_ae: 28.785019 loss_g: 20.278965 loss_d: -18.740947 time: 16.1 s\n",
      "[8 250] loss_ae: 29.136825 loss_g: 20.698670 loss_d: -19.147798 time: 16.0 s\n",
      "[8 300] loss_ae: 28.838522 loss_g: 20.784983 loss_d: -19.078067 time: 16.9 s\n",
      "[8 350] loss_ae: 33.091952 loss_g: 21.202008 loss_d: -19.553401 time: 16.5 s\n",
      "[8 400] loss_ae: 27.887170 loss_g: 20.967563 loss_d: -19.289049 time: 16.2 s\n",
      "[8 450] loss_ae: 30.187173 loss_g: 20.939358 loss_d: -19.383496 time: 15.7 s\n",
      "[8 500] loss_ae: 30.355768 loss_g: 22.778393 loss_d: -21.118147 time: 16.5 s\n",
      "[8 550] loss_ae: 31.022505 loss_g: 22.499997 loss_d: -20.862710 time: 16.7 s\n",
      "[8 600] loss_ae: 27.748695 loss_g: 21.708855 loss_d: -20.173339 time: 16.7 s\n",
      "[8 650] loss_ae: 26.232695 loss_g: 22.159199 loss_d: -20.604345 time: 16.1 s\n",
      "[8 700] loss_ae: 27.404354 loss_g: 23.519855 loss_d: -21.738900 time: 14.7 s\n",
      "[8 750] loss_ae: 29.508650 loss_g: 23.490103 loss_d: -21.767520 time: 9.9 s\n",
      "[8 800] loss_ae: 30.434254 loss_g: 23.272636 loss_d: -21.633576 time: 10.0 s\n",
      "[8 850] loss_ae: 28.230748 loss_g: 24.002463 loss_d: -22.596892 time: 9.9 s\n",
      "[8 900] loss_ae: 28.135639 loss_g: 23.977847 loss_d: -22.452239 time: 10.0 s\n",
      "[8 950] loss_ae: 29.942578 loss_g: 24.757275 loss_d: -23.440326 time: 9.9 s\n",
      "[8 1000] loss_ae: 29.343355 loss_g: 24.987757 loss_d: -23.451010 time: 9.9 s\n",
      "[8 1050] loss_ae: 23.731057 loss_g: 24.133355 loss_d: -22.587213 time: 9.8 s\n",
      "[8 1100] loss_ae: 29.093733 loss_g: 25.811594 loss_d: -24.229935 time: 9.9 s\n",
      "[8 1150] loss_ae: 29.550557 loss_g: 26.518166 loss_d: -24.993447 time: 9.9 s\n",
      "[8 1200] loss_ae: 29.372727 loss_g: 25.908175 loss_d: -24.367020 time: 9.7 s\n",
      "[8 1250] loss_ae: 28.830651 loss_g: 26.233433 loss_d: -24.829192 time: 9.7 s\n",
      "Evaluating....\n",
      "Epoch:  9\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[9 50] loss_ae: 31.102143 loss_g: 26.944787 loss_d: -25.495746 time: 9.8 s\n",
      "[9 100] loss_ae: 27.125261 loss_g: 26.704084 loss_d: -25.108610 time: 9.9 s\n",
      "[9 150] loss_ae: 26.166947 loss_g: 28.203430 loss_d: -26.706902 time: 9.8 s\n",
      "[9 200] loss_ae: 27.094767 loss_g: 29.101992 loss_d: -27.725288 time: 9.9 s\n",
      "[9 250] loss_ae: 27.797996 loss_g: 28.347184 loss_d: -26.838609 time: 9.7 s\n",
      "[9 300] loss_ae: 28.687721 loss_g: 28.184219 loss_d: -26.616548 time: 9.8 s\n",
      "[9 350] loss_ae: 27.375911 loss_g: 29.480192 loss_d: -27.893841 time: 9.7 s\n",
      "[9 400] loss_ae: 25.835868 loss_g: 29.237915 loss_d: -27.566308 time: 9.9 s\n",
      "[9 450] loss_ae: 28.379064 loss_g: 29.916474 loss_d: -28.277932 time: 9.6 s\n",
      "[9 500] loss_ae: 27.658210 loss_g: 29.518277 loss_d: -27.924686 time: 9.8 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 550] loss_ae: 27.186585 loss_g: 30.330113 loss_d: -28.816940 time: 9.7 s\n",
      "[9 600] loss_ae: 28.296203 loss_g: 31.404423 loss_d: -29.884428 time: 9.6 s\n",
      "[9 650] loss_ae: 28.992795 loss_g: 32.532437 loss_d: -31.093220 time: 9.7 s\n",
      "[9 700] loss_ae: 25.947943 loss_g: 31.456332 loss_d: -30.074452 time: 9.8 s\n",
      "[9 750] loss_ae: 26.714921 loss_g: 31.091417 loss_d: -29.636735 time: 9.7 s\n",
      "[9 800] loss_ae: 24.871557 loss_g: 32.259931 loss_d: -30.757568 time: 9.7 s\n",
      "[9 850] loss_ae: 28.846435 loss_g: 32.680481 loss_d: -31.104310 time: 9.8 s\n",
      "[9 900] loss_ae: 27.489551 loss_g: 33.479888 loss_d: -31.993742 time: 9.6 s\n",
      "[9 950] loss_ae: 25.767852 loss_g: 33.775568 loss_d: -32.205791 time: 9.6 s\n",
      "[9 1000] loss_ae: 25.624436 loss_g: 32.484541 loss_d: -30.946149 time: 9.6 s\n",
      "[9 1050] loss_ae: 26.808944 loss_g: 31.962697 loss_d: -30.184419 time: 9.7 s\n",
      "[9 1100] loss_ae: 27.484942 loss_g: 32.816767 loss_d: -31.329340 time: 9.7 s\n",
      "[9 1150] loss_ae: 29.139065 loss_g: 34.913741 loss_d: -33.506772 time: 9.7 s\n",
      "[9 1200] loss_ae: 27.481302 loss_g: 34.887739 loss_d: -33.402446 time: 9.7 s\n",
      "[9 1250] loss_ae: 26.263764 loss_g: 34.068753 loss_d: -32.529310 time: 9.8 s\n",
      "Evaluating....\n",
      "Epoch:  10\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[10 50] loss_ae: 26.196233 loss_g: 33.985483 loss_d: -32.408459 time: 9.9 s\n",
      "[10 100] loss_ae: 25.961005 loss_g: 36.407015 loss_d: -34.774012 time: 9.8 s\n",
      "[10 150] loss_ae: 27.524638 loss_g: 35.574584 loss_d: -33.996225 time: 9.9 s\n",
      "[10 200] loss_ae: 26.466736 loss_g: 36.346810 loss_d: -34.784198 time: 9.8 s\n",
      "[10 250] loss_ae: 26.693783 loss_g: 37.295180 loss_d: -35.682518 time: 9.8 s\n",
      "[10 300] loss_ae: 27.766036 loss_g: 36.560301 loss_d: -34.849227 time: 9.8 s\n",
      "[10 350] loss_ae: 28.148753 loss_g: 35.830031 loss_d: -33.940365 time: 9.8 s\n",
      "[10 400] loss_ae: 26.455917 loss_g: 35.979031 loss_d: -34.107689 time: 9.8 s\n",
      "[10 450] loss_ae: 23.590832 loss_g: 36.299724 loss_d: -34.707302 time: 9.9 s\n",
      "[10 500] loss_ae: 25.681176 loss_g: 37.842423 loss_d: -36.233007 time: 9.9 s\n",
      "[10 550] loss_ae: 26.382423 loss_g: 37.912736 loss_d: -36.275093 time: 9.8 s\n",
      "[10 600] loss_ae: 25.391975 loss_g: 39.332008 loss_d: -37.663009 time: 9.8 s\n",
      "[10 650] loss_ae: 26.071825 loss_g: 38.780905 loss_d: -37.026124 time: 9.8 s\n",
      "[10 700] loss_ae: 25.755998 loss_g: 39.377662 loss_d: -37.797234 time: 9.7 s\n",
      "[10 750] loss_ae: 23.724214 loss_g: 38.453433 loss_d: -36.884813 time: 9.8 s\n",
      "[10 800] loss_ae: 27.921598 loss_g: 38.984558 loss_d: -37.261692 time: 9.7 s\n",
      "[10 850] loss_ae: 25.939754 loss_g: 41.300323 loss_d: -39.994629 time: 9.7 s\n",
      "[10 900] loss_ae: 30.720654 loss_g: 42.664555 loss_d: -41.204283 time: 9.7 s\n",
      "[10 950] loss_ae: 28.525003 loss_g: 42.710655 loss_d: -41.175174 time: 9.7 s\n",
      "[10 1000] loss_ae: 25.345315 loss_g: 42.439025 loss_d: -40.725160 time: 9.7 s\n",
      "[10 1050] loss_ae: 25.081967 loss_g: 43.911196 loss_d: -42.520331 time: 9.7 s\n",
      "[10 1100] loss_ae: 24.955656 loss_g: 43.499113 loss_d: -41.713485 time: 9.8 s\n",
      "[10 1150] loss_ae: 26.324822 loss_g: 44.558578 loss_d: -42.773207 time: 9.7 s\n",
      "[10 1200] loss_ae: 26.492152 loss_g: 44.041857 loss_d: -42.414745 time: 9.7 s\n",
      "[10 1250] loss_ae: 26.337160 loss_g: 44.472216 loss_d: -42.689295 time: 9.6 s\n",
      "Evaluating....\n",
      "Epoch:  11\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[11 50] loss_ae: 25.800102 loss_g: 44.417449 loss_d: -42.406004 time: 9.6 s\n",
      "[11 100] loss_ae: 23.519176 loss_g: 40.454028 loss_d: -38.198465 time: 9.7 s\n",
      "[11 150] loss_ae: 23.689949 loss_g: 44.858324 loss_d: -42.977476 time: 9.8 s\n",
      "[11 200] loss_ae: 24.252879 loss_g: 43.294149 loss_d: -41.509116 time: 9.7 s\n",
      "[11 250] loss_ae: 31.439465 loss_g: 46.001353 loss_d: -44.289732 time: 9.7 s\n",
      "[11 300] loss_ae: 24.252377 loss_g: 45.343514 loss_d: -43.498032 time: 9.6 s\n",
      "[11 350] loss_ae: 23.946352 loss_g: 47.741391 loss_d: -45.745369 time: 9.7 s\n",
      "[11 400] loss_ae: 26.104182 loss_g: 47.368022 loss_d: -45.810722 time: 9.6 s\n",
      "[11 450] loss_ae: 25.202271 loss_g: 47.468312 loss_d: -45.707545 time: 9.7 s\n",
      "[11 500] loss_ae: 24.038268 loss_g: 47.114635 loss_d: -45.240116 time: 9.7 s\n",
      "[11 550] loss_ae: 27.157346 loss_g: 47.188917 loss_d: -46.057731 time: 9.8 s\n",
      "[11 600] loss_ae: 26.923256 loss_g: 48.939996 loss_d: -47.146971 time: 9.9 s\n",
      "[11 650] loss_ae: 24.900482 loss_g: 49.135712 loss_d: -47.305139 time: 9.7 s\n",
      "[11 700] loss_ae: 23.026929 loss_g: 49.713985 loss_d: -48.309854 time: 9.8 s\n",
      "[11 750] loss_ae: 25.696834 loss_g: 52.586121 loss_d: -50.905339 time: 9.9 s\n",
      "[11 800] loss_ae: 26.199034 loss_g: 51.552576 loss_d: -49.622961 time: 9.9 s\n",
      "[11 850] loss_ae: 24.298815 loss_g: 53.576665 loss_d: -51.951055 time: 10.0 s\n",
      "[11 900] loss_ae: 25.128359 loss_g: 51.373344 loss_d: -49.562836 time: 9.8 s\n",
      "[11 950] loss_ae: 23.959676 loss_g: 51.590400 loss_d: -49.636381 time: 9.8 s\n",
      "[11 1000] loss_ae: 27.041514 loss_g: 51.836010 loss_d: -50.071079 time: 9.9 s\n",
      "[11 1050] loss_ae: 25.783101 loss_g: 50.226944 loss_d: -48.095805 time: 9.8 s\n",
      "[11 1100] loss_ae: 27.942866 loss_g: 51.429596 loss_d: -49.369987 time: 9.8 s\n",
      "[11 1150] loss_ae: 23.515551 loss_g: 52.417921 loss_d: -50.449457 time: 9.9 s\n",
      "[11 1200] loss_ae: 25.839053 loss_g: 51.510441 loss_d: -49.077707 time: 9.8 s\n",
      "[11 1250] loss_ae: 25.369846 loss_g: 52.996207 loss_d: -50.420835 time: 9.8 s\n",
      "Evaluating....\n",
      "Epoch:  12\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[12 50] loss_ae: 23.918557 loss_g: 53.980953 loss_d: -51.298414 time: 9.8 s\n",
      "[12 100] loss_ae: 23.301173 loss_g: 51.943023 loss_d: -49.473160 time: 9.7 s\n",
      "[12 150] loss_ae: 22.835567 loss_g: 51.725526 loss_d: -49.887396 time: 9.9 s\n",
      "[12 200] loss_ae: 22.096883 loss_g: 50.546120 loss_d: -48.387618 time: 9.8 s\n",
      "[12 250] loss_ae: 19.599619 loss_g: 47.546130 loss_d: -44.705558 time: 9.7 s\n",
      "[12 300] loss_ae: 23.760576 loss_g: 50.389653 loss_d: -48.022144 time: 9.8 s\n",
      "[12 350] loss_ae: 23.784190 loss_g: 51.241324 loss_d: -48.607764 time: 9.6 s\n",
      "[12 400] loss_ae: 23.539601 loss_g: 51.210783 loss_d: -48.560826 time: 9.7 s\n",
      "[12 450] loss_ae: 22.328386 loss_g: 50.410925 loss_d: -47.969421 time: 9.5 s\n",
      "[12 500] loss_ae: 24.503328 loss_g: 52.921854 loss_d: -50.567122 time: 9.7 s\n",
      "[12 550] loss_ae: 25.450035 loss_g: 51.323855 loss_d: -48.710324 time: 9.7 s\n",
      "[12 600] loss_ae: 23.552978 loss_g: 55.171667 loss_d: -52.922417 time: 9.7 s\n",
      "[12 650] loss_ae: 22.765703 loss_g: 54.931748 loss_d: -53.249969 time: 9.7 s\n",
      "[12 700] loss_ae: 24.005726 loss_g: 55.300582 loss_d: -52.253607 time: 9.6 s\n",
      "[12 750] loss_ae: 21.647008 loss_g: 50.690631 loss_d: -47.942735 time: 9.7 s\n",
      "[12 800] loss_ae: 22.291708 loss_g: 48.847935 loss_d: -45.564242 time: 9.7 s\n",
      "[12 850] loss_ae: 21.875486 loss_g: 52.536288 loss_d: -49.542156 time: 9.7 s\n",
      "[12 900] loss_ae: 23.407750 loss_g: 51.386581 loss_d: -47.949649 time: 9.8 s\n",
      "[12 950] loss_ae: 21.716587 loss_g: 48.829131 loss_d: -45.957865 time: 9.7 s\n",
      "[12 1000] loss_ae: 23.531622 loss_g: 50.554698 loss_d: -47.471286 time: 9.8 s\n",
      "[12 1050] loss_ae: 23.305616 loss_g: 49.983011 loss_d: -46.645767 time: 9.7 s\n",
      "[12 1100] loss_ae: 20.552989 loss_g: 51.197142 loss_d: -47.560584 time: 9.9 s\n",
      "[12 1150] loss_ae: 22.547188 loss_g: 49.261239 loss_d: -46.059570 time: 10.0 s\n",
      "[12 1200] loss_ae: 22.402549 loss_g: 49.398849 loss_d: -46.214344 time: 9.9 s\n",
      "[12 1250] loss_ae: 20.686273 loss_g: 49.977137 loss_d: -45.660277 time: 9.8 s\n",
      "Evaluating....\n",
      "Epoch:  13\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[13 50] loss_ae: 24.661982 loss_g: 50.703509 loss_d: -46.923226 time: 9.9 s\n",
      "[13 100] loss_ae: 23.013771 loss_g: 41.899842 loss_d: -37.528990 time: 9.9 s\n",
      "[13 150] loss_ae: 21.740726 loss_g: 47.460419 loss_d: -43.378431 time: 9.8 s\n",
      "[13 200] loss_ae: 21.054329 loss_g: 47.301635 loss_d: -43.897988 time: 9.8 s\n",
      "[13 250] loss_ae: 20.364619 loss_g: 47.358107 loss_d: -43.562083 time: 9.8 s\n",
      "[13 300] loss_ae: 21.010982 loss_g: 42.656742 loss_d: -38.522660 time: 9.9 s\n",
      "[13 350] loss_ae: 22.656514 loss_g: 45.820935 loss_d: -41.412376 time: 9.5 s\n",
      "[13 400] loss_ae: 23.579514 loss_g: 47.519509 loss_d: -43.050804 time: 9.8 s\n",
      "[13 450] loss_ae: 26.607998 loss_g: 50.187263 loss_d: -45.877014 time: 9.8 s\n",
      "[13 500] loss_ae: 20.039926 loss_g: 42.875509 loss_d: -38.540345 time: 9.8 s\n",
      "[13 550] loss_ae: 20.575927 loss_g: 37.153363 loss_d: -32.894970 time: 9.7 s\n",
      "[13 600] loss_ae: 22.540010 loss_g: 37.113620 loss_d: -33.217646 time: 9.9 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13 650] loss_ae: 20.147793 loss_g: 39.559189 loss_d: -35.983067 time: 9.8 s\n",
      "[13 700] loss_ae: 21.377983 loss_g: 38.643849 loss_d: -34.895344 time: 9.7 s\n",
      "[13 750] loss_ae: 22.262758 loss_g: 34.967984 loss_d: -31.698883 time: 9.7 s\n",
      "[13 800] loss_ae: 20.861565 loss_g: 37.047480 loss_d: -33.475615 time: 9.7 s\n",
      "[13 850] loss_ae: 22.427043 loss_g: 39.427452 loss_d: -34.851948 time: 9.8 s\n",
      "[13 900] loss_ae: 24.014749 loss_g: 34.902353 loss_d: -30.447578 time: 9.7 s\n",
      "[13 950] loss_ae: 21.324488 loss_g: 33.862068 loss_d: -29.388565 time: 9.7 s\n",
      "[13 1000] loss_ae: 20.321437 loss_g: 34.198361 loss_d: -29.761123 time: 9.5 s\n",
      "[13 1050] loss_ae: 21.351447 loss_g: 34.231571 loss_d: -31.460623 time: 9.8 s\n",
      "[13 1100] loss_ae: 23.056334 loss_g: 37.965926 loss_d: -34.086987 time: 9.6 s\n",
      "[13 1150] loss_ae: 22.303441 loss_g: 32.989465 loss_d: -28.432210 time: 9.7 s\n",
      "[13 1200] loss_ae: 21.254090 loss_g: 33.655690 loss_d: -29.491164 time: 9.6 s\n",
      "[13 1250] loss_ae: 22.074044 loss_g: 35.200301 loss_d: -30.936599 time: 9.6 s\n",
      "Evaluating....\n",
      "Epoch:  14\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[14 50] loss_ae: 21.969280 loss_g: 28.590671 loss_d: -24.194415 time: 9.6 s\n",
      "[14 100] loss_ae: 18.524711 loss_g: 29.855001 loss_d: -26.885931 time: 9.6 s\n",
      "[14 150] loss_ae: 21.254669 loss_g: 35.146882 loss_d: -31.635417 time: 9.7 s\n",
      "[14 200] loss_ae: 23.165600 loss_g: 30.861199 loss_d: -26.675816 time: 9.9 s\n",
      "[14 250] loss_ae: 21.533461 loss_g: 33.015326 loss_d: -29.722192 time: 9.7 s\n",
      "[14 300] loss_ae: 20.422218 loss_g: 31.637590 loss_d: -28.151615 time: 9.5 s\n",
      "[14 350] loss_ae: 20.672000 loss_g: 31.354803 loss_d: -27.932462 time: 9.7 s\n",
      "[14 400] loss_ae: 24.799134 loss_g: 30.780668 loss_d: -27.936354 time: 10.0 s\n",
      "[14 450] loss_ae: 21.752042 loss_g: 32.774785 loss_d: -29.066900 time: 9.9 s\n",
      "[14 500] loss_ae: 19.755075 loss_g: 27.207583 loss_d: -24.401330 time: 10.1 s\n",
      "[14 550] loss_ae: 21.438051 loss_g: 25.776250 loss_d: -21.657720 time: 9.9 s\n",
      "[14 600] loss_ae: 23.407498 loss_g: 29.032891 loss_d: -26.412436 time: 9.8 s\n",
      "[14 650] loss_ae: 19.842551 loss_g: 27.505540 loss_d: -24.714454 time: 9.8 s\n",
      "[14 700] loss_ae: 21.563497 loss_g: 26.680634 loss_d: -23.639032 time: 9.8 s\n",
      "[14 750] loss_ae: 21.468736 loss_g: 27.699511 loss_d: -24.285984 time: 9.8 s\n",
      "[14 1000] loss_ae: 20.910732 loss_g: 29.108996 loss_d: -25.599013 time: 9.7 s\n",
      "[14 1050] loss_ae: 20.581331 loss_g: 26.405504 loss_d: -24.161792 time: 9.9 s\n",
      "[14 1100] loss_ae: 20.857103 loss_g: 28.369678 loss_d: -24.068854 time: 9.8 s\n",
      "[14 1150] loss_ae: 23.702039 loss_g: 30.476015 loss_d: -26.592364 time: 9.9 s\n",
      "[14 1200] loss_ae: 20.747341 loss_g: 25.061233 loss_d: -21.904042 time: 9.8 s\n",
      "[14 1250] loss_ae: 21.386390 loss_g: 28.790635 loss_d: -23.621560 time: 9.7 s\n",
      "Evaluating....\n",
      "Epoch:  15\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[15 50] loss_ae: 18.761743 loss_g: 28.290728 loss_d: -23.591424 time: 9.6 s\n",
      "[15 100] loss_ae: 20.422151 loss_g: 28.641755 loss_d: -26.426191 time: 9.8 s\n",
      "[15 150] loss_ae: 19.763998 loss_g: 31.125721 loss_d: -27.339448 time: 9.7 s\n",
      "[15 200] loss_ae: 21.158528 loss_g: 30.683781 loss_d: -27.640720 time: 9.7 s\n",
      "[15 250] loss_ae: 19.644832 loss_g: 33.060931 loss_d: -30.207606 time: 9.8 s\n",
      "[15 300] loss_ae: 19.539455 loss_g: 31.469796 loss_d: -28.494389 time: 9.6 s\n",
      "[15 350] loss_ae: 20.665916 loss_g: 30.543048 loss_d: -26.914677 time: 9.7 s\n",
      "[15 400] loss_ae: 21.239720 loss_g: 27.349112 loss_d: -24.398297 time: 9.9 s\n",
      "[15 450] loss_ae: 21.479382 loss_g: 32.199922 loss_d: -28.515333 time: 9.8 s\n",
      "[15 500] loss_ae: 19.487054 loss_g: 28.351026 loss_d: -25.228328 time: 9.6 s\n",
      "[15 550] loss_ae: 22.643605 loss_g: 34.141127 loss_d: -30.412944 time: 9.8 s\n",
      "[15 600] loss_ae: 21.885126 loss_g: 31.568865 loss_d: -28.092903 time: 9.7 s\n",
      "[15 650] loss_ae: 22.972410 loss_g: 33.866452 loss_d: -29.170869 time: 9.5 s\n",
      "[15 700] loss_ae: 21.036183 loss_g: 31.442332 loss_d: -26.617978 time: 9.7 s\n",
      "[15 750] loss_ae: 21.235044 loss_g: 31.865353 loss_d: -26.405949 time: 9.5 s\n",
      "[15 800] loss_ae: 23.364500 loss_g: 33.867596 loss_d: -29.412689 time: 9.7 s\n",
      "[15 850] loss_ae: 21.416338 loss_g: 27.438073 loss_d: -23.302985 time: 9.6 s\n",
      "[15 900] loss_ae: 22.167768 loss_g: 27.061621 loss_d: -22.960722 time: 8.7 s\n",
      "[15 950] loss_ae: 24.252087 loss_g: 28.493930 loss_d: -24.385807 time: 8.2 s\n",
      "[15 1000] loss_ae: 20.958587 loss_g: 30.272719 loss_d: -26.985949 time: 8.1 s\n",
      "[15 1050] loss_ae: 21.127798 loss_g: 33.244126 loss_d: -29.870437 time: 8.3 s\n",
      "[15 1100] loss_ae: 22.179210 loss_g: 30.628512 loss_d: -26.977859 time: 8.3 s\n",
      "[15 1150] loss_ae: 22.048276 loss_g: 29.169344 loss_d: -26.320817 time: 8.2 s\n",
      "[15 1200] loss_ae: 19.816156 loss_g: 28.840697 loss_d: -24.777589 time: 8.2 s\n",
      "[15 1250] loss_ae: 22.744856 loss_g: 32.870006 loss_d: -27.531284 time: 8.2 s\n",
      "Evaluating....\n",
      "Epoch:  16\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[16 50] loss_ae: 22.121040 loss_g: 29.316316 loss_d: -25.287707 time: 8.4 s\n",
      "[16 100] loss_ae: 22.696818 loss_g: 29.214315 loss_d: -26.001367 time: 8.2 s\n",
      "[16 150] loss_ae: 19.861320 loss_g: 28.748036 loss_d: -23.790629 time: 8.3 s\n",
      "[16 200] loss_ae: 20.100760 loss_g: 26.191223 loss_d: -22.887378 time: 8.4 s\n",
      "[16 250] loss_ae: 20.809448 loss_g: 27.776230 loss_d: -24.020211 time: 8.3 s\n",
      "[16 300] loss_ae: 19.523481 loss_g: 31.636234 loss_d: -27.597160 time: 8.3 s\n",
      "[16 350] loss_ae: 19.940183 loss_g: 28.652811 loss_d: -24.961177 time: 8.3 s\n",
      "[16 400] loss_ae: 18.315582 loss_g: 31.275637 loss_d: -27.210067 time: 8.2 s\n",
      "[16 450] loss_ae: 20.232110 loss_g: 29.334727 loss_d: -24.713156 time: 8.4 s\n",
      "[16 500] loss_ae: 21.210894 loss_g: 29.698398 loss_d: -25.369468 time: 8.2 s\n",
      "[16 550] loss_ae: 23.272414 loss_g: 31.414599 loss_d: -27.661376 time: 8.4 s\n",
      "[16 600] loss_ae: 20.662181 loss_g: 29.925932 loss_d: -26.535412 time: 8.2 s\n",
      "[16 650] loss_ae: 22.502247 loss_g: 28.591675 loss_d: -24.277833 time: 8.4 s\n",
      "[16 700] loss_ae: 19.891060 loss_g: 27.474100 loss_d: -24.122822 time: 8.3 s\n",
      "[16 750] loss_ae: 20.895253 loss_g: 30.689371 loss_d: -27.511884 time: 8.4 s\n",
      "[16 800] loss_ae: 21.526236 loss_g: 30.924596 loss_d: -27.198620 time: 8.4 s\n",
      "[16 850] loss_ae: 20.694550 loss_g: 28.578815 loss_d: -24.890902 time: 8.3 s\n",
      "[16 900] loss_ae: 19.522289 loss_g: 28.413567 loss_d: -24.716404 time: 8.4 s\n",
      "[16 950] loss_ae: 21.852681 loss_g: 30.852516 loss_d: -27.720084 time: 8.3 s\n",
      "[16 1000] loss_ae: 19.769673 loss_g: 28.971741 loss_d: -25.237733 time: 8.2 s\n",
      "[16 1050] loss_ae: 17.782861 loss_g: 30.338304 loss_d: -26.907063 time: 8.4 s\n",
      "[16 1100] loss_ae: 19.444433 loss_g: 29.512427 loss_d: -23.703193 time: 8.3 s\n",
      "[16 1150] loss_ae: 19.703151 loss_g: 28.005102 loss_d: -24.928425 time: 8.3 s\n",
      "[16 1200] loss_ae: 21.596098 loss_g: 27.452451 loss_d: -24.161685 time: 8.3 s\n",
      "[16 1250] loss_ae: 22.452862 loss_g: 30.447180 loss_d: -27.248724 time: 8.3 s\n",
      "Evaluating....\n",
      "Epoch:  17\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[17 50] loss_ae: 20.491072 loss_g: 31.703109 loss_d: -26.575184 time: 8.3 s\n",
      "[17 100] loss_ae: 19.866975 loss_g: 29.628438 loss_d: -26.288918 time: 8.2 s\n",
      "[17 150] loss_ae: 19.025863 loss_g: 33.291217 loss_d: -30.496841 time: 8.3 s\n",
      "[17 200] loss_ae: 19.556993 loss_g: 33.278473 loss_d: -27.544880 time: 8.2 s\n",
      "[17 250] loss_ae: 20.261854 loss_g: 31.991914 loss_d: -28.867349 time: 8.3 s\n",
      "[17 300] loss_ae: 23.876515 loss_g: 32.305736 loss_d: -27.652595 time: 8.4 s\n",
      "[17 350] loss_ae: 21.200929 loss_g: 28.867437 loss_d: -26.177061 time: 8.3 s\n",
      "[17 400] loss_ae: 19.975133 loss_g: 33.084161 loss_d: -29.510237 time: 8.4 s\n",
      "[17 450] loss_ae: 22.878518 loss_g: 33.171718 loss_d: -29.405211 time: 8.2 s\n",
      "[17 500] loss_ae: 20.816380 loss_g: 33.303057 loss_d: -30.504620 time: 8.3 s\n",
      "[17 550] loss_ae: 21.714947 loss_g: 35.065741 loss_d: -30.854504 time: 8.4 s\n",
      "[17 600] loss_ae: 19.939199 loss_g: 32.581976 loss_d: -29.189552 time: 8.2 s\n",
      "[17 650] loss_ae: 19.859616 loss_g: 32.641105 loss_d: -30.832121 time: 8.3 s\n",
      "[17 700] loss_ae: 20.061765 loss_g: 31.396382 loss_d: -27.589586 time: 8.4 s\n",
      "[17 750] loss_ae: 21.664675 loss_g: 31.303091 loss_d: -28.015099 time: 8.3 s\n",
      "[17 800] loss_ae: 19.529674 loss_g: 31.895337 loss_d: -29.190807 time: 8.2 s\n",
      "[17 850] loss_ae: 20.932124 loss_g: 34.174999 loss_d: -30.392494 time: 8.9 s\n",
      "[17 900] loss_ae: 21.081678 loss_g: 37.286083 loss_d: -32.139780 time: 10.9 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17 950] loss_ae: 18.575243 loss_g: 37.636724 loss_d: -32.320322 time: 8.4 s\n",
      "[17 1000] loss_ae: 17.308367 loss_g: 33.547353 loss_d: -29.296290 time: 8.1 s\n",
      "[17 1050] loss_ae: 18.388342 loss_g: 36.622879 loss_d: -33.363054 time: 8.4 s\n",
      "[17 1100] loss_ae: 18.355875 loss_g: 35.853897 loss_d: -30.095099 time: 8.4 s\n",
      "[17 1150] loss_ae: 18.858322 loss_g: 34.428349 loss_d: -30.245642 time: 8.3 s\n",
      "[17 1200] loss_ae: 21.042025 loss_g: 36.157712 loss_d: -33.849894 time: 8.2 s\n",
      "[17 1250] loss_ae: 17.914418 loss_g: 35.508517 loss_d: -31.905805 time: 8.3 s\n",
      "Evaluating....\n",
      "Epoch:  18\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[18 50] loss_ae: 20.126635 loss_g: 35.524317 loss_d: -31.672435 time: 15.5 s\n",
      "[18 100] loss_ae: 20.588881 loss_g: 34.070028 loss_d: -29.479850 time: 20.8 s\n",
      "[18 150] loss_ae: 18.720538 loss_g: 34.022009 loss_d: -31.305369 time: 21.0 s\n",
      "[18 200] loss_ae: 19.336663 loss_g: 35.493099 loss_d: -32.130773 time: 20.7 s\n",
      "[18 250] loss_ae: 19.965893 loss_g: 37.532155 loss_d: -32.129594 time: 20.7 s\n",
      "[18 300] loss_ae: 21.661417 loss_g: 35.730860 loss_d: -31.019053 time: 20.4 s\n",
      "[18 350] loss_ae: 18.578026 loss_g: 38.925553 loss_d: -33.885344 time: 20.9 s\n",
      "[18 400] loss_ae: 18.353623 loss_g: 36.532437 loss_d: -32.721537 time: 20.9 s\n",
      "[18 450] loss_ae: 18.907654 loss_g: 38.965521 loss_d: -34.993005 time: 20.9 s\n",
      "[18 500] loss_ae: 20.673061 loss_g: 34.385625 loss_d: -30.284808 time: 20.6 s\n",
      "[18 550] loss_ae: 18.920681 loss_g: 38.649981 loss_d: -33.702465 time: 20.4 s\n",
      "[18 600] loss_ae: 18.484351 loss_g: 33.508260 loss_d: -30.116671 time: 20.4 s\n",
      "[18 650] loss_ae: 17.498477 loss_g: 34.357811 loss_d: -30.785936 time: 21.0 s\n",
      "[18 700] loss_ae: 19.625933 loss_g: 35.318905 loss_d: -33.248347 time: 20.6 s\n",
      "[18 750] loss_ae: 21.139244 loss_g: 34.167966 loss_d: -29.375080 time: 21.1 s\n",
      "[18 800] loss_ae: 19.027037 loss_g: 34.789800 loss_d: -29.950863 time: 20.9 s\n",
      "[18 850] loss_ae: 20.427027 loss_g: 33.966524 loss_d: -29.549688 time: 20.8 s\n",
      "[18 900] loss_ae: 20.205599 loss_g: 38.786536 loss_d: -34.942010 time: 20.9 s\n",
      "[18 950] loss_ae: 20.469600 loss_g: 36.233758 loss_d: -32.387115 time: 20.5 s\n",
      "[18 1000] loss_ae: 23.232240 loss_g: 38.560936 loss_d: -35.000912 time: 20.9 s\n",
      "[18 1050] loss_ae: 21.422691 loss_g: 39.110509 loss_d: -34.836120 time: 20.7 s\n",
      "[18 1100] loss_ae: 17.447185 loss_g: 33.734823 loss_d: -29.123014 time: 20.8 s\n",
      "[18 1150] loss_ae: 16.800202 loss_g: 34.973041 loss_d: -28.539633 time: 20.7 s\n",
      "[18 1200] loss_ae: 18.797887 loss_g: 37.571166 loss_d: -32.743310 time: 20.6 s\n",
      "[18 1250] loss_ae: 19.714233 loss_g: 37.585529 loss_d: -32.182379 time: 20.7 s\n",
      "Evaluating....\n",
      "Epoch:  19\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[19 50] loss_ae: 19.602554 loss_g: 36.131601 loss_d: -31.892780 time: 20.6 s\n",
      "[19 100] loss_ae: 18.915179 loss_g: 33.583513 loss_d: -30.321738 time: 20.7 s\n",
      "[19 150] loss_ae: 18.058609 loss_g: 37.635757 loss_d: -33.616303 time: 21.0 s\n",
      "[19 200] loss_ae: 20.797335 loss_g: 38.335607 loss_d: -34.100760 time: 20.8 s\n",
      "[19 250] loss_ae: 20.119689 loss_g: 38.208612 loss_d: -32.945540 time: 20.4 s\n",
      "[19 300] loss_ae: 18.303055 loss_g: 39.340255 loss_d: -32.628798 time: 20.6 s\n",
      "[19 350] loss_ae: 19.844540 loss_g: 39.016747 loss_d: -34.763298 time: 20.5 s\n",
      "[19 400] loss_ae: 18.204296 loss_g: 36.380460 loss_d: -31.873921 time: 20.4 s\n",
      "[19 450] loss_ae: 20.488822 loss_g: 37.767545 loss_d: -32.626606 time: 20.8 s\n",
      "[19 500] loss_ae: 16.556814 loss_g: 37.953304 loss_d: -33.105545 time: 20.7 s\n",
      "[19 550] loss_ae: 19.752578 loss_g: 39.230662 loss_d: -34.999855 time: 20.7 s\n",
      "[19 600] loss_ae: 18.849995 loss_g: 36.069810 loss_d: -30.297690 time: 20.7 s\n",
      "[19 650] loss_ae: 20.134052 loss_g: 38.461236 loss_d: -33.490393 time: 20.6 s\n",
      "[19 700] loss_ae: 20.835439 loss_g: 40.812700 loss_d: -37.345788 time: 20.7 s\n",
      "[19 750] loss_ae: 19.382577 loss_g: 41.953599 loss_d: -38.019210 time: 20.6 s\n",
      "[19 800] loss_ae: 19.037821 loss_g: 38.959296 loss_d: -36.003077 time: 20.7 s\n",
      "[19 850] loss_ae: 19.169880 loss_g: 40.899675 loss_d: -38.514753 time: 20.8 s\n",
      "[19 900] loss_ae: 19.664344 loss_g: 39.862149 loss_d: -35.748324 time: 20.5 s\n",
      "[19 950] loss_ae: 18.232485 loss_g: 38.059725 loss_d: -32.619898 time: 20.7 s\n",
      "[19 1000] loss_ae: 20.448661 loss_g: 39.911271 loss_d: -35.174831 time: 20.6 s\n",
      "[19 1050] loss_ae: 19.203289 loss_g: 38.984767 loss_d: -36.089144 time: 20.8 s\n",
      "[19 1100] loss_ae: 18.669819 loss_g: 38.306588 loss_d: -33.682815 time: 20.7 s\n",
      "[19 1150] loss_ae: 20.116389 loss_g: 40.034942 loss_d: -38.569405 time: 20.7 s\n",
      "[19 1200] loss_ae: 20.109724 loss_g: 38.875243 loss_d: -33.992748 time: 20.7 s\n",
      "[19 1250] loss_ae: 17.611664 loss_g: 36.763218 loss_d: -33.139910 time: 20.8 s\n",
      "Evaluating....\n",
      "Epoch:  20\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[20 50] loss_ae: 18.252439 loss_g: 35.423255 loss_d: -31.809119 time: 20.4 s\n",
      "[20 100] loss_ae: 16.361839 loss_g: 43.068349 loss_d: -38.186315 time: 20.4 s\n",
      "[20 150] loss_ae: 18.572618 loss_g: 43.230326 loss_d: -37.855894 time: 20.6 s\n",
      "[20 200] loss_ae: 18.362964 loss_g: 37.962125 loss_d: -33.842942 time: 20.9 s\n",
      "[20 250] loss_ae: 19.643885 loss_g: 41.787632 loss_d: -36.585239 time: 20.8 s\n",
      "[20 300] loss_ae: 20.281180 loss_g: 41.354480 loss_d: -36.254011 time: 20.5 s\n",
      "[20 350] loss_ae: 17.777771 loss_g: 38.655942 loss_d: -34.288626 time: 20.6 s\n",
      "[20 400] loss_ae: 20.187354 loss_g: 41.622743 loss_d: -35.406714 time: 20.4 s\n",
      "[20 450] loss_ae: 18.324403 loss_g: 35.336316 loss_d: -31.350199 time: 20.5 s\n",
      "[20 500] loss_ae: 19.044533 loss_g: 41.275080 loss_d: -35.915577 time: 20.4 s\n",
      "[20 550] loss_ae: 17.479283 loss_g: 37.506255 loss_d: -36.285958 time: 20.7 s\n",
      "[20 600] loss_ae: 18.335730 loss_g: 39.933474 loss_d: -35.969484 time: 20.6 s\n",
      "[20 650] loss_ae: 18.490832 loss_g: 40.191148 loss_d: -35.077494 time: 20.3 s\n",
      "[20 700] loss_ae: 19.778181 loss_g: 39.766093 loss_d: -34.208288 time: 20.3 s\n",
      "[20 750] loss_ae: 20.967557 loss_g: 43.344449 loss_d: -37.406641 time: 20.2 s\n",
      "[20 800] loss_ae: 20.585291 loss_g: 42.928459 loss_d: -40.017470 time: 20.5 s\n",
      "[20 850] loss_ae: 17.519894 loss_g: 42.815619 loss_d: -38.016675 time: 15.9 s\n",
      "[20 900] loss_ae: 16.872487 loss_g: 41.347239 loss_d: -37.531217 time: 11.6 s\n",
      "[20 950] loss_ae: 20.062894 loss_g: 41.608652 loss_d: -36.082740 time: 11.6 s\n",
      "[20 1000] loss_ae: 21.265627 loss_g: 42.090826 loss_d: -37.364694 time: 12.0 s\n",
      "[20 1050] loss_ae: 20.166393 loss_g: 39.132527 loss_d: -35.767318 time: 11.9 s\n",
      "[20 1100] loss_ae: 20.113538 loss_g: 45.290766 loss_d: -38.693801 time: 11.7 s\n",
      "[20 1150] loss_ae: 18.363329 loss_g: 46.361871 loss_d: -40.765598 time: 12.1 s\n",
      "[20 1200] loss_ae: 18.178788 loss_g: 41.453574 loss_d: -36.520823 time: 11.9 s\n",
      "[20 1250] loss_ae: 20.147111 loss_g: 45.716589 loss_d: -40.893250 time: 11.8 s\n",
      "Evaluating....\n",
      "Epoch:  21\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[21 50] loss_ae: 19.345522 loss_g: 42.406484 loss_d: -36.191054 time: 11.9 s\n",
      "[21 100] loss_ae: 17.173857 loss_g: 44.236198 loss_d: -39.424958 time: 12.1 s\n",
      "[21 150] loss_ae: 17.745830 loss_g: 42.515694 loss_d: -37.281838 time: 11.7 s\n",
      "[21 200] loss_ae: 19.912344 loss_g: 43.792892 loss_d: -37.404460 time: 12.1 s\n",
      "[21 250] loss_ae: 18.689565 loss_g: 46.069966 loss_d: -39.662892 time: 11.9 s\n",
      "[21 300] loss_ae: 17.251654 loss_g: 42.644261 loss_d: -37.445896 time: 11.8 s\n",
      "[21 350] loss_ae: 16.544223 loss_g: 41.259536 loss_d: -35.397082 time: 12.0 s\n",
      "[21 400] loss_ae: 19.424163 loss_g: 39.487467 loss_d: -35.299249 time: 11.8 s\n",
      "[21 450] loss_ae: 17.263232 loss_g: 38.865144 loss_d: -34.768089 time: 11.8 s\n",
      "[21 500] loss_ae: 17.597294 loss_g: 41.292334 loss_d: -35.411161 time: 12.0 s\n",
      "[21 550] loss_ae: 20.304386 loss_g: 43.516759 loss_d: -38.146229 time: 11.9 s\n",
      "[21 600] loss_ae: 17.297377 loss_g: 43.301649 loss_d: -37.863710 time: 11.8 s\n",
      "[21 650] loss_ae: 17.719823 loss_g: 39.582755 loss_d: -34.380071 time: 12.0 s\n",
      "[21 700] loss_ae: 19.085468 loss_g: 41.902165 loss_d: -35.494062 time: 11.9 s\n",
      "[21 750] loss_ae: 19.883245 loss_g: 43.122017 loss_d: -37.386396 time: 11.6 s\n",
      "[21 800] loss_ae: 20.581524 loss_g: 43.973926 loss_d: -39.666851 time: 11.6 s\n",
      "[21 850] loss_ae: 17.891739 loss_g: 41.471789 loss_d: -35.505045 time: 11.8 s\n",
      "[21 900] loss_ae: 19.392570 loss_g: 44.732612 loss_d: -38.760122 time: 11.8 s\n",
      "[21 950] loss_ae: 18.447166 loss_g: 39.785654 loss_d: -33.282853 time: 11.8 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21 1000] loss_ae: 17.847564 loss_g: 40.273684 loss_d: -33.863154 time: 11.7 s\n",
      "[21 1050] loss_ae: 19.222470 loss_g: 45.819096 loss_d: -39.444961 time: 11.9 s\n",
      "[21 1100] loss_ae: 17.659443 loss_g: 38.446475 loss_d: -32.723279 time: 11.7 s\n",
      "[21 1150] loss_ae: 18.935319 loss_g: 43.383489 loss_d: -39.712220 time: 11.8 s\n",
      "[21 1200] loss_ae: 18.331022 loss_g: 42.060851 loss_d: -38.113651 time: 12.0 s\n",
      "[21 1250] loss_ae: 16.932001 loss_g: 43.906909 loss_d: -39.653926 time: 11.7 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  there ' s a lot of people who ' ve been in the past couple of years and they ' re all grown up and </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  do you think that ' s the best way to go to the movies and the </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  the only thing that i ' d like to see is the <unk> of the <unk> and the <unk> of the <unk> and the <unk> of the </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  <unk> </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  hum </s>\n",
      "true response:  oh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and i like i like i like i like i like i like i like i like the <unk> type stuff i like i like the <unk> type stuff i\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  you have to you have to have a you have a you have a you have a you have a you have a you have a you have a you\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah it </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  i you know i don ' t </s>\n",
      "true response:  but they </s>\n",
      "generate response:  yeah they ' re they ' re they ' re they ' re a little bit more expensive than they used to be </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and you can ' t you can ' t you can ' t have a you can you can you can you can you can you can you can you\n",
      "true response:  and </s>\n",
      "generate response:  and we had a lot of <unk> <unk> and <unk> and <unk> and <unk> and <unk> and </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  and at least there ' s a lot of people that don ' t have to go to the beach and then </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  i don ' t know </s>\n",
      "BLEU1 0.403569, BLEU2 0.322495, BLEU3 0.269441, BLEU4 0.215354, inter_dist1 0.004790, inter_dist2 0.026065 avg_len 16.110199\n",
      " time: 209.3 s\n",
      "Done testing\n",
      "Epoch:  22\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[22 50] loss_ae: 19.057104 loss_g: 45.566913 loss_d: -41.743344 time: 11.8 s\n",
      "[22 100] loss_ae: 16.987067 loss_g: 40.552291 loss_d: -36.956614 time: 11.7 s\n",
      "[22 150] loss_ae: 16.705228 loss_g: 43.809325 loss_d: -37.915927 time: 12.0 s\n",
      "[22 200] loss_ae: 17.751518 loss_g: 45.092644 loss_d: -39.939050 time: 12.0 s\n",
      "[22 250] loss_ae: 20.717041 loss_g: 44.938731 loss_d: -39.002607 time: 11.7 s\n",
      "[22 300] loss_ae: 16.620295 loss_g: 43.799833 loss_d: -36.842467 time: 12.1 s\n",
      "[22 350] loss_ae: 18.167085 loss_g: 42.873166 loss_d: -38.570406 time: 12.0 s\n",
      "[22 400] loss_ae: 18.678262 loss_g: 44.126857 loss_d: -38.007467 time: 11.9 s\n",
      "[22 450] loss_ae: 15.579200 loss_g: 45.324879 loss_d: -41.422654 time: 12.0 s\n",
      "[22 500] loss_ae: 16.365161 loss_g: 43.599337 loss_d: -37.763976 time: 11.9 s\n",
      "[22 550] loss_ae: 16.857661 loss_g: 47.048078 loss_d: -38.824680 time: 11.8 s\n",
      "[22 600] loss_ae: 17.527878 loss_g: 46.075993 loss_d: -39.320532 time: 11.9 s\n",
      "[22 650] loss_ae: 15.622732 loss_g: 44.256308 loss_d: -37.648466 time: 11.9 s\n",
      "[22 700] loss_ae: 16.859807 loss_g: 40.215530 loss_d: -33.931996 time: 11.7 s\n",
      "[22 750] loss_ae: 16.645141 loss_g: 40.920992 loss_d: -35.897552 time: 11.7 s\n",
      "[22 800] loss_ae: 14.866399 loss_g: 39.162131 loss_d: -30.540404 time: 12.0 s\n",
      "[22 850] loss_ae: 16.753796 loss_g: 43.284519 loss_d: -37.344676 time: 11.9 s\n",
      "[22 900] loss_ae: 21.291997 loss_g: 40.539362 loss_d: -35.256918 time: 11.6 s\n",
      "[22 950] loss_ae: 17.159825 loss_g: 39.364705 loss_d: -33.818586 time: 12.1 s\n",
      "[22 1000] loss_ae: 19.038771 loss_g: 40.626088 loss_d: -35.659315 time: 11.9 s\n",
      "[22 1050] loss_ae: 18.410472 loss_g: 44.417861 loss_d: -38.802018 time: 11.8 s\n",
      "[22 1100] loss_ae: 16.943871 loss_g: 43.713727 loss_d: -39.753444 time: 11.7 s\n",
      "[22 1150] loss_ae: 18.264156 loss_g: 44.007484 loss_d: -40.412761 time: 11.8 s\n",
      "[22 1200] loss_ae: 17.466281 loss_g: 42.037447 loss_d: -38.047078 time: 11.6 s\n",
      "[22 1250] loss_ae: 16.686778 loss_g: 38.113819 loss_d: -33.280424 time: 11.9 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  but i don ' t know what the school system is but i </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  they don ' t do </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  to have you know they have a they have a they have a they have a they have a they have a they have a </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  he just he just he just he just he just you know he just <unk> a </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  do they </s>\n",
      "true response:  oh </s>\n",
      "generate response:  and </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and i just don ' t have to worry about it but i don ' t know what it is but i don ' t know </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  and yeah </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and we had to put a little bit of <unk> and i think that ' s the way we did it and i think it ' </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah they ' ve got </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  you know you ' re you ' re you ' re you ' re you ' re a you ' re a you ' re a <unk> you </s>\n",
      "true response:  but they </s>\n",
      "generate response:  so they don ' t do that much for the whole thing but i think it ' s a lot of </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  i don ' t i don ' t know i don ' t know i don ' t know i don ' t know i don ' t </s>\n",
      "true response:  and </s>\n",
      "generate response:  you know i was just thinking about it i think it was a good movie i guess it was </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  they don ' t they don ' t </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  and you know they don ' t have to have a lot of </s>\n",
      "BLEU1 0.418964, BLEU2 0.336116, BLEU3 0.280271, BLEU4 0.223299, inter_dist1 0.006147, inter_dist2 0.034528 avg_len 13.742930\n",
      " time: 202.9 s\n",
      "Done testing\n",
      "Epoch:  23\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[23 50] loss_ae: 16.348063 loss_g: 44.315454 loss_d: -40.222847 time: 11.7 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23 100] loss_ae: 16.347742 loss_g: 41.656369 loss_d: -36.345287 time: 11.9 s\n",
      "[23 150] loss_ae: 16.496165 loss_g: 45.731339 loss_d: -39.214445 time: 11.8 s\n",
      "[23 200] loss_ae: 16.712152 loss_g: 43.293928 loss_d: -38.145871 time: 11.7 s\n",
      "[23 250] loss_ae: 15.576974 loss_g: 38.719015 loss_d: -36.966442 time: 11.8 s\n",
      "[23 300] loss_ae: 16.776312 loss_g: 44.333146 loss_d: -38.556081 time: 11.9 s\n",
      "[23 350] loss_ae: 16.802333 loss_g: 43.260780 loss_d: -36.988033 time: 11.7 s\n",
      "[23 400] loss_ae: 16.874404 loss_g: 39.533107 loss_d: -32.992594 time: 11.7 s\n",
      "[23 450] loss_ae: 17.790325 loss_g: 38.940751 loss_d: -36.524079 time: 11.8 s\n",
      "[23 500] loss_ae: 16.916476 loss_g: 37.040757 loss_d: -33.396891 time: 11.8 s\n",
      "[23 550] loss_ae: 16.114051 loss_g: 41.441024 loss_d: -35.887354 time: 11.9 s\n",
      "[23 600] loss_ae: 17.167469 loss_g: 41.164264 loss_d: -38.557910 time: 11.6 s\n",
      "[23 650] loss_ae: 16.034474 loss_g: 44.030381 loss_d: -38.581377 time: 12.0 s\n",
      "[23 700] loss_ae: 15.767735 loss_g: 44.336854 loss_d: -40.305299 time: 11.8 s\n",
      "[23 750] loss_ae: 18.104985 loss_g: 48.010062 loss_d: -42.134521 time: 11.7 s\n",
      "[23 800] loss_ae: 17.167715 loss_g: 42.416359 loss_d: -38.986621 time: 12.0 s\n",
      "[23 850] loss_ae: 18.408313 loss_g: 44.629641 loss_d: -39.830681 time: 11.8 s\n",
      "[23 900] loss_ae: 18.082818 loss_g: 43.537908 loss_d: -38.682111 time: 11.8 s\n",
      "[23 950] loss_ae: 17.311356 loss_g: 42.599846 loss_d: -39.232235 time: 11.9 s\n",
      "[23 1000] loss_ae: 16.230211 loss_g: 39.137139 loss_d: -32.265491 time: 11.8 s\n",
      "[23 1050] loss_ae: 17.689706 loss_g: 41.779037 loss_d: -37.681306 time: 11.6 s\n",
      "[23 1100] loss_ae: 16.180980 loss_g: 41.650870 loss_d: -35.484951 time: 11.9 s\n",
      "[23 1150] loss_ae: 15.947913 loss_g: 41.972063 loss_d: -37.318729 time: 12.0 s\n",
      "[23 1200] loss_ae: 15.867538 loss_g: 43.644292 loss_d: -38.043173 time: 11.7 s\n",
      "[23 1250] loss_ae: 19.790523 loss_g: 40.896149 loss_d: -37.403729 time: 11.8 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  or at least at least in the end of the year and then they have to go back and forth and i don ' t think it ' s </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  um - hum what ' s the difference i ' m not sure </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and just like the idea of what you know is if you ' re going to have to pay for the government to make a decision to make it </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  that ' s not it ' s not a real good thing to do you know if you ' re not going to have to </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  and at least that ' s the only thing that i ' ve ever done is you know i ' ve got some of the <unk> and stuff and </s>\n",
      "true response:  oh </s>\n",
      "generate response:  oh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and and and then if you if you if you if you if you if you if you if you if you don ' t have to go out to\n",
      "true response:  uh - huh </s>\n",
      "generate response:  and i </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  to have a <unk> in a <unk> i ' ve got a <unk> in a <unk> <unk> <unk> <unk> </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  i don ' t </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and you know you ' re going to have to do it you know you ' re going to have to do it you know and you know </s>\n",
      "true response:  but they </s>\n",
      "generate response:  and he has a lot of <unk> and he has a lot of <unk> and so forth and so forth and so forth </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah i just got a little bit of it i mean i just i just love it </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and we have a lot of <unk> and we have a lot of <unk> and we have a lot of </s>\n",
      "true response:  and </s>\n",
      "generate response:  we do have a lot of <unk> and we have a lot of <unk> and we have a lot of <unk> and we have a lot of </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  yeah i don ' t think it ' s going to be a lot of fun because i think it ' s a lot of fun </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  you know they don ' t do that </s>\n",
      "BLEU1 0.437273, BLEU2 0.351062, BLEU3 0.293505, BLEU4 0.234309, inter_dist1 0.005556, inter_dist2 0.032941 avg_len 15.334793\n",
      " time: 201.9 s\n",
      "Done testing\n",
      "Epoch:  24\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[24 50] loss_ae: 18.810518 loss_g: 40.167289 loss_d: -33.862852 time: 11.9 s\n",
      "[24 100] loss_ae: 18.146583 loss_g: 41.764695 loss_d: -36.097821 time: 11.8 s\n",
      "[24 150] loss_ae: 17.248887 loss_g: 45.133337 loss_d: -41.018758 time: 11.9 s\n",
      "[24 200] loss_ae: 16.811794 loss_g: 45.060082 loss_d: -38.308241 time: 12.0 s\n",
      "[24 250] loss_ae: 15.135915 loss_g: 48.505534 loss_d: -40.952227 time: 11.6 s\n",
      "[24 300] loss_ae: 14.895321 loss_g: 47.291475 loss_d: -40.789611 time: 11.7 s\n",
      "[24 350] loss_ae: 17.699542 loss_g: 46.974433 loss_d: -40.974482 time: 12.0 s\n",
      "[24 400] loss_ae: 16.740399 loss_g: 46.898412 loss_d: -41.349179 time: 11.7 s\n",
      "[24 450] loss_ae: 17.066624 loss_g: 41.818894 loss_d: -35.042038 time: 11.8 s\n",
      "[24 500] loss_ae: 16.170379 loss_g: 44.397548 loss_d: -41.250886 time: 12.0 s\n",
      "[24 550] loss_ae: 16.728425 loss_g: 44.716348 loss_d: -39.059514 time: 11.9 s\n",
      "[24 600] loss_ae: 15.222616 loss_g: 42.479560 loss_d: -36.367907 time: 11.8 s\n",
      "[24 650] loss_ae: 15.817594 loss_g: 43.604511 loss_d: -39.672067 time: 12.0 s\n",
      "[24 700] loss_ae: 17.545719 loss_g: 44.732549 loss_d: -39.259696 time: 12.0 s\n",
      "[24 750] loss_ae: 18.643936 loss_g: 42.053616 loss_d: -35.214144 time: 11.7 s\n",
      "[24 800] loss_ae: 17.596494 loss_g: 42.805734 loss_d: -38.137107 time: 11.8 s\n",
      "[24 850] loss_ae: 16.471172 loss_g: 43.637465 loss_d: -36.868938 time: 11.9 s\n",
      "[24 900] loss_ae: 16.370548 loss_g: 42.617100 loss_d: -35.710055 time: 11.7 s\n",
      "[24 950] loss_ae: 16.196424 loss_g: 46.711407 loss_d: -39.346350 time: 11.7 s\n",
      "[24 1000] loss_ae: 16.495224 loss_g: 48.043304 loss_d: -38.730840 time: 11.9 s\n",
      "[24 1050] loss_ae: 16.673865 loss_g: 49.001400 loss_d: -40.612492 time: 11.8 s\n",
      "[24 1100] loss_ae: 15.040879 loss_g: 43.505025 loss_d: -40.483474 time: 11.7 s\n",
      "[24 1150] loss_ae: 15.574737 loss_g: 45.961912 loss_d: -41.112068 time: 12.0 s\n",
      "[24 1200] loss_ae: 16.608355 loss_g: 44.010566 loss_d: -38.406647 time: 11.6 s\n",
      "[24 1250] loss_ae: 17.781084 loss_g: 50.989206 loss_d: -44.309528 time: 11.9 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  to be in the same age and they ' re in the process of the school system and they ' re not going to be </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  i ' m a little bit of a <unk> of my own business i ' m not sure what i ' m doing </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  oh they ' re they ' re just they ' re just a little bit of a </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  just to have a <unk> of a <unk> you know you have to have a <unk> </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  i know i don ' t know i don ' t know i don ' t know if they ' re familiar with it </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  oh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  but you know there ' s a lot of things that you can do with it is that you can ' t get a lot of </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  uh - huh </s>\n",
      "generate response:  and i ' m i ' m a i ' m a i ' m a i ' m a i ' m a i ' m a </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  i don ' t know i just i just i just love it i love it i love it i love it </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  oh </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and </s>\n",
      "true response:  but they </s>\n",
      "generate response:  oh yeah that ' s what they ' re doing is they ' re </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  with some of the things that you can do is you can ' t even get a little bit of a little bit of a </s>\n",
      "true response:  and </s>\n",
      "generate response:  i don ' t think they ' re going to be in the in the in the in the in the in the in the in the in the </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  yeah </s>\n",
      "BLEU1 0.432648, BLEU2 0.348465, BLEU3 0.291695, BLEU4 0.232985, inter_dist1 0.006190, inter_dist2 0.034466 avg_len 14.795475\n",
      " time: 202.0 s\n",
      "Done testing\n",
      "Epoch:  25\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[25 50] loss_ae: 15.143997 loss_g: 46.089544 loss_d: -39.604639 time: 12.0 s\n",
      "[25 100] loss_ae: 18.730410 loss_g: 46.808474 loss_d: -42.575539 time: 11.7 s\n",
      "[25 150] loss_ae: 16.895222 loss_g: 43.060988 loss_d: -37.360276 time: 11.8 s\n",
      "[25 200] loss_ae: 15.772703 loss_g: 46.850091 loss_d: -39.583620 time: 11.6 s\n",
      "[25 250] loss_ae: 17.613225 loss_g: 47.036244 loss_d: -43.272350 time: 11.8 s\n",
      "[25 300] loss_ae: 18.032924 loss_g: 49.822355 loss_d: -43.641933 time: 11.7 s\n",
      "[25 350] loss_ae: 17.635075 loss_g: 42.318653 loss_d: -39.686433 time: 12.0 s\n",
      "[25 400] loss_ae: 15.018514 loss_g: 45.697793 loss_d: -40.458568 time: 12.0 s\n",
      "[25 450] loss_ae: 16.816013 loss_g: 48.228294 loss_d: -41.973058 time: 11.9 s\n",
      "[25 500] loss_ae: 14.697217 loss_g: 47.094738 loss_d: -39.451006 time: 11.6 s\n",
      "[25 550] loss_ae: 18.798486 loss_g: 48.244706 loss_d: -43.369258 time: 11.5 s\n",
      "[25 600] loss_ae: 18.312570 loss_g: 49.467687 loss_d: -43.147260 time: 11.4 s\n",
      "[25 650] loss_ae: 17.203129 loss_g: 48.511607 loss_d: -41.569043 time: 11.5 s\n",
      "[25 700] loss_ae: 17.049672 loss_g: 42.037416 loss_d: -34.270724 time: 11.6 s\n",
      "[25 750] loss_ae: 16.960269 loss_g: 47.650930 loss_d: -43.036179 time: 11.6 s\n",
      "[25 800] loss_ae: 14.386598 loss_g: 39.690213 loss_d: -34.203982 time: 11.4 s\n",
      "[25 850] loss_ae: 17.009286 loss_g: 38.483578 loss_d: -35.208258 time: 11.3 s\n",
      "[25 900] loss_ae: 15.783658 loss_g: 47.424619 loss_d: -41.482886 time: 11.5 s\n",
      "[25 950] loss_ae: 16.343003 loss_g: 47.577113 loss_d: -42.602576 time: 11.6 s\n",
      "[25 1000] loss_ae: 14.414165 loss_g: 46.040567 loss_d: -40.302744 time: 11.6 s\n",
      "[25 1050] loss_ae: 16.775859 loss_g: 49.367312 loss_d: -42.694313 time: 11.5 s\n",
      "[25 1100] loss_ae: 16.283870 loss_g: 49.605407 loss_d: -43.424687 time: 11.3 s\n",
      "[25 1150] loss_ae: 17.570961 loss_g: 51.499385 loss_d: -43.004948 time: 11.6 s\n",
      "[25 1200] loss_ae: 16.159131 loss_g: 48.155355 loss_d: -40.444028 time: 11.6 s\n",
      "[25 1250] loss_ae: 17.327567 loss_g: 51.710331 loss_d: -44.948028 time: 11.4 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  maybe it ' s not a good one it ' s not the one that ' s not the one that ' s the one that ' s the one\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  in the winter time i ' ll be able to go to a dinner party and go to the beach </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh well i ' m not sure that it ' s not the case i ' m not sure that it ' s not </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and i just don ' t have a lot of <unk> i just don ' t have a lot of time to do that </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  you know i just don ' t know i just don ' t think i ' d ever get to see that as a as a </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  you </s>\n",
      "true response:  oh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and i ' ve you know i ' ve been to a <unk> <unk> and we have a lot of <unk> and we have a lot of <unk> </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  we ' ve got we have a we have a we have a we have a <unk> <unk> we have a <unk> <unk> we have a <unk> <unk> </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and you know they have a they have a they have a they have a they have a <unk> and they have a </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  yeah they have you know they have a they have a they have a they have a they have a they have a you have a </s>\n",
      "true response:  but they </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  and </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and there are a lot of people that are going to be able to do that and they ' re going to be </s>\n",
      "true response:  and </s>\n",
      "generate response:  and i ' ll tell you if you ' re going to go to a restaurant and you ' re going to go to a restaurant and </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  yeah but it ' s a it ' s a </s>\n",
      "BLEU1 0.431579, BLEU2 0.346780, BLEU3 0.289622, BLEU4 0.231221, inter_dist1 0.006185, inter_dist2 0.037320 avg_len 15.369093\n",
      " time: 213.5 s\n",
      "Done testing\n",
      "Epoch:  26\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[26 50] loss_ae: 14.079347 loss_g: 45.967590 loss_d: -40.815661 time: 11.7 s\n",
      "[26 100] loss_ae: 17.679514 loss_g: 51.126518 loss_d: -44.825544 time: 11.6 s\n",
      "[26 150] loss_ae: 16.266575 loss_g: 47.146648 loss_d: -41.085649 time: 12.0 s\n",
      "[26 200] loss_ae: 17.420394 loss_g: 52.102996 loss_d: -45.226670 time: 11.7 s\n",
      "[26 250] loss_ae: 15.940011 loss_g: 52.262727 loss_d: -45.685558 time: 11.7 s\n",
      "[26 300] loss_ae: 17.597626 loss_g: 51.215513 loss_d: -49.239027 time: 11.8 s\n",
      "[26 350] loss_ae: 16.645432 loss_g: 46.864468 loss_d: -40.698732 time: 11.9 s\n",
      "[26 400] loss_ae: 17.043579 loss_g: 44.266970 loss_d: -36.527683 time: 11.8 s\n",
      "[26 450] loss_ae: 16.637668 loss_g: 50.937407 loss_d: -43.821802 time: 11.6 s\n",
      "[26 500] loss_ae: 16.867652 loss_g: 42.860980 loss_d: -37.717755 time: 11.7 s\n",
      "[26 550] loss_ae: 16.715432 loss_g: 41.973382 loss_d: -40.559549 time: 12.0 s\n",
      "[26 600] loss_ae: 15.583241 loss_g: 55.884761 loss_d: -46.715642 time: 11.7 s\n",
      "[26 650] loss_ae: 16.282296 loss_g: 45.964959 loss_d: -36.915061 time: 11.9 s\n",
      "[26 700] loss_ae: 14.808567 loss_g: 48.966471 loss_d: -41.984671 time: 11.9 s\n",
      "[26 750] loss_ae: 15.486312 loss_g: 50.175134 loss_d: -43.544801 time: 11.8 s\n",
      "[26 800] loss_ae: 16.413668 loss_g: 51.760273 loss_d: -45.754810 time: 11.8 s\n",
      "[26 850] loss_ae: 16.137273 loss_g: 55.152061 loss_d: -47.216538 time: 12.0 s\n",
      "[26 900] loss_ae: 14.786610 loss_g: 48.215132 loss_d: -41.111971 time: 12.0 s\n",
      "[26 950] loss_ae: 17.079914 loss_g: 50.910553 loss_d: -44.069237 time: 11.8 s\n",
      "[26 1000] loss_ae: 16.309474 loss_g: 48.945787 loss_d: -43.036555 time: 11.9 s\n",
      "[26 1050] loss_ae: 16.197307 loss_g: 48.602519 loss_d: -44.638509 time: 11.9 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26 1100] loss_ae: 15.954533 loss_g: 46.110864 loss_d: -41.220070 time: 11.7 s\n",
      "[26 1150] loss_ae: 15.281787 loss_g: 54.024956 loss_d: -48.768736 time: 11.8 s\n",
      "[26 1200] loss_ae: 15.793415 loss_g: 47.190856 loss_d: -40.651883 time: 12.0 s\n",
      "[26 1250] loss_ae: 18.749435 loss_g: 53.990301 loss_d: -45.938214 time: 11.6 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  do you think that ' s the most important thing to do with the idea of having a family reunion that ' s a good idea </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and but you know the guy who ' s the one who ' s the one who ' s the only one that ' s in the </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  and the guy who ' s the one that ' s the one that ' s the one that ' s the one that ' s the one that '\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  right </s>\n",
      "true response:  oh </s>\n",
      "generate response:  oh you are you know we ' re just a <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and we have been <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  and but but they would they would have been a lot more fun to do with them but i think that ' s a good idea </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  or <unk> <unk> i like to have a <unk> <unk> and i have a friend who ' s who is a <unk> and a cat </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and they ' re trying to get a little bit of money that ' s going to be a lot of fun and </s>\n",
      "true response:  but they </s>\n",
      "generate response:  but </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  right </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and i ' m not sure what i ' m saying i ' m not sure what i ' m saying i ' m not sure </s>\n",
      "true response:  and </s>\n",
      "generate response:  and i just don ' t have a problem with that i think that ' s a good idea </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  yeah it ' s yeah </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  yeah </s>\n",
      "BLEU1 0.436206, BLEU2 0.352032, BLEU3 0.295518, BLEU4 0.236640, inter_dist1 0.006369, inter_dist2 0.039564 avg_len 15.754607\n",
      " time: 203.0 s\n",
      "Done testing\n",
      "Epoch:  27\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[27 50] loss_ae: 14.881766 loss_g: 50.978363 loss_d: -43.478839 time: 12.0 s\n",
      "[27 100] loss_ae: 17.235064 loss_g: 52.458502 loss_d: -43.327094 time: 11.7 s\n",
      "[27 150] loss_ae: 14.612134 loss_g: 45.165401 loss_d: -39.657961 time: 11.8 s\n",
      "[27 200] loss_ae: 16.796033 loss_g: 51.231155 loss_d: -45.515330 time: 11.7 s\n",
      "[27 250] loss_ae: 15.820251 loss_g: 52.059061 loss_d: -46.923714 time: 11.8 s\n",
      "[27 300] loss_ae: 15.095241 loss_g: 50.140255 loss_d: -44.836277 time: 11.8 s\n",
      "[27 350] loss_ae: 13.826114 loss_g: 46.341727 loss_d: -40.601502 time: 12.0 s\n",
      "[27 400] loss_ae: 16.126559 loss_g: 52.035012 loss_d: -45.530728 time: 12.1 s\n",
      "[27 450] loss_ae: 17.501504 loss_g: 49.223939 loss_d: -44.523958 time: 11.9 s\n",
      "[27 500] loss_ae: 18.694036 loss_g: 51.976220 loss_d: -48.374865 time: 11.8 s\n",
      "[27 550] loss_ae: 15.689443 loss_g: 48.164884 loss_d: -43.411593 time: 11.7 s\n",
      "[27 600] loss_ae: 16.912629 loss_g: 48.961057 loss_d: -42.790179 time: 11.9 s\n",
      "[27 650] loss_ae: 15.693347 loss_g: 52.315011 loss_d: -48.578607 time: 11.7 s\n",
      "[27 700] loss_ae: 16.009072 loss_g: 49.197370 loss_d: -43.516274 time: 11.7 s\n",
      "[27 750] loss_ae: 16.530701 loss_g: 47.577532 loss_d: -39.334651 time: 11.6 s\n",
      "[27 800] loss_ae: 16.933428 loss_g: 51.441506 loss_d: -47.784851 time: 11.7 s\n",
      "[27 850] loss_ae: 17.251082 loss_g: 53.164142 loss_d: -44.220753 time: 11.6 s\n",
      "[27 900] loss_ae: 17.868332 loss_g: 49.010161 loss_d: -43.618446 time: 11.6 s\n",
      "[27 950] loss_ae: 17.677245 loss_g: 51.641522 loss_d: -43.703614 time: 11.8 s\n",
      "[27 1000] loss_ae: 15.629654 loss_g: 57.642005 loss_d: -49.522061 time: 11.6 s\n",
      "[27 1050] loss_ae: 15.015203 loss_g: 56.531092 loss_d: -51.978497 time: 11.7 s\n",
      "[27 1100] loss_ae: 15.855712 loss_g: 54.662396 loss_d: -47.418662 time: 11.7 s\n",
      "[27 1150] loss_ae: 16.831156 loss_g: 53.129368 loss_d: -45.156502 time: 11.7 s\n",
      "[27 1200] loss_ae: 14.705269 loss_g: 52.708718 loss_d: -48.040897 time: 11.5 s\n",
      "[27 1250] loss_ae: 14.254158 loss_g: 48.158137 loss_d: -42.581869 time: 11.7 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and but you know it ' s not that much of a problem but </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  i have to say that i ' m going to have to do that and i ' m going to have to do that and i ' m </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  yeah i know what i mean to do </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and you have to have a you have a you have a you have a you have a <unk> you have to have a <unk> </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  you have to have a you have a you have a you have a you have a you have a <unk> you have to have a </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  and you know we were in the middle of the country and we were in the middle of the country and we had a lot of </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  but i ' m it ' s i don ' t know i don ' t think i ' m going to do that </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  and you </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  do well that ' s what we ' re going to do is we ' re going to have to do this and this is just this is </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  you know a lot of a lot of people have a lot of people </s>\n",
      "true response:  but they </s>\n",
      "generate response:  and do it so i don ' t know if it ' s a problem or not but i think it ' s a lot of </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  sure </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know and you know you ' re going to have to do something like that you know and i think that ' s the way </s>\n",
      "true response:  and </s>\n",
      "generate response:  <unk> <unk> i have a friend who ' s a <unk> and i have a friend who is a <unk> <unk> <unk> <unk> </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  have you do you have you have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  um - hum </s>\n",
      "generate response:  yeah </s>\n",
      "BLEU1 0.433188, BLEU2 0.347611, BLEU3 0.291351, BLEU4 0.233199, inter_dist1 0.006540, inter_dist2 0.038785 avg_len 14.814085\n",
      " time: 214.8 s\n",
      "Done testing\n",
      "Epoch:  28\n",
      "Train begins with 6398 batches with 12 left over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28 50] loss_ae: 16.715142 loss_g: 47.754024 loss_d: -39.335933 time: 11.9 s\n",
      "[28 100] loss_ae: 14.990292 loss_g: 53.663593 loss_d: -49.795186 time: 11.7 s\n",
      "[28 150] loss_ae: 14.135208 loss_g: 55.472925 loss_d: -48.524078 time: 12.0 s\n",
      "[28 200] loss_ae: 16.764632 loss_g: 54.909294 loss_d: -46.861897 time: 12.0 s\n",
      "[28 250] loss_ae: 16.466419 loss_g: 47.914995 loss_d: -41.882195 time: 11.8 s\n",
      "[28 300] loss_ae: 15.170659 loss_g: 48.991169 loss_d: -45.397120 time: 11.9 s\n",
      "[28 350] loss_ae: 16.410373 loss_g: 52.057193 loss_d: -45.704337 time: 11.7 s\n",
      "[28 400] loss_ae: 17.192936 loss_g: 54.001135 loss_d: -48.032908 time: 12.1 s\n",
      "[28 450] loss_ae: 15.972217 loss_g: 53.668984 loss_d: -47.990660 time: 11.9 s\n",
      "[28 500] loss_ae: 15.152699 loss_g: 49.934871 loss_d: -43.338299 time: 11.7 s\n",
      "[28 550] loss_ae: 15.890078 loss_g: 56.351540 loss_d: -51.909413 time: 11.9 s\n",
      "[28 600] loss_ae: 14.326134 loss_g: 52.114688 loss_d: -45.404619 time: 12.0 s\n",
      "[28 650] loss_ae: 17.834261 loss_g: 58.039721 loss_d: -48.441588 time: 11.8 s\n",
      "[28 700] loss_ae: 15.442334 loss_g: 58.413366 loss_d: -48.834096 time: 11.7 s\n",
      "[28 750] loss_ae: 14.835587 loss_g: 46.632239 loss_d: -41.199192 time: 11.9 s\n",
      "[28 800] loss_ae: 15.217955 loss_g: 52.072198 loss_d: -45.952901 time: 11.8 s\n",
      "[28 850] loss_ae: 15.125182 loss_g: 48.686682 loss_d: -41.366613 time: 11.8 s\n",
      "[28 900] loss_ae: 14.742829 loss_g: 50.149885 loss_d: -42.518390 time: 11.8 s\n",
      "[28 950] loss_ae: 15.044400 loss_g: 54.217238 loss_d: -46.632089 time: 11.9 s\n",
      "[28 1000] loss_ae: 15.382131 loss_g: 52.219809 loss_d: -45.240216 time: 11.7 s\n",
      "[28 1050] loss_ae: 18.019947 loss_g: 50.310039 loss_d: -45.220476 time: 12.3 s\n",
      "[28 1100] loss_ae: 15.460619 loss_g: 56.412971 loss_d: -49.445593 time: 11.9 s\n",
      "[28 1150] loss_ae: 16.431533 loss_g: 51.211087 loss_d: -48.859360 time: 11.7 s\n",
      "[28 1200] loss_ae: 16.440034 loss_g: 57.500886 loss_d: -49.722377 time: 11.9 s\n",
      "[28 1250] loss_ae: 16.329282 loss_g: 56.774403 loss_d: -52.342939 time: 11.9 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  but there ' s a lot of people that are in there and they ' re not going to be able to do a lot of things </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  and i was thinking that the only thing that i would like to see is the most important thing is that you can do </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  i don ' t </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  i don ' t have to worry about it but i don ' t think that it ' s </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  yeah i know it was it was it was it was a it was a little <unk> and it was a little bit of a <unk> but it was </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  oh i just love it just like it ' s just </s>\n",
      "true response:  oh </s>\n",
      "generate response:  there </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and he has a real big problem with him and he ' s been real good about it and he ' s been in the middle of the </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  at least you ' re right on the other side of the country and what ' s the other one is that </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and they have a they have a they have a they have a <unk> you know </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  they have a big problem with the kids that they ' re going to go to school and they ' re going to go and they ' re </s>\n",
      "true response:  but they </s>\n",
      "generate response:  but </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah they ' re just they ' re just they ' re just they ' re just they ' re just a little <unk> </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know a lot of people have to have a <unk> you know </s>\n",
      "true response:  and </s>\n",
      "generate response:  and we have a we have a we have a we have a we have a we have a <unk> <unk> <unk> </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  oh you are you are </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.436151, BLEU2 0.354673, BLEU3 0.299077, BLEU4 0.240128, inter_dist1 0.005835, inter_dist2 0.034161 avg_len 16.603722\n",
      " time: 202.1 s\n",
      "Done testing\n",
      "Epoch:  29\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[29 50] loss_ae: 15.486361 loss_g: 53.375088 loss_d: -45.214425 time: 11.7 s\n",
      "[29 100] loss_ae: 15.599169 loss_g: 56.934351 loss_d: -48.056163 time: 12.0 s\n",
      "[29 150] loss_ae: 15.188733 loss_g: 51.273049 loss_d: -47.438439 time: 11.7 s\n",
      "[29 200] loss_ae: 15.484484 loss_g: 53.092842 loss_d: -47.237746 time: 12.0 s\n",
      "[29 250] loss_ae: 15.511839 loss_g: 51.794969 loss_d: -47.455196 time: 12.1 s\n",
      "[29 300] loss_ae: 15.087134 loss_g: 55.191558 loss_d: -48.567363 time: 11.8 s\n",
      "[29 350] loss_ae: 16.753786 loss_g: 53.198312 loss_d: -44.205636 time: 11.7 s\n",
      "[29 400] loss_ae: 15.951508 loss_g: 61.443828 loss_d: -52.918342 time: 12.1 s\n",
      "[29 450] loss_ae: 14.913643 loss_g: 55.476651 loss_d: -49.458628 time: 11.9 s\n",
      "[29 500] loss_ae: 14.508130 loss_g: 49.417152 loss_d: -43.175949 time: 12.0 s\n",
      "[29 550] loss_ae: 15.488953 loss_g: 58.132638 loss_d: -46.588979 time: 12.0 s\n",
      "[29 600] loss_ae: 17.974916 loss_g: 60.715235 loss_d: -53.904989 time: 11.6 s\n",
      "[29 650] loss_ae: 16.561953 loss_g: 52.974928 loss_d: -45.775798 time: 11.4 s\n",
      "[29 700] loss_ae: 15.296912 loss_g: 53.177793 loss_d: -45.153294 time: 11.6 s\n",
      "[29 750] loss_ae: 14.887184 loss_g: 53.459969 loss_d: -45.229458 time: 11.4 s\n",
      "[29 800] loss_ae: 15.736650 loss_g: 56.882799 loss_d: -50.514893 time: 11.7 s\n",
      "[29 850] loss_ae: 15.721695 loss_g: 55.193059 loss_d: -52.109291 time: 11.7 s\n",
      "[29 900] loss_ae: 15.854343 loss_g: 56.434601 loss_d: -48.704982 time: 11.4 s\n",
      "[29 950] loss_ae: 14.737319 loss_g: 59.186632 loss_d: -53.594709 time: 11.4 s\n",
      "[29 1000] loss_ae: 15.645173 loss_g: 57.450653 loss_d: -48.864067 time: 11.7 s\n",
      "[29 1050] loss_ae: 17.132299 loss_g: 61.124136 loss_d: -53.571828 time: 11.6 s\n",
      "[29 1100] loss_ae: 15.549518 loss_g: 53.267342 loss_d: -42.110215 time: 11.5 s\n",
      "[29 1150] loss_ae: 18.500915 loss_g: 56.262442 loss_d: -46.938285 time: 11.5 s\n",
      "[29 1200] loss_ae: 15.110883 loss_g: 53.221674 loss_d: -44.467429 time: 11.5 s\n",
      "[29 1250] loss_ae: 18.513333 loss_g: 58.856781 loss_d: -49.751713 time: 11.5 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and we have a lot of people that are in the in the world and they ' re not they ' re not </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  yeah well what about you </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  i would think </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and they have a lot of problems with the people who are in their own business and they ' re not </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  a </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  the other day that </s>\n",
      "true response:  oh </s>\n",
      "generate response:  oh you know you just got a lot of a lot of a lot of a lot of a lot of a lot of people </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  to </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  oh yes </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  or you know just like the <unk> or something like that </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  uh - huh </s>\n",
      "generate response:  </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  have been in a situation where they were in a nursing home and </s>\n",
      "true response:  but they </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  oh you know like that </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know if you if you ' re a kid you ' re a lot more than you ' re in a you know you ' re a </s>\n",
      "true response:  and </s>\n",
      "generate response:  do it all right do you have a favorite team or </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  they do have a lot of good stuff there ' s a lot of good places to go </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  uh - huh </s>\n",
      "BLEU1 0.439508, BLEU2 0.354773, BLEU3 0.297834, BLEU4 0.238537, inter_dist1 0.006896, inter_dist2 0.042368 avg_len 15.663200\n",
      " time: 219.0 s\n",
      "Done testing\n",
      "Epoch:  30\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[30 50] loss_ae: 15.525765 loss_g: 48.377546 loss_d: -39.890220 time: 12.0 s\n",
      "[30 100] loss_ae: 17.997323 loss_g: 59.961613 loss_d: -49.250782 time: 11.7 s\n",
      "[30 150] loss_ae: 15.327624 loss_g: 56.286661 loss_d: -48.760371 time: 11.8 s\n",
      "[30 200] loss_ae: 16.857894 loss_g: 56.787786 loss_d: -49.235064 time: 12.0 s\n",
      "[30 250] loss_ae: 17.246350 loss_g: 61.632731 loss_d: -51.723396 time: 11.7 s\n",
      "[30 300] loss_ae: 14.493207 loss_g: 55.639427 loss_d: -50.572833 time: 11.9 s\n",
      "[30 350] loss_ae: 16.462366 loss_g: 61.158258 loss_d: -51.821669 time: 11.9 s\n",
      "[30 400] loss_ae: 15.382878 loss_g: 56.284183 loss_d: -46.759372 time: 11.8 s\n",
      "[30 450] loss_ae: 15.636837 loss_g: 50.573511 loss_d: -40.583774 time: 11.8 s\n",
      "[30 500] loss_ae: 16.262769 loss_g: 50.976774 loss_d: -44.246571 time: 11.9 s\n",
      "[30 550] loss_ae: 13.561823 loss_g: 46.900845 loss_d: -43.010626 time: 11.7 s\n",
      "[30 600] loss_ae: 16.284628 loss_g: 49.553924 loss_d: -43.449382 time: 11.8 s\n",
      "[30 650] loss_ae: 15.342958 loss_g: 55.705904 loss_d: -47.109709 time: 11.9 s\n",
      "[30 700] loss_ae: 15.291234 loss_g: 52.627505 loss_d: -46.599937 time: 11.8 s\n",
      "[30 750] loss_ae: 15.580880 loss_g: 58.426196 loss_d: -53.705471 time: 11.8 s\n",
      "[30 800] loss_ae: 14.757406 loss_g: 56.220371 loss_d: -46.892841 time: 11.6 s\n",
      "[30 850] loss_ae: 15.638923 loss_g: 54.751577 loss_d: -44.439589 time: 12.0 s\n",
      "[30 900] loss_ae: 15.913124 loss_g: 55.502028 loss_d: -48.330923 time: 11.9 s\n",
      "[30 950] loss_ae: 15.570442 loss_g: 53.672760 loss_d: -43.335843 time: 11.7 s\n",
      "[30 1000] loss_ae: 14.618551 loss_g: 63.845668 loss_d: -56.943584 time: 11.9 s\n",
      "[30 1050] loss_ae: 18.877027 loss_g: 61.119172 loss_d: -50.352800 time: 11.8 s\n",
      "[30 1100] loss_ae: 16.207818 loss_g: 59.791787 loss_d: -49.429866 time: 11.8 s\n",
      "[30 1150] loss_ae: 16.107425 loss_g: 61.609329 loss_d: -49.291532 time: 11.8 s\n",
      "[30 1200] loss_ae: 15.311591 loss_g: 54.553197 loss_d: -45.397618 time: 12.0 s\n",
      "[30 1250] loss_ae: 15.094068 loss_g: 52.955338 loss_d: -42.973395 time: 11.8 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and i ' m not sure that it ' s not a good idea but i think it ' s a good idea to have a good education </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  but it </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  actually my husband ' s favorite </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and i ' ve just you know i ' ve never heard of that before i was in a position of <unk> </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  right </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  and </s>\n",
      "true response:  oh </s>\n",
      "generate response:  and we had to go to the park and we had a lot of snow and it was really nice </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  you know a lot of people have to go to the <unk> and you know the <unk> and the <unk> and the <unk> and the <unk> and the <unk> </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah and i ' ve been to a couple of years ago and i ' ve been to the point where i was in college and </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and we have a we have a we have a we have a we have a dog and we have a dog and we have a dog </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  really </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  you know and we ' re going to have to have a problem we ' re going to have to have a problem and we ' re going to </s>\n",
      "true response:  but they </s>\n",
      "generate response:  yeah oh yeah it ' s just it ' s just you know it ' s like a <unk> you know it ' s like </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and you know you don ' t have to worry about it and you don ' t know what you ' re saying </s>\n",
      "true response:  and </s>\n",
      "generate response:  just a little bit of a little bit of a lot of people you know they ' re just starting to get a lot of </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  that there ' s not that much of a lot of things that you don ' t know if you ' re going to have to </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  and i ' ve been i ' ve been in a nursing home and i was in a nursing home and i was in a </s>\n",
      "BLEU1 0.446753, BLEU2 0.362176, BLEU3 0.304577, BLEU4 0.244091, inter_dist1 0.006579, inter_dist2 0.039673 avg_len 15.695311\n",
      " time: 203.7 s\n",
      "Done testing\n",
      "Epoch:  31\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[31 50] loss_ae: 14.983789 loss_g: 56.987213 loss_d: -46.059263 time: 11.7 s\n",
      "[31 100] loss_ae: 15.517513 loss_g: 57.571619 loss_d: -47.729587 time: 12.0 s\n",
      "[31 150] loss_ae: 15.474257 loss_g: 51.370459 loss_d: -44.141612 time: 11.9 s\n",
      "[31 200] loss_ae: 16.484874 loss_g: 58.917546 loss_d: -48.550422 time: 11.7 s\n",
      "[31 250] loss_ae: 17.160121 loss_g: 56.142788 loss_d: -47.337138 time: 12.1 s\n",
      "[31 300] loss_ae: 13.957790 loss_g: 59.234595 loss_d: -49.721482 time: 11.8 s\n",
      "[31 350] loss_ae: 15.038291 loss_g: 59.528664 loss_d: -50.803951 time: 11.9 s\n",
      "[31 400] loss_ae: 14.284293 loss_g: 60.121385 loss_d: -52.225514 time: 11.9 s\n",
      "[31 450] loss_ae: 16.973546 loss_g: 60.962490 loss_d: -51.663531 time: 11.7 s\n",
      "[31 500] loss_ae: 15.037281 loss_g: 58.741708 loss_d: -50.141149 time: 11.9 s\n",
      "[31 550] loss_ae: 13.917273 loss_g: 63.700512 loss_d: -53.515408 time: 12.0 s\n",
      "[31 600] loss_ae: 15.110460 loss_g: 61.200941 loss_d: -55.462465 time: 12.0 s\n",
      "[31 650] loss_ae: 16.638881 loss_g: 61.706248 loss_d: -52.037774 time: 11.8 s\n",
      "[31 700] loss_ae: 14.595144 loss_g: 53.988508 loss_d: -45.385855 time: 11.6 s\n",
      "[31 750] loss_ae: 15.996122 loss_g: 64.808774 loss_d: -58.142393 time: 11.6 s\n",
      "[31 800] loss_ae: 16.116454 loss_g: 62.149012 loss_d: -52.530789 time: 11.8 s\n",
      "[31 850] loss_ae: 15.481002 loss_g: 56.165495 loss_d: -46.113802 time: 11.5 s\n",
      "[31 900] loss_ae: 16.363076 loss_g: 59.587185 loss_d: -54.133204 time: 11.7 s\n",
      "[31 950] loss_ae: 16.412849 loss_g: 54.620852 loss_d: -50.561827 time: 11.3 s\n",
      "[31 1000] loss_ae: 16.153364 loss_g: 56.359993 loss_d: -48.671608 time: 11.5 s\n",
      "[31 1050] loss_ae: 15.070144 loss_g: 56.152073 loss_d: -49.870321 time: 11.5 s\n",
      "[31 1100] loss_ae: 15.993953 loss_g: 65.240265 loss_d: -54.688820 time: 11.6 s\n",
      "[31 1150] loss_ae: 17.501685 loss_g: 56.689616 loss_d: -51.096206 time: 11.7 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31 1200] loss_ae: 14.499870 loss_g: 54.068565 loss_d: -43.466257 time: 11.7 s\n",
      "[31 1250] loss_ae: 17.580406 loss_g: 55.534192 loss_d: -49.338411 time: 11.6 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and they don ' t want to go to school and i don ' t know if they ' re going to be in the middle of the </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  not really a good idea i like to do that i like to do that </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  right and i </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and i don ' t i don ' t i don ' t i don ' t have a i don ' t have a i don ' t have\n",
      "true response:  something like that </s>\n",
      "generate response:  huh </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  you know like you know the <unk> and the <unk> and the <unk> and the <unk> and the <unk> and the <unk> and the <unk> and the <unk> </s>\n",
      "true response:  oh </s>\n",
      "generate response:  i know you know it was just it was just a really good movie </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  oh you ' ve you have to you have to have a you have a you have a you have a you have a you have a </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  they have a they have a they have a they have a they have a they have a they have a they have a they have a </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and you know like the big cities like that you know they ' re going to go to the city and you know the kids are just not </s>\n",
      "true response:  but they </s>\n",
      "generate response:  or if you have a car that ' s right i mean it ' s not like they ' re not </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  and </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know you know you can ' t do that you know i mean i ' m not i ' m not i ' m not i don ' t\n",
      "true response:  and </s>\n",
      "generate response:  and you know if you know if you know if you know if you know if you ' re going to be a good person ' s <unk> </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  they do that in the summer time and it ' s really nice </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  to do </s>\n",
      "BLEU1 0.443601, BLEU2 0.359909, BLEU3 0.304715, BLEU4 0.245542, inter_dist1 0.006137, inter_dist2 0.038555 avg_len 17.600255\n",
      " time: 214.4 s\n",
      "Done testing\n",
      "Epoch:  32\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[32 50] loss_ae: 13.394894 loss_g: 59.269228 loss_d: -49.816354 time: 10.5 s\n",
      "[32 100] loss_ae: 15.609291 loss_g: 59.901986 loss_d: -51.445218 time: 10.7 s\n",
      "[32 150] loss_ae: 14.840726 loss_g: 63.894504 loss_d: -56.575394 time: 10.4 s\n",
      "[32 200] loss_ae: 13.789292 loss_g: 55.575689 loss_d: -44.785660 time: 10.6 s\n",
      "[32 250] loss_ae: 15.258331 loss_g: 62.648441 loss_d: -51.529793 time: 10.8 s\n",
      "[32 300] loss_ae: 15.045021 loss_g: 57.471044 loss_d: -51.055726 time: 10.6 s\n",
      "[32 350] loss_ae: 15.539261 loss_g: 58.595920 loss_d: -49.161765 time: 10.4 s\n",
      "[32 400] loss_ae: 13.795545 loss_g: 60.207658 loss_d: -47.904705 time: 10.5 s\n",
      "[32 450] loss_ae: 17.231374 loss_g: 60.382512 loss_d: -52.010964 time: 10.6 s\n",
      "[32 500] loss_ae: 16.952661 loss_g: 56.591829 loss_d: -50.080515 time: 10.5 s\n",
      "[32 550] loss_ae: 14.378575 loss_g: 53.000557 loss_d: -43.557181 time: 10.5 s\n",
      "[32 600] loss_ae: 13.543579 loss_g: 59.979239 loss_d: -51.621348 time: 10.5 s\n",
      "[32 650] loss_ae: 15.439646 loss_g: 61.245682 loss_d: -49.361524 time: 10.5 s\n",
      "[32 700] loss_ae: 13.902019 loss_g: 50.923724 loss_d: -44.244059 time: 10.5 s\n",
      "[32 750] loss_ae: 13.458601 loss_g: 55.317086 loss_d: -46.231188 time: 10.4 s\n",
      "[32 800] loss_ae: 14.750897 loss_g: 59.912489 loss_d: -50.986073 time: 10.5 s\n",
      "[32 850] loss_ae: 14.626075 loss_g: 62.080600 loss_d: -53.230671 time: 10.5 s\n",
      "[32 900] loss_ae: 15.122270 loss_g: 57.212089 loss_d: -47.323944 time: 10.5 s\n",
      "[32 950] loss_ae: 14.755814 loss_g: 59.847103 loss_d: -51.599062 time: 10.5 s\n",
      "[32 1000] loss_ae: 15.039953 loss_g: 56.982054 loss_d: -48.395099 time: 10.4 s\n",
      "[32 1050] loss_ae: 14.034919 loss_g: 69.104930 loss_d: -59.717694 time: 10.2 s\n",
      "[32 1100] loss_ae: 14.747342 loss_g: 60.958077 loss_d: -47.266331 time: 10.8 s\n",
      "[32 1150] loss_ae: 12.754686 loss_g: 54.276726 loss_d: -47.556318 time: 10.5 s\n",
      "[32 1200] loss_ae: 15.284345 loss_g: 61.434378 loss_d: -52.733176 time: 10.2 s\n",
      "[32 1250] loss_ae: 15.212105 loss_g: 59.061719 loss_d: -51.710193 time: 10.5 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and he has a lot of <unk> and he ' s a <unk> and he ' s a <unk> and he ' s a <unk> </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  yeah i think it ' s a good idea i don ' t know i ' m not a real big one i ' m not </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  and we ' ve we ' ve we ' ve had a lot of <unk> and we have a lot of <unk> and </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  yeah i mean i really don ' t know what the answer is </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  they all have to be in the same place </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  and </s>\n",
      "true response:  oh </s>\n",
      "generate response:  and i don ' t know it ' s just it ' s just it ' s just a matter of fact i ' m not sure how </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and i ' ve you know i ' ve you know you can ' t have a <unk> you know you can ' t have to have </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  i </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  i don ' t know if you ' ve ever heard of it but they ' re not they ' re not they ' re not </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  then i think that ' s a good idea i think that ' s a good idea i think that ' s a good idea </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and the other thing is that you know they ' re not going to have to be <unk> to the extent of the country and </s>\n",
      "true response:  but they </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  oh really </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and you know a lot of people have to have a lot of people have to have a lot of people have to have a lot of </s>\n",
      "true response:  and </s>\n",
      "generate response:  and you know there is a lot of people that are going to do that and they ' re going to have to be </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  you know they ' re really not a lot of fun because they ' re so big and they ' re real nice and they ' re </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  i think it was a really good idea to have </s>\n",
      "BLEU1 0.455691, BLEU2 0.370465, BLEU3 0.313596, BLEU4 0.252636, inter_dist1 0.005984, inter_dist2 0.036557 avg_len 16.798394\n",
      " time: 184.1 s\n",
      "Done testing\n",
      "Epoch:  33\n",
      "Train begins with 6398 batches with 12 left over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33 50] loss_ae: 15.087180 loss_g: 52.778548 loss_d: -43.413756 time: 10.4 s\n",
      "[33 100] loss_ae: 15.104458 loss_g: 57.144391 loss_d: -49.095902 time: 10.3 s\n",
      "[33 150] loss_ae: 17.438085 loss_g: 65.095002 loss_d: -52.025290 time: 10.5 s\n",
      "[33 200] loss_ae: 14.973750 loss_g: 61.671786 loss_d: -49.643110 time: 10.4 s\n",
      "[33 250] loss_ae: 14.259963 loss_g: 61.164263 loss_d: -51.121805 time: 10.6 s\n",
      "[33 300] loss_ae: 14.093850 loss_g: 58.772030 loss_d: -50.939692 time: 10.6 s\n",
      "[33 350] loss_ae: 14.555262 loss_g: 58.549418 loss_d: -50.716444 time: 10.3 s\n",
      "[33 400] loss_ae: 13.789466 loss_g: 65.281822 loss_d: -57.247269 time: 10.4 s\n",
      "[33 450] loss_ae: 13.405034 loss_g: 58.386706 loss_d: -50.352901 time: 10.6 s\n",
      "[33 500] loss_ae: 13.638375 loss_g: 57.809190 loss_d: -50.439020 time: 10.6 s\n",
      "[33 550] loss_ae: 14.029018 loss_g: 54.037604 loss_d: -43.396455 time: 10.6 s\n",
      "[33 600] loss_ae: 14.999777 loss_g: 63.479599 loss_d: -53.019400 time: 10.4 s\n",
      "[33 650] loss_ae: 16.249062 loss_g: 60.284534 loss_d: -50.624787 time: 10.7 s\n",
      "[33 700] loss_ae: 14.953219 loss_g: 54.111071 loss_d: -49.727769 time: 10.6 s\n",
      "[33 750] loss_ae: 14.249932 loss_g: 58.894386 loss_d: -49.899197 time: 10.6 s\n",
      "[33 800] loss_ae: 15.747970 loss_g: 59.170198 loss_d: -47.701717 time: 10.2 s\n",
      "[33 850] loss_ae: 13.371923 loss_g: 57.153873 loss_d: -47.350025 time: 10.3 s\n",
      "[33 900] loss_ae: 13.443972 loss_g: 62.166908 loss_d: -53.996270 time: 10.1 s\n",
      "[33 950] loss_ae: 12.654873 loss_g: 60.183312 loss_d: -48.200152 time: 10.1 s\n",
      "[33 1000] loss_ae: 15.223902 loss_g: 53.915312 loss_d: -45.173577 time: 10.1 s\n",
      "[33 1050] loss_ae: 14.829978 loss_g: 61.523307 loss_d: -51.204854 time: 10.2 s\n",
      "[33 1100] loss_ae: 15.170683 loss_g: 61.731140 loss_d: -53.449469 time: 9.9 s\n",
      "[33 1150] loss_ae: 14.798955 loss_g: 59.353202 loss_d: -47.265423 time: 10.0 s\n",
      "[33 1200] loss_ae: 15.448756 loss_g: 61.428477 loss_d: -49.624226 time: 10.4 s\n",
      "[33 1250] loss_ae: 14.623001 loss_g: 57.680343 loss_d: -48.248788 time: 9.9 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and we have a lot of <unk> and i ' m not sure that we ' re not going to do that but i think it ' s </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  do you </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  i don ' t know i ' m a little bit older than i am i am i am i am i am in the morning </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  they have to do that and they ' re not going to do that they ' re not going to do that but they do have a </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  or <unk> or whatever they have to do with the <unk> and the <unk> and </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  but you know </s>\n",
      "true response:  oh </s>\n",
      "generate response:  it ' s really not really it ' s not really not really it ' s not really not really not as bad as it is </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  really </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  right </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  they all have a good time and they ' re not they ' re not they ' re not </s>\n",
      "true response:  but they </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah but you can </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  just you know you have to have a you know a you know a <unk> of a <unk> you know you have to have a </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  right </s>\n",
      "BLEU1 0.442798, BLEU2 0.358403, BLEU3 0.302348, BLEU4 0.242919, inter_dist1 0.006514, inter_dist2 0.040359 avg_len 16.665390\n",
      " time: 208.3 s\n",
      "Done testing\n",
      "Epoch:  34\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[34 50] loss_ae: 13.624158 loss_g: 64.574583 loss_d: -55.819621 time: 10.6 s\n",
      "[34 100] loss_ae: 15.182056 loss_g: 56.147725 loss_d: -48.450698 time: 10.7 s\n",
      "[34 150] loss_ae: 15.589058 loss_g: 59.234583 loss_d: -46.806630 time: 10.3 s\n",
      "[34 200] loss_ae: 12.993088 loss_g: 70.750883 loss_d: -59.627047 time: 10.7 s\n",
      "[34 250] loss_ae: 14.516161 loss_g: 73.219635 loss_d: -63.496012 time: 10.4 s\n",
      "[34 300] loss_ae: 14.678893 loss_g: 62.975817 loss_d: -51.493569 time: 10.5 s\n",
      "[34 350] loss_ae: 15.007605 loss_g: 50.514095 loss_d: -43.880506 time: 10.4 s\n",
      "[34 400] loss_ae: 12.625134 loss_g: 49.963089 loss_d: -46.312418 time: 10.3 s\n",
      "[34 450] loss_ae: 15.162203 loss_g: 56.412403 loss_d: -48.909518 time: 10.5 s\n",
      "[34 500] loss_ae: 13.811594 loss_g: 58.203180 loss_d: -51.253814 time: 10.6 s\n",
      "[34 550] loss_ae: 14.631368 loss_g: 53.625885 loss_d: -45.313250 time: 10.3 s\n",
      "[34 600] loss_ae: 14.226205 loss_g: 64.141703 loss_d: -49.059405 time: 10.5 s\n",
      "[34 650] loss_ae: 14.345422 loss_g: 58.471305 loss_d: -44.671795 time: 10.5 s\n",
      "[34 700] loss_ae: 15.627868 loss_g: 57.875207 loss_d: -49.713146 time: 10.2 s\n",
      "[34 750] loss_ae: 13.912054 loss_g: 56.989787 loss_d: -49.969012 time: 10.5 s\n",
      "[34 800] loss_ae: 13.842744 loss_g: 60.742094 loss_d: -48.686892 time: 10.6 s\n",
      "[34 850] loss_ae: 14.961431 loss_g: 53.155592 loss_d: -47.702503 time: 10.5 s\n",
      "[34 900] loss_ae: 16.579072 loss_g: 63.394546 loss_d: -55.677095 time: 10.6 s\n",
      "[34 950] loss_ae: 16.247039 loss_g: 61.686427 loss_d: -53.801149 time: 10.5 s\n",
      "[34 1000] loss_ae: 13.498026 loss_g: 57.694096 loss_d: -48.116520 time: 10.5 s\n",
      "[34 1050] loss_ae: 16.560546 loss_g: 52.851649 loss_d: -47.085463 time: 10.3 s\n",
      "[34 1100] loss_ae: 13.785149 loss_g: 61.565416 loss_d: -51.610806 time: 10.4 s\n",
      "[34 1150] loss_ae: 14.919711 loss_g: 55.786502 loss_d: -45.250605 time: 10.4 s\n",
      "[34 1200] loss_ae: 16.172223 loss_g: 71.351426 loss_d: -60.706466 time: 10.6 s\n",
      "[34 1250] loss_ae: 14.138329 loss_g: 56.024134 loss_d: -49.896564 time: 10.3 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and they also have the same thing as a matter of fact i think that ' s </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  to do that but </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and i don ' t know if you know if you ' re not going to have to do it you know i mean i think it ' </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  and you don ' t like it </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  have you tried that if you ' ve got a dog that ' s a good one that ' s a good one that ' s a </s>\n",
      "true response:  oh </s>\n",
      "generate response:  uh - huh they were going to have a big <unk> of their <unk> and </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and i can ' t remember the last time i ' ve been to a couple of years ago and i ' ve been in the </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and and then </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  because we have a lot of problems with the families and the kids are very <unk> and they ' re not going to be able to </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  but they </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  but there is a lot of that </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and we have a we have a we have a we have a we have a <unk> we have a <unk> we have a <unk> </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  i have i have a sister that lives in a nursing home and i have a sister that lives in a nursing home </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  trying to do something to the wrong thing that i think that i think that the people that i think that ' s a good idea </s>\n",
      "BLEU1 0.459248, BLEU2 0.372249, BLEU3 0.313621, BLEU4 0.251779, inter_dist1 0.006422, inter_dist2 0.039243 avg_len 16.335705\n",
      " time: 182.3 s\n",
      "Done testing\n",
      "Epoch:  35\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[35 50] loss_ae: 13.802285 loss_g: 59.387540 loss_d: -47.566094 time: 10.5 s\n",
      "[35 100] loss_ae: 15.711012 loss_g: 60.982875 loss_d: -50.280220 time: 10.5 s\n",
      "[35 150] loss_ae: 14.035106 loss_g: 61.234221 loss_d: -51.856946 time: 10.4 s\n",
      "[35 200] loss_ae: 14.631014 loss_g: 60.097930 loss_d: -52.601386 time: 10.6 s\n",
      "[35 250] loss_ae: 13.490142 loss_g: 63.984786 loss_d: -55.827734 time: 10.6 s\n",
      "[35 300] loss_ae: 14.066794 loss_g: 63.292623 loss_d: -54.793037 time: 10.5 s\n",
      "[35 350] loss_ae: 15.950609 loss_g: 59.075507 loss_d: -50.218383 time: 10.4 s\n",
      "[35 400] loss_ae: 14.687319 loss_g: 58.426084 loss_d: -49.577080 time: 10.6 s\n",
      "[35 450] loss_ae: 14.757051 loss_g: 59.827578 loss_d: -50.740331 time: 10.7 s\n",
      "[35 500] loss_ae: 13.668979 loss_g: 67.191146 loss_d: -55.978041 time: 10.6 s\n",
      "[35 550] loss_ae: 14.815109 loss_g: 64.891474 loss_d: -54.492312 time: 10.5 s\n",
      "[35 600] loss_ae: 14.145737 loss_g: 63.524343 loss_d: -52.967321 time: 10.4 s\n",
      "[35 650] loss_ae: 14.371288 loss_g: 67.646417 loss_d: -61.182972 time: 10.5 s\n",
      "[35 700] loss_ae: 14.937447 loss_g: 60.057849 loss_d: -46.956953 time: 10.5 s\n",
      "[35 750] loss_ae: 12.151039 loss_g: 63.747297 loss_d: -51.769306 time: 10.4 s\n",
      "[35 800] loss_ae: 13.985611 loss_g: 68.672866 loss_d: -55.306313 time: 10.4 s\n",
      "[35 850] loss_ae: 14.481069 loss_g: 65.012811 loss_d: -51.367743 time: 10.6 s\n",
      "[35 900] loss_ae: 13.689011 loss_g: 65.470541 loss_d: -51.073529 time: 10.6 s\n",
      "[35 950] loss_ae: 13.857980 loss_g: 63.317571 loss_d: -51.437280 time: 10.0 s\n",
      "[35 1000] loss_ae: 15.347628 loss_g: 67.717263 loss_d: -60.405430 time: 10.0 s\n",
      "[35 1050] loss_ae: 15.734907 loss_g: 59.582288 loss_d: -44.046432 time: 10.4 s\n",
      "[35 1100] loss_ae: 14.389164 loss_g: 63.380554 loss_d: -53.386565 time: 10.0 s\n",
      "[35 1150] loss_ae: 14.797801 loss_g: 64.321628 loss_d: -53.971639 time: 10.1 s\n",
      "[35 1200] loss_ae: 14.337499 loss_g: 58.040348 loss_d: -45.379526 time: 10.4 s\n",
      "[35 1250] loss_ae: 14.394167 loss_g: 58.991184 loss_d: -52.382062 time: 10.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  oh really </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  have you had a <unk> or a <unk> or a <unk> or a <unk> or a <unk> or a <unk> or something </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  right </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  they just have to have to have a <unk> they have to have a <unk> they have a they have a they have a <unk> they have a they have\n",
      "true response:  something like that </s>\n",
      "generate response:  and you know he ' s got to have a <unk> and he ' s got a <unk> </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  have you ever had a <unk> of a <unk> of a <unk> </s>\n",
      "true response:  oh </s>\n",
      "generate response:  uh - huh we have a lot of fun with the kids and we </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  right </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  you know they were they were they were they were they were they were </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  they ' re they ' re they ' re they ' re a <unk> they ' ve got a they have a they have a <unk> </s>\n",
      "true response:  but they </s>\n",
      "generate response:  and </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  uh - huh well they ' re they ' re they ' re they ' re they ' re they ' re they ' re they ' re they '\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and i was a i was a i was a i was a i was a i was a i was a i was a i was a </s>\n",
      "true response:  and </s>\n",
      "generate response:  <unk> </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  and a lot of the people are just kind of a <unk> of a <unk> and they ' re not they ' re not very good </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  uh - huh they ' re not going to be in trouble </s>\n",
      "BLEU1 0.442836, BLEU2 0.357143, BLEU3 0.299889, BLEU4 0.240349, inter_dist1 0.007062, inter_dist2 0.042882 avg_len 16.120781\n",
      " time: 210.2 s\n",
      "Done testing\n",
      "Epoch:  36\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[36 50] loss_ae: 13.173523 loss_g: 62.934056 loss_d: -51.853903 time: 8.3 s\n",
      "[36 100] loss_ae: 14.533696 loss_g: 70.067937 loss_d: -52.086706 time: 8.2 s\n",
      "[36 150] loss_ae: 13.270286 loss_g: 66.942896 loss_d: -57.464809 time: 8.1 s\n",
      "[36 200] loss_ae: 13.978663 loss_g: 62.405457 loss_d: -50.957000 time: 8.3 s\n",
      "[36 250] loss_ae: 14.448524 loss_g: 63.435245 loss_d: -54.344063 time: 8.1 s\n",
      "[36 300] loss_ae: 16.585124 loss_g: 66.800112 loss_d: -56.524093 time: 8.0 s\n",
      "[36 350] loss_ae: 13.896269 loss_g: 54.884215 loss_d: -47.467299 time: 8.4 s\n",
      "[36 400] loss_ae: 13.096860 loss_g: 67.353249 loss_d: -56.499406 time: 8.1 s\n",
      "[36 450] loss_ae: 14.180006 loss_g: 61.283636 loss_d: -53.878961 time: 8.1 s\n",
      "[36 500] loss_ae: 14.844500 loss_g: 62.925824 loss_d: -56.273559 time: 8.1 s\n",
      "[36 550] loss_ae: 14.455936 loss_g: 57.706785 loss_d: -51.216354 time: 8.1 s\n",
      "[36 600] loss_ae: 14.387764 loss_g: 60.445210 loss_d: -50.631157 time: 8.2 s\n",
      "[36 650] loss_ae: 14.170934 loss_g: 56.115371 loss_d: -48.300028 time: 8.2 s\n",
      "[36 700] loss_ae: 14.383908 loss_g: 66.279656 loss_d: -55.291737 time: 8.1 s\n",
      "[36 750] loss_ae: 13.623803 loss_g: 60.378199 loss_d: -53.092623 time: 8.3 s\n",
      "[36 800] loss_ae: 15.410058 loss_g: 59.401037 loss_d: -48.491426 time: 8.1 s\n",
      "[36 850] loss_ae: 14.599622 loss_g: 61.576884 loss_d: -53.945160 time: 8.2 s\n",
      "[36 900] loss_ae: 14.723276 loss_g: 65.763357 loss_d: -56.460926 time: 8.0 s\n",
      "[36 950] loss_ae: 16.083617 loss_g: 73.405115 loss_d: -54.724227 time: 8.1 s\n",
      "[36 1000] loss_ae: 14.729452 loss_g: 65.407024 loss_d: -53.346624 time: 8.2 s\n",
      "[36 1050] loss_ae: 15.388304 loss_g: 63.009027 loss_d: -53.727380 time: 8.1 s\n",
      "[36 1100] loss_ae: 14.264574 loss_g: 62.053370 loss_d: -51.075789 time: 8.2 s\n",
      "[36 1150] loss_ae: 14.444066 loss_g: 62.376826 loss_d: -51.451666 time: 8.2 s\n",
      "[36 1200] loss_ae: 15.387105 loss_g: 50.271789 loss_d: -44.803841 time: 8.1 s\n",
      "[36 1250] loss_ae: 15.940624 loss_g: 62.716243 loss_d: -51.121973 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  you know he ' s a he ' s a <unk> and he ' s a </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  have you been in any lately </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  yeah </s>\n",
      "generate response:  and they </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  you can you can get a <unk> <unk> you know you can you can you can you can you can </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  has been a <unk> of <unk> and they have a lot of <unk> and they have a lot of <unk> and </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  oh </s>\n",
      "generate response:  you know and we have a we have a we have a we have a we have a <unk> a <unk> a <unk> <unk> </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  it ' ll do that but you know you can ' t really get it on a little bit of a <unk> or something like that </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  they </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  they do have a lot of problems with </s>\n",
      "true response:  but they </s>\n",
      "generate response:  but </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  i </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and i don ' t know i just i don ' t know i just i just </s>\n",
      "true response:  and </s>\n",
      "generate response:  do </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  well i know in dallas they ' ve got a lot of good weather and the weather and the weather ' s been nice talking to you </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  hum </s>\n",
      "BLEU1 0.450023, BLEU2 0.365403, BLEU3 0.308766, BLEU4 0.248377, inter_dist1 0.006504, inter_dist2 0.039964 avg_len 16.831600\n",
      " time: 144.9 s\n",
      "Done testing\n",
      "Epoch:  37\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[37 50] loss_ae: 14.506789 loss_g: 61.545470 loss_d: -49.268615 time: 8.1 s\n",
      "[37 100] loss_ae: 14.647840 loss_g: 58.434679 loss_d: -47.403072 time: 8.4 s\n",
      "[37 150] loss_ae: 15.756565 loss_g: 61.619546 loss_d: -48.741101 time: 8.1 s\n",
      "[37 200] loss_ae: 14.031495 loss_g: 67.063956 loss_d: -58.440207 time: 8.1 s\n",
      "[37 250] loss_ae: 13.451679 loss_g: 62.278943 loss_d: -52.732842 time: 8.2 s\n",
      "[37 300] loss_ae: 15.150987 loss_g: 62.200756 loss_d: -52.733225 time: 8.1 s\n",
      "[37 350] loss_ae: 14.526245 loss_g: 60.927267 loss_d: -47.288843 time: 8.3 s\n",
      "[37 400] loss_ae: 14.217896 loss_g: 62.236913 loss_d: -53.318306 time: 8.1 s\n",
      "[37 450] loss_ae: 13.873695 loss_g: 71.082089 loss_d: -58.332981 time: 8.2 s\n",
      "[37 500] loss_ae: 14.604969 loss_g: 67.270572 loss_d: -54.419274 time: 8.3 s\n",
      "[37 550] loss_ae: 13.635424 loss_g: 72.011726 loss_d: -57.272051 time: 8.1 s\n",
      "[37 600] loss_ae: 14.713364 loss_g: 72.270291 loss_d: -59.332232 time: 8.2 s\n",
      "[37 650] loss_ae: 14.411809 loss_g: 74.197192 loss_d: -60.155829 time: 8.1 s\n",
      "[37 700] loss_ae: 13.669918 loss_g: 65.629121 loss_d: -52.690312 time: 8.2 s\n",
      "[37 750] loss_ae: 14.963971 loss_g: 67.012718 loss_d: -54.959186 time: 8.1 s\n",
      "[37 800] loss_ae: 14.513451 loss_g: 69.636587 loss_d: -61.802808 time: 8.1 s\n",
      "[37 850] loss_ae: 14.285823 loss_g: 66.921022 loss_d: -57.121627 time: 8.2 s\n",
      "[37 900] loss_ae: 14.061074 loss_g: 72.410469 loss_d: -60.118730 time: 8.1 s\n",
      "[37 950] loss_ae: 13.675788 loss_g: 69.140875 loss_d: -60.262105 time: 8.3 s\n",
      "[37 1000] loss_ae: 13.784531 loss_g: 72.168250 loss_d: -61.013453 time: 8.2 s\n",
      "[37 1050] loss_ae: 13.732308 loss_g: 68.475912 loss_d: -53.565579 time: 8.2 s\n",
      "[37 1100] loss_ae: 16.693562 loss_g: 66.370869 loss_d: -52.073201 time: 8.2 s\n",
      "[37 1150] loss_ae: 14.248631 loss_g: 63.107122 loss_d: -52.527022 time: 8.1 s\n",
      "[37 1200] loss_ae: 15.077645 loss_g: 63.762326 loss_d: -47.518068 time: 8.3 s\n",
      "[37 1250] loss_ae: 14.347020 loss_g: 68.770788 loss_d: -56.052354 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and you know </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  and i like to like the <unk> and things like that i like to do that </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  and </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and that that ' s that that ' s the that it that it that it that it that it that it that it that it that it that it\n",
      "true response:  something like that </s>\n",
      "generate response:  huh well they have a they have a they have a <unk> <unk> in a <unk> </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  oh i ' m just thinking of getting out of it i ' m not sure what it is but i think it ' s </s>\n",
      "true response:  oh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and i don ' t know if you ' ve been there a long time and you ' ve got to be a lot of fun going to </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  oh it ' s just not just a <unk> and it ' s not </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  we had to have to have a <unk> and we had a <unk> a <unk> a <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  because we have to have to have to have a problem with that is it </s>\n",
      "true response:  but they </s>\n",
      "generate response:  and we have a lot of people that are in the in the in the in the in the in the state of maryland and the state of </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  i </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  they just have to have to have a <unk> </s>\n",
      "true response:  and </s>\n",
      "generate response:  uh - huh well i have a friend of mine who ' s a nurse and she ' s a senior and she ' s </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  i guess we have to have a little bit of a <unk> here </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  uh - huh </s>\n",
      "BLEU1 0.448599, BLEU2 0.362921, BLEU3 0.305191, BLEU4 0.244691, inter_dist1 0.006832, inter_dist2 0.041069 avg_len 16.024083\n",
      " time: 144.8 s\n",
      "Done testing\n",
      "Epoch:  38\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[38 50] loss_ae: 14.765924 loss_g: 68.900445 loss_d: -55.552485 time: 8.1 s\n",
      "[38 100] loss_ae: 12.939365 loss_g: 69.916760 loss_d: -54.977740 time: 8.1 s\n",
      "[38 150] loss_ae: 13.421179 loss_g: 68.797193 loss_d: -55.719329 time: 8.1 s\n",
      "[38 200] loss_ae: 13.256308 loss_g: 64.643254 loss_d: -53.551102 time: 8.2 s\n",
      "[38 250] loss_ae: 15.822127 loss_g: 80.053764 loss_d: -63.464712 time: 8.2 s\n",
      "[38 300] loss_ae: 14.585675 loss_g: 73.900881 loss_d: -61.299965 time: 8.2 s\n",
      "[38 350] loss_ae: 12.674153 loss_g: 65.257389 loss_d: -54.424976 time: 8.2 s\n",
      "[38 400] loss_ae: 14.925858 loss_g: 65.783341 loss_d: -55.438685 time: 8.1 s\n",
      "[38 450] loss_ae: 15.304001 loss_g: 67.096759 loss_d: -54.333136 time: 8.2 s\n",
      "[38 500] loss_ae: 14.995907 loss_g: 65.670845 loss_d: -55.890380 time: 8.3 s\n",
      "[38 550] loss_ae: 15.286325 loss_g: 69.960719 loss_d: -61.379451 time: 8.1 s\n",
      "[38 600] loss_ae: 13.918726 loss_g: 73.636460 loss_d: -57.357419 time: 8.3 s\n",
      "[38 650] loss_ae: 13.791510 loss_g: 78.461896 loss_d: -65.185569 time: 8.1 s\n",
      "[38 700] loss_ae: 12.347515 loss_g: 68.864683 loss_d: -54.269228 time: 8.3 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38 750] loss_ae: 12.531853 loss_g: 66.618644 loss_d: -56.023406 time: 8.1 s\n",
      "[38 800] loss_ae: 14.257061 loss_g: 67.287107 loss_d: -52.885913 time: 8.2 s\n",
      "[38 850] loss_ae: 14.524394 loss_g: 70.956589 loss_d: -61.854369 time: 8.2 s\n",
      "[38 900] loss_ae: 14.110746 loss_g: 67.181309 loss_d: -52.647604 time: 8.1 s\n",
      "[38 950] loss_ae: 13.267765 loss_g: 64.166645 loss_d: -53.292514 time: 8.3 s\n",
      "[38 1000] loss_ae: 14.224413 loss_g: 64.629637 loss_d: -52.875391 time: 8.1 s\n",
      "[38 1050] loss_ae: 13.973133 loss_g: 71.807679 loss_d: -57.223994 time: 8.1 s\n",
      "[38 1100] loss_ae: 12.566089 loss_g: 63.054411 loss_d: -50.376710 time: 8.2 s\n",
      "[38 1150] loss_ae: 14.395494 loss_g: 72.347236 loss_d: -58.313389 time: 8.2 s\n",
      "[38 1200] loss_ae: 15.890694 loss_g: 75.810319 loss_d: -59.664959 time: 8.2 s\n",
      "[38 1250] loss_ae: 15.534064 loss_g: 72.259208 loss_d: -54.209964 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  to have to do a lot of work for the kids and they have a lot of kids and they ' re not even </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  you know like you say it ' s a lot of work and i think that ' s a good idea i think it ' s </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  and then it ' s the one that i think that i think that it ' s a good idea but i think it ' s </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and but in other words if you have a gun if you ' re not going to do it you don ' t have to worry about </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  they have to have to have a they have a they have a they have a <unk> they have a <unk> </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  you know if you if you don ' t have that you don ' t have to worry about that you don ' t have to worry about </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  and but </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh huh </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  especially with the children that are not the ones that are the ones that are the ones that are the ones that are the ones that are </s>\n",
      "true response:  but they </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  they don ' t have to have a lot of <unk> too much to do that </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  they just have to be a little more <unk> than they are in there </s>\n",
      "true response:  and </s>\n",
      "generate response:  we don ' t have we don ' t have a we have a we have a we have a we have a <unk> we have a </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  uh - huh </s>\n",
      "BLEU1 0.439308, BLEU2 0.357724, BLEU3 0.302458, BLEU4 0.243322, inter_dist1 0.006858, inter_dist2 0.041586 avg_len 17.027185\n",
      " time: 144.1 s\n",
      "Done testing\n",
      "Epoch:  39\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[39 50] loss_ae: 12.640880 loss_g: 68.603560 loss_d: -61.765822 time: 8.1 s\n",
      "[39 100] loss_ae: 15.385853 loss_g: 73.262021 loss_d: -63.600437 time: 8.2 s\n",
      "[39 150] loss_ae: 14.211204 loss_g: 61.777217 loss_d: -50.515261 time: 8.2 s\n",
      "[39 200] loss_ae: 13.781478 loss_g: 70.888542 loss_d: -58.910034 time: 8.2 s\n",
      "[39 250] loss_ae: 13.038450 loss_g: 74.717709 loss_d: -59.204375 time: 8.1 s\n",
      "[39 300] loss_ae: 12.370355 loss_g: 75.549663 loss_d: -60.727476 time: 8.3 s\n",
      "[39 350] loss_ae: 12.907575 loss_g: 60.209181 loss_d: -54.417180 time: 8.2 s\n",
      "[39 400] loss_ae: 13.811272 loss_g: 72.659619 loss_d: -63.981488 time: 8.2 s\n",
      "[39 450] loss_ae: 14.874879 loss_g: 75.330078 loss_d: -64.114190 time: 8.1 s\n",
      "[39 500] loss_ae: 15.514787 loss_g: 71.258872 loss_d: -56.939459 time: 8.2 s\n",
      "[39 550] loss_ae: 13.395110 loss_g: 71.304053 loss_d: -58.231738 time: 8.2 s\n",
      "[39 600] loss_ae: 15.500411 loss_g: 76.364318 loss_d: -62.028320 time: 8.1 s\n",
      "[39 650] loss_ae: 13.147180 loss_g: 76.620556 loss_d: -60.240455 time: 8.1 s\n",
      "[39 700] loss_ae: 15.699231 loss_g: 80.189982 loss_d: -66.339629 time: 8.1 s\n",
      "[39 750] loss_ae: 13.952991 loss_g: 65.191786 loss_d: -55.519941 time: 8.2 s\n",
      "[39 800] loss_ae: 14.859485 loss_g: 77.602994 loss_d: -64.635616 time: 8.1 s\n",
      "[39 850] loss_ae: 13.802229 loss_g: 79.402094 loss_d: -63.741601 time: 8.3 s\n",
      "[39 900] loss_ae: 13.982577 loss_g: 63.069031 loss_d: -51.023508 time: 8.2 s\n",
      "[39 950] loss_ae: 14.039344 loss_g: 73.981572 loss_d: -59.134415 time: 8.2 s\n",
      "[39 1000] loss_ae: 13.875704 loss_g: 72.802712 loss_d: -59.785878 time: 8.1 s\n",
      "[39 1050] loss_ae: 14.828234 loss_g: 70.965624 loss_d: -60.315282 time: 8.3 s\n",
      "[39 1100] loss_ae: 14.399325 loss_g: 78.964566 loss_d: -64.503982 time: 8.2 s\n",
      "[39 1150] loss_ae: 15.704160 loss_g: 75.562215 loss_d: -56.611508 time: 8.0 s\n",
      "[39 1200] loss_ae: 14.993209 loss_g: 78.055725 loss_d: -61.766805 time: 8.2 s\n",
      "[39 1250] loss_ae: 14.946460 loss_g: 81.231411 loss_d: -65.769386 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  you know they have to have to go to a nursing home and they have to go to a nursing home and they have a </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  and that ' s it that ' s the that it ' s that it that it that it that it that it that it that it </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  well yeah and that was good talking to you </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  you know they just say well the thing is that they can do that they can do </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  i really don ' t know i don ' t know i don ' t know if you ' ve heard of it or not </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and you know just like you have to have a you have a you have a you have a you have a you have a you have a </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  oh yeah it really does and it ' s really </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  there is a lot of people that are in the in the in the in the in the in the in the state and the state of florida </s>\n",
      "true response:  but they </s>\n",
      "generate response:  sure </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know you just have to have a you have a hard time to do that you know you ' re going to have to </s>\n",
      "true response:  and </s>\n",
      "generate response:  do you get any kind of weather that you can do that you can get out of the <unk> and </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  yeah </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.442836, BLEU2 0.358711, BLEU3 0.302301, BLEU4 0.242885, inter_dist1 0.006852, inter_dist2 0.041511 avg_len 15.416895\n",
      " time: 144.1 s\n",
      "Done testing\n",
      "Epoch:  40\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[40 50] loss_ae: 14.115519 loss_g: 73.345139 loss_d: -62.360778 time: 8.2 s\n",
      "[40 100] loss_ae: 14.183557 loss_g: 81.769256 loss_d: -68.878419 time: 8.1 s\n",
      "[40 150] loss_ae: 14.893810 loss_g: 72.059781 loss_d: -55.476762 time: 8.1 s\n",
      "[40 200] loss_ae: 13.619443 loss_g: 64.338232 loss_d: -50.754033 time: 8.1 s\n",
      "[40 250] loss_ae: 15.063963 loss_g: 75.102769 loss_d: -57.882945 time: 8.1 s\n",
      "[40 300] loss_ae: 15.663721 loss_g: 73.852754 loss_d: -61.669175 time: 8.2 s\n",
      "[40 350] loss_ae: 13.473955 loss_g: 73.479257 loss_d: -62.510004 time: 8.0 s\n",
      "[40 400] loss_ae: 13.610918 loss_g: 70.765913 loss_d: -62.472615 time: 8.3 s\n",
      "[40 450] loss_ae: 13.212073 loss_g: 75.257348 loss_d: -57.889950 time: 8.1 s\n",
      "[40 500] loss_ae: 14.747538 loss_g: 67.467923 loss_d: -56.319969 time: 8.2 s\n",
      "[40 550] loss_ae: 13.570597 loss_g: 78.242620 loss_d: -66.285481 time: 8.3 s\n",
      "[40 600] loss_ae: 16.452559 loss_g: 78.879149 loss_d: -64.672646 time: 8.4 s\n",
      "[40 650] loss_ae: 13.977226 loss_g: 71.992024 loss_d: -60.392171 time: 8.1 s\n",
      "[40 700] loss_ae: 13.183453 loss_g: 80.686156 loss_d: -70.112238 time: 8.3 s\n",
      "[40 750] loss_ae: 15.439157 loss_g: 76.435905 loss_d: -59.752148 time: 8.1 s\n",
      "[40 800] loss_ae: 13.502826 loss_g: 71.422286 loss_d: -61.129137 time: 8.1 s\n",
      "[40 850] loss_ae: 13.860176 loss_g: 75.372636 loss_d: -61.322767 time: 8.3 s\n",
      "[40 900] loss_ae: 14.547888 loss_g: 67.225879 loss_d: -50.582156 time: 8.1 s\n",
      "[40 950] loss_ae: 13.227848 loss_g: 72.207214 loss_d: -63.433943 time: 8.2 s\n",
      "[40 1000] loss_ae: 14.185929 loss_g: 77.429900 loss_d: -66.092870 time: 8.1 s\n",
      "[40 1050] loss_ae: 15.597327 loss_g: 83.315343 loss_d: -67.352909 time: 8.2 s\n",
      "[40 1100] loss_ae: 13.801278 loss_g: 76.511981 loss_d: -65.229382 time: 8.2 s\n",
      "[40 1150] loss_ae: 14.421091 loss_g: 70.717135 loss_d: -57.974853 time: 8.1 s\n",
      "[40 1200] loss_ae: 12.455058 loss_g: 82.234757 loss_d: -62.888350 time: 8.2 s\n",
      "[40 1250] loss_ae: 13.380363 loss_g: 77.571699 loss_d: -66.754977 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  you know and he he ' s just he just he just he just <unk> a little bit and he ' s got a lot of </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  but well i think that the people that i think are really important to do that </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  you know what ' s the what ' s the difference between the <unk> and the <unk> and </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and i don ' t know i just don ' t have a lot of <unk> in a lot of cases i don ' t </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  i have i have a friend that i ' m in a in a in a in a in a in a in a in a </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  oh yeah </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  about two years ago and i was in the air force and i was in the air force and i was in the air force and i </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  and oh they ' re really good for them </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  hum </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  like huh that ' s interesting i think that ' s the way it is i think it ' s going to be a lot more to do </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  but </s>\n",
      "true response:  but they </s>\n",
      "generate response:  but </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah that ' s true they ' re they ' re they ' re they ' re they ' re they ' re real good they ' re </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  but i know that it it ' s it it ' s it it ' s it it ' s it it ' s it it ' s it </s>\n",
      "true response:  and </s>\n",
      "generate response:  you know you can ' t have a you have a you have a you have a you have a you have a you have a </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.448788, BLEU2 0.365139, BLEU3 0.308534, BLEU4 0.248193, inter_dist1 0.007149, inter_dist2 0.043612 avg_len 16.409962\n",
      " time: 144.4 s\n",
      "Done testing\n",
      "Epoch:  41\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[41 50] loss_ae: 14.674407 loss_g: 83.429755 loss_d: -72.034976 time: 8.3 s\n",
      "[41 100] loss_ae: 13.057239 loss_g: 81.604928 loss_d: -71.220911 time: 8.1 s\n",
      "[41 150] loss_ae: 16.167911 loss_g: 75.587653 loss_d: -63.723338 time: 8.3 s\n",
      "[41 200] loss_ae: 13.749142 loss_g: 84.655514 loss_d: -64.329833 time: 8.1 s\n",
      "[41 250] loss_ae: 13.697658 loss_g: 73.141280 loss_d: -58.729107 time: 8.2 s\n",
      "[41 300] loss_ae: 12.018967 loss_g: 70.002479 loss_d: -54.817110 time: 8.2 s\n",
      "[41 350] loss_ae: 13.017978 loss_g: 79.190168 loss_d: -65.253272 time: 8.1 s\n",
      "[41 400] loss_ae: 13.771554 loss_g: 68.772413 loss_d: -55.337565 time: 8.2 s\n",
      "[41 450] loss_ae: 13.421592 loss_g: 65.390086 loss_d: -54.815804 time: 8.2 s\n",
      "[41 500] loss_ae: 12.842236 loss_g: 70.172282 loss_d: -55.125581 time: 8.1 s\n",
      "[41 550] loss_ae: 13.947772 loss_g: 72.961102 loss_d: -62.925540 time: 8.2 s\n",
      "[41 600] loss_ae: 14.787108 loss_g: 63.068433 loss_d: -54.845285 time: 8.0 s\n",
      "[41 650] loss_ae: 13.895555 loss_g: 76.289948 loss_d: -62.230010 time: 8.1 s\n",
      "[41 700] loss_ae: 13.588235 loss_g: 69.611278 loss_d: -58.255304 time: 8.3 s\n",
      "[41 750] loss_ae: 16.135734 loss_g: 81.878777 loss_d: -72.171467 time: 8.1 s\n",
      "[41 800] loss_ae: 14.053380 loss_g: 70.242284 loss_d: -59.452606 time: 8.2 s\n",
      "[41 850] loss_ae: 14.687985 loss_g: 85.466616 loss_d: -68.131390 time: 8.1 s\n",
      "[41 900] loss_ae: 15.105182 loss_g: 74.170139 loss_d: -59.398115 time: 8.0 s\n",
      "[41 950] loss_ae: 15.191863 loss_g: 73.267971 loss_d: -57.433798 time: 8.3 s\n",
      "[41 1000] loss_ae: 14.630881 loss_g: 82.311145 loss_d: -66.813017 time: 8.1 s\n",
      "[41 1050] loss_ae: 14.006813 loss_g: 71.191645 loss_d: -54.839990 time: 8.1 s\n",
      "[41 1100] loss_ae: 13.291066 loss_g: 81.353871 loss_d: -67.389743 time: 8.3 s\n",
      "[41 1150] loss_ae: 14.292036 loss_g: 75.343037 loss_d: -63.620268 time: 8.0 s\n",
      "[41 1200] loss_ae: 14.374442 loss_g: 85.050669 loss_d: -75.106656 time: 8.1 s\n",
      "[41 1250] loss_ae: 15.106213 loss_g: 79.742644 loss_d: -67.229907 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and they have to have to go to a nursing home and they do the same thing and they ' re not they ' re not </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  do you think that the company has a good idea of having a problem with the with the with the with the with the </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  sure </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  i was just the only thing that i ' ve ever seen a lot of people in the united states and </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  and i just don ' t think i would like to see </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  oh </s>\n",
      "generate response:  and yeah </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and they have to have to have a big <unk> and they have a lot of <unk> and they ' ve got a lot of <unk> and they ' </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  uh - huh </s>\n",
      "generate response:  right </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  you know they have to have a <unk> they have a they have a <unk> you know they have a they have a <unk> you have to have a </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  but but </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  but they </s>\n",
      "generate response:  and i ' ve i ' ve had to have a <unk> <unk> <unk> </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  um - hum i do </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  they have to have to have to have a you have to have a have a you have to have a have a you have to have a have a\n",
      "true response:  and </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  have you been to a big place in texas and that ' s where </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.439174, BLEU2 0.355557, BLEU3 0.298795, BLEU4 0.239270, inter_dist1 0.007604, inter_dist2 0.046409 avg_len 15.715928\n",
      " time: 144.5 s\n",
      "Done testing\n",
      "Epoch:  42\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[42 50] loss_ae: 13.685791 loss_g: 83.547332 loss_d: -72.402510 time: 8.2 s\n",
      "[42 100] loss_ae: 13.548506 loss_g: 82.850919 loss_d: -68.846882 time: 8.1 s\n",
      "[42 150] loss_ae: 13.321863 loss_g: 99.125884 loss_d: -84.029452 time: 8.0 s\n",
      "[42 200] loss_ae: 14.071565 loss_g: 70.611921 loss_d: -57.076748 time: 8.2 s\n",
      "[42 250] loss_ae: 14.841294 loss_g: 86.694018 loss_d: -75.886183 time: 8.2 s\n",
      "[42 300] loss_ae: 13.842741 loss_g: 77.875941 loss_d: -64.768687 time: 8.1 s\n",
      "[42 350] loss_ae: 14.760675 loss_g: 76.726012 loss_d: -64.036112 time: 8.1 s\n",
      "[42 400] loss_ae: 12.446553 loss_g: 86.004423 loss_d: -68.938650 time: 8.0 s\n",
      "[42 450] loss_ae: 13.883537 loss_g: 85.050676 loss_d: -66.074736 time: 8.2 s\n",
      "[42 500] loss_ae: 13.609552 loss_g: 74.849598 loss_d: -61.920582 time: 8.1 s\n",
      "[42 550] loss_ae: 12.213925 loss_g: 84.438385 loss_d: -65.559674 time: 8.0 s\n",
      "[42 600] loss_ae: 13.541134 loss_g: 80.075363 loss_d: -64.185703 time: 8.3 s\n",
      "[42 650] loss_ae: 13.182556 loss_g: 86.201333 loss_d: -70.463399 time: 8.2 s\n",
      "[42 700] loss_ae: 13.247388 loss_g: 83.436771 loss_d: -71.339372 time: 8.2 s\n",
      "[42 750] loss_ae: 12.159689 loss_g: 81.028130 loss_d: -65.110210 time: 8.1 s\n",
      "[42 800] loss_ae: 13.541334 loss_g: 83.335280 loss_d: -67.021236 time: 8.1 s\n",
      "[42 850] loss_ae: 12.462488 loss_g: 77.735101 loss_d: -66.835705 time: 8.2 s\n",
      "[42 900] loss_ae: 14.262212 loss_g: 79.714906 loss_d: -65.339530 time: 8.2 s\n",
      "[42 950] loss_ae: 13.051216 loss_g: 75.976110 loss_d: -61.613870 time: 8.2 s\n",
      "[42 1000] loss_ae: 13.785519 loss_g: 71.619237 loss_d: -60.053659 time: 8.1 s\n",
      "[42 1050] loss_ae: 14.131107 loss_g: 85.018345 loss_d: -71.863296 time: 8.1 s\n",
      "[42 1100] loss_ae: 14.272873 loss_g: 72.731155 loss_d: -59.552941 time: 8.2 s\n",
      "[42 1150] loss_ae: 13.962877 loss_g: 72.948265 loss_d: -63.925034 time: 8.1 s\n",
      "[42 1200] loss_ae: 16.542778 loss_g: 81.925477 loss_d: -63.674895 time: 8.1 s\n",
      "[42 1250] loss_ae: 11.748489 loss_g: 83.310382 loss_d: -69.614523 time: 8.3 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  they ' re not going to do that but i think that ' s the only thing i ' ve ever done is that i </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  they do have a lot of <unk> and i have a lot of <unk> and i have a lot of <unk> and i have </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  it doesn ' t really </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  has been <unk> for the people who are in the military service and </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  they have a <unk> in a in a <unk> <unk> they have a <unk> <unk> in a <unk> </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  oh really </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  especially if it ' s if it ' s not a good way to go to the beach but i ' ll have to go out </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  and there ' s a lot of people that are in the in the in the in the in the in the in the in the in the in the\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and it ' s it it ' s it it ' s it it ' s it it ' s it it ' s it it ' s it </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  if they were the ones that were the ones </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and we have a lot of <unk> <unk> and we have a we have a we have a <unk> and we have a <unk> and we have a </s>\n",
      "true response:  but they </s>\n",
      "generate response:  they have they have they have they have a they have a they have a they have a <unk> they have a </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  right but it ' s not that it ' s not the same i think it ' s a good idea to have a family but </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  they have they have they have a they have a <unk> <unk> they have a <unk> they have a <unk> they have a <unk> they have a </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.447636, BLEU2 0.362901, BLEU3 0.305740, BLEU4 0.245435, inter_dist1 0.006972, inter_dist2 0.043083 avg_len 16.250319\n",
      " time: 144.6 s\n",
      "Done testing\n",
      "Epoch:  43\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[43 50] loss_ae: 16.093721 loss_g: 84.488648 loss_d: -62.858390 time: 8.2 s\n",
      "[43 100] loss_ae: 16.676248 loss_g: 71.292024 loss_d: -60.955046 time: 8.1 s\n",
      "[43 150] loss_ae: 13.253765 loss_g: 79.281599 loss_d: -64.317089 time: 8.1 s\n",
      "[43 200] loss_ae: 12.291877 loss_g: 73.097514 loss_d: -60.657020 time: 8.2 s\n",
      "[43 250] loss_ae: 12.175099 loss_g: 80.159218 loss_d: -63.741312 time: 8.1 s\n",
      "[43 300] loss_ae: 13.446215 loss_g: 70.934189 loss_d: -62.340724 time: 8.2 s\n",
      "[43 350] loss_ae: 13.609259 loss_g: 86.941165 loss_d: -71.242956 time: 8.2 s\n",
      "[43 400] loss_ae: 13.536973 loss_g: 78.692191 loss_d: -64.003106 time: 8.1 s\n",
      "[43 450] loss_ae: 15.112712 loss_g: 69.632522 loss_d: -63.865466 time: 8.2 s\n",
      "[43 500] loss_ae: 13.471626 loss_g: 80.705723 loss_d: -69.189169 time: 8.1 s\n",
      "[43 550] loss_ae: 13.502996 loss_g: 74.442870 loss_d: -61.441439 time: 8.2 s\n",
      "[43 600] loss_ae: 13.869040 loss_g: 73.190058 loss_d: -63.701284 time: 8.2 s\n",
      "[43 650] loss_ae: 12.310237 loss_g: 74.927523 loss_d: -60.764842 time: 8.1 s\n",
      "[43 700] loss_ae: 12.276351 loss_g: 75.618003 loss_d: -59.279604 time: 8.3 s\n",
      "[43 750] loss_ae: 14.171931 loss_g: 82.553058 loss_d: -65.802903 time: 8.1 s\n",
      "[43 800] loss_ae: 13.822502 loss_g: 79.140393 loss_d: -64.349878 time: 8.1 s\n",
      "[43 850] loss_ae: 13.806141 loss_g: 71.027730 loss_d: -58.038455 time: 8.2 s\n",
      "[43 900] loss_ae: 12.679932 loss_g: 77.971351 loss_d: -64.029768 time: 8.0 s\n",
      "[43 950] loss_ae: 13.049122 loss_g: 68.063900 loss_d: -63.590224 time: 8.2 s\n",
      "[43 1000] loss_ae: 14.186909 loss_g: 75.164791 loss_d: -58.054426 time: 8.0 s\n",
      "[43 1050] loss_ae: 12.765039 loss_g: 66.758934 loss_d: -48.212435 time: 8.1 s\n",
      "[43 1100] loss_ae: 13.358802 loss_g: 83.279318 loss_d: -68.906792 time: 8.2 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43 1150] loss_ae: 11.303818 loss_g: 70.866063 loss_d: -60.696419 time: 8.2 s\n",
      "[43 1200] loss_ae: 12.802812 loss_g: 69.402908 loss_d: -53.274745 time: 8.2 s\n",
      "[43 1250] loss_ae: 12.084300 loss_g: 63.054377 loss_d: -52.976743 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  you know they just don ' t know what ' s going on in the world and they ' re not going to be able to </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  and they have they have a they have a <unk> <unk> they have a <unk> they have a <unk> </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  don don don don ' t they don ' t really do anything like that either </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  we have to have a <unk> we have a we have a we have a we have a we have a <unk> a <unk> a <unk> a </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  oh i ' ll bet you </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  they have a they have a they have a they have a they have a they have a <unk> they have a <unk> you have a </s>\n",
      "true response:  oh </s>\n",
      "generate response:  oh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and you know you just have to go in and get it done and then it ' s just a little bit more fun and </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  there ' s so many people that are in the in the in the in the in the in the in the in the in the in the </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and well i think that ' s the only thing i ' ve ever done is that i ' ve been in the past three years </s>\n",
      "true response:  but they </s>\n",
      "generate response:  they have they have they have a they have a they have a they have a they have a they have a </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  you can you can get your <unk> in your <unk> and you can you can you can you can you can you can you can you can </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and you know you can you can you can you can you can you can you can you can you can you can you can you can you can </s>\n",
      "true response:  and </s>\n",
      "generate response:  oh that ' s what ' s it ' s been nice talking with you too bye - bye and you know bye - bye </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  we have to have to have a we have a we have a we have a we have a we have a <unk> we have a </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  uh - huh </s>\n",
      "BLEU1 0.440062, BLEU2 0.358509, BLEU3 0.302909, BLEU4 0.243472, inter_dist1 0.006724, inter_dist2 0.040079 avg_len 16.471447\n",
      " time: 145.0 s\n",
      "Done testing\n",
      "Epoch:  44\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[44 50] loss_ae: 13.385975 loss_g: 78.694662 loss_d: -59.943410 time: 8.1 s\n",
      "[44 100] loss_ae: 13.354492 loss_g: 77.989536 loss_d: -67.234590 time: 8.1 s\n",
      "[44 150] loss_ae: 13.501206 loss_g: 77.006888 loss_d: -65.561417 time: 8.0 s\n",
      "[44 200] loss_ae: 13.067580 loss_g: 70.985658 loss_d: -56.969974 time: 8.0 s\n",
      "[44 250] loss_ae: 11.882005 loss_g: 78.859352 loss_d: -64.405375 time: 8.2 s\n",
      "[44 300] loss_ae: 13.203064 loss_g: 78.457175 loss_d: -59.173092 time: 8.2 s\n",
      "[44 350] loss_ae: 14.000143 loss_g: 77.699623 loss_d: -59.330429 time: 8.2 s\n",
      "[44 400] loss_ae: 15.129263 loss_g: 75.937857 loss_d: -63.970202 time: 8.1 s\n",
      "[44 450] loss_ae: 12.832745 loss_g: 70.604963 loss_d: -57.951920 time: 8.1 s\n",
      "[44 500] loss_ae: 13.452018 loss_g: 73.323580 loss_d: -57.451149 time: 8.2 s\n",
      "[44 550] loss_ae: 13.682946 loss_g: 78.159447 loss_d: -64.299971 time: 8.2 s\n",
      "[44 600] loss_ae: 12.395275 loss_g: 75.624316 loss_d: -58.013211 time: 8.2 s\n",
      "[44 650] loss_ae: 15.624383 loss_g: 83.045675 loss_d: -62.402428 time: 8.2 s\n",
      "[44 700] loss_ae: 14.872810 loss_g: 71.329025 loss_d: -58.742128 time: 8.2 s\n",
      "[44 750] loss_ae: 13.244031 loss_g: 70.803681 loss_d: -63.579991 time: 8.2 s\n",
      "[44 800] loss_ae: 12.706594 loss_g: 74.684176 loss_d: -62.398917 time: 8.2 s\n",
      "[44 850] loss_ae: 15.222142 loss_g: 72.807466 loss_d: -59.200851 time: 8.2 s\n",
      "[44 900] loss_ae: 12.644101 loss_g: 67.906347 loss_d: -51.473142 time: 8.2 s\n",
      "[44 950] loss_ae: 12.284595 loss_g: 72.516674 loss_d: -60.402398 time: 8.2 s\n",
      "[44 1000] loss_ae: 15.193521 loss_g: 71.803632 loss_d: -53.342258 time: 8.2 s\n",
      "[44 1050] loss_ae: 13.395177 loss_g: 78.110233 loss_d: -57.744736 time: 8.2 s\n",
      "[44 1100] loss_ae: 14.130915 loss_g: 78.794681 loss_d: -64.103530 time: 8.1 s\n",
      "[44 1150] loss_ae: 12.836912 loss_g: 72.170793 loss_d: -56.070817 time: 8.2 s\n",
      "[44 1200] loss_ae: 13.487405 loss_g: 71.974714 loss_d: -62.133794 time: 8.1 s\n",
      "[44 1250] loss_ae: 12.988426 loss_g: 72.069602 loss_d: -55.764955 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and it it ' s it it ' s it it ' s it it ' s it it ' s it it ' s it it ' </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  let ' s see we </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  i guess i ' ve heard of it that </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  and you know </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  oh really </s>\n",
      "true response:  oh </s>\n",
      "generate response:  and i know it </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and even though the weather is pretty dry and the snow and the rain and the rain and the rain and the rain and everything and </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  right </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  if </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  they just have to go to the park and they have a lot of problems with the kids and the kids and the kids and </s>\n",
      "true response:  but they </s>\n",
      "generate response:  and yeah </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  oh </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know we have a lot of people in our family we have a lot of people that are in the home and i ' m </s>\n",
      "true response:  and </s>\n",
      "generate response:  but it ' s it ' s it ' s it ' s it ' s it ' s it ' s it ' s it ' s </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  and that ' s what it ' s like when you get out there and you ' re not going to get a good </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  uh - huh huh </s>\n",
      "BLEU1 0.441604, BLEU2 0.360268, BLEU3 0.305227, BLEU4 0.245822, inter_dist1 0.007019, inter_dist2 0.044878 avg_len 16.792009\n",
      " time: 144.9 s\n",
      "Done testing\n",
      "Epoch:  45\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[45 50] loss_ae: 13.267441 loss_g: 73.404063 loss_d: -59.221283 time: 8.1 s\n",
      "[45 100] loss_ae: 13.368439 loss_g: 79.870507 loss_d: -67.385080 time: 8.1 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45 150] loss_ae: 13.264503 loss_g: 69.318129 loss_d: -55.219090 time: 8.2 s\n",
      "[45 200] loss_ae: 11.572850 loss_g: 71.265170 loss_d: -58.761178 time: 8.2 s\n",
      "[45 250] loss_ae: 12.365019 loss_g: 75.066510 loss_d: -57.947377 time: 8.1 s\n",
      "[45 300] loss_ae: 14.626301 loss_g: 77.675261 loss_d: -62.770832 time: 8.3 s\n",
      "[45 350] loss_ae: 15.023743 loss_g: 74.126505 loss_d: -57.686898 time: 8.1 s\n",
      "[45 400] loss_ae: 13.372075 loss_g: 76.184355 loss_d: -64.123051 time: 8.2 s\n",
      "[45 450] loss_ae: 15.212722 loss_g: 81.684488 loss_d: -66.254872 time: 8.3 s\n",
      "[45 500] loss_ae: 14.968741 loss_g: 62.515265 loss_d: -52.146907 time: 8.2 s\n",
      "[45 550] loss_ae: 13.380012 loss_g: 66.144416 loss_d: -52.880338 time: 8.1 s\n",
      "[45 600] loss_ae: 13.765258 loss_g: 70.493287 loss_d: -58.919594 time: 8.2 s\n",
      "[45 650] loss_ae: 13.844587 loss_g: 71.334291 loss_d: -59.161275 time: 8.1 s\n",
      "[45 700] loss_ae: 13.179829 loss_g: 73.632174 loss_d: -56.947465 time: 8.2 s\n",
      "[45 750] loss_ae: 14.250845 loss_g: 75.113453 loss_d: -67.107167 time: 8.2 s\n",
      "[45 800] loss_ae: 12.800891 loss_g: 75.735921 loss_d: -60.685484 time: 8.1 s\n",
      "[45 850] loss_ae: 12.866678 loss_g: 79.662618 loss_d: -69.737354 time: 8.2 s\n",
      "[45 900] loss_ae: 14.074450 loss_g: 69.486860 loss_d: -55.335613 time: 8.1 s\n",
      "[45 950] loss_ae: 14.251641 loss_g: 73.834876 loss_d: -62.737864 time: 8.1 s\n",
      "[45 1000] loss_ae: 13.074853 loss_g: 76.744861 loss_d: -63.608088 time: 8.2 s\n",
      "[45 1050] loss_ae: 12.409213 loss_g: 70.210840 loss_d: -57.991957 time: 8.1 s\n",
      "[45 1100] loss_ae: 13.313830 loss_g: 71.594457 loss_d: -58.584795 time: 8.0 s\n",
      "[45 1150] loss_ae: 13.434598 loss_g: 75.537195 loss_d: -56.345118 time: 8.1 s\n",
      "[45 1200] loss_ae: 15.367662 loss_g: 73.826274 loss_d: -56.837115 time: 8.3 s\n",
      "[45 1250] loss_ae: 12.293345 loss_g: 81.247734 loss_d: -61.496233 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  you know just to get to get a little <unk> and you know you can ' t get a <unk> of a <unk> and you </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  you know if the judge </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  and you know they ' re just they ' re just they ' re just <unk> <unk> </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  they just you know they have a little bit of a <unk> and they ' re all they ' re all they ' re all they ' re </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and they have a lot of people that are in the in the in the in the in the in the in the community and the </s>\n",
      "true response:  but they </s>\n",
      "generate response:  i ' ve just you know you have to have a <unk> and you have to have a <unk> you have to have a <unk> you have to have </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  i don ' t know i ' ve i ' ve been in the past several years and i ' ve been in the past three years and </s>\n",
      "true response:  and </s>\n",
      "generate response:  you know you just have to have a have a <unk> you have to have a you have a you have a <unk> you have to have a </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  yeah you don ' t have any problem with them or anything </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  you know and she just kind of have a <unk> and she just you know just a <unk> and she just <unk> </s>\n",
      "BLEU1 0.438053, BLEU2 0.355208, BLEU3 0.299205, BLEU4 0.240042, inter_dist1 0.006741, inter_dist2 0.040495 avg_len 16.213282\n",
      " time: 144.7 s\n",
      "Done testing\n",
      "Epoch:  46\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[46 50] loss_ae: 14.725185 loss_g: 81.895841 loss_d: -63.880410 time: 8.1 s\n",
      "[46 100] loss_ae: 13.483785 loss_g: 76.767814 loss_d: -62.608190 time: 8.3 s\n",
      "[46 150] loss_ae: 15.112469 loss_g: 77.949084 loss_d: -62.679606 time: 8.1 s\n",
      "[46 200] loss_ae: 11.030782 loss_g: 73.961726 loss_d: -57.937426 time: 8.4 s\n",
      "[46 250] loss_ae: 12.550138 loss_g: 76.002404 loss_d: -58.726102 time: 8.1 s\n",
      "[46 300] loss_ae: 13.733611 loss_g: 77.385670 loss_d: -60.686054 time: 8.2 s\n",
      "[46 350] loss_ae: 14.723650 loss_g: 78.266746 loss_d: -55.183940 time: 8.3 s\n",
      "[46 400] loss_ae: 12.225817 loss_g: 65.661603 loss_d: -50.403026 time: 8.1 s\n",
      "[46 450] loss_ae: 13.756237 loss_g: 67.009770 loss_d: -53.763231 time: 8.2 s\n",
      "[46 500] loss_ae: 13.546024 loss_g: 65.065089 loss_d: -51.932561 time: 8.1 s\n",
      "[46 550] loss_ae: 13.043021 loss_g: 72.332763 loss_d: -57.660947 time: 8.2 s\n",
      "[46 600] loss_ae: 13.589135 loss_g: 69.890620 loss_d: -50.464302 time: 8.3 s\n",
      "[46 650] loss_ae: 12.958153 loss_g: 73.716756 loss_d: -64.002144 time: 8.1 s\n",
      "[46 700] loss_ae: 14.200185 loss_g: 67.405679 loss_d: -52.578308 time: 8.1 s\n",
      "[46 750] loss_ae: 11.894941 loss_g: 74.841655 loss_d: -59.414909 time: 8.4 s\n",
      "[46 800] loss_ae: 13.283116 loss_g: 79.039343 loss_d: -68.079114 time: 8.2 s\n",
      "[46 850] loss_ae: 13.814091 loss_g: 77.630217 loss_d: -60.301376 time: 8.3 s\n",
      "[46 900] loss_ae: 13.882561 loss_g: 82.425516 loss_d: -64.741297 time: 8.1 s\n",
      "[46 950] loss_ae: 13.181322 loss_g: 74.375997 loss_d: -60.854422 time: 8.2 s\n",
      "[46 1000] loss_ae: 13.601787 loss_g: 76.086004 loss_d: -59.714045 time: 8.3 s\n",
      "[46 1050] loss_ae: 13.630888 loss_g: 75.189377 loss_d: -60.066225 time: 8.2 s\n",
      "[46 1100] loss_ae: 15.178917 loss_g: 81.866922 loss_d: -64.507781 time: 8.2 s\n",
      "[46 1150] loss_ae: 12.955194 loss_g: 80.965374 loss_d: -71.017374 time: 8.3 s\n",
      "[46 1200] loss_ae: 13.236036 loss_g: 80.740023 loss_d: -66.610281 time: 8.0 s\n",
      "[46 1250] loss_ae: 11.546835 loss_g: 74.524643 loss_d: -59.222494 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  you know a lot of people have a lot of <unk> and they have a lot of <unk> and they have a </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  and </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  oh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  it i just think it </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  they just get </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  and it doesn ' t have that much </s>\n",
      "true response:  oh </s>\n",
      "generate response:  i don ' t know i don ' t know i think it ' s a good idea </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and they have to go in and get the <unk> and the <unk> and the <unk> and the <unk> and the <unk> and the <unk> and </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah they ' re they ' re they ' re they ' re they ' re they ' re they ' re they ' re they ' re </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and you know they were all over the country and they were really good and </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  oh </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and if they do that they ' ll do that they ' ll do that they ' ll do that they ' ll do it again </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  but they </s>\n",
      "generate response:  you know in fact he ' s a he ' s a <unk> and he ' s a real good guy he ' s a real good </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  you know it ' s not really expensive </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  oh </s>\n",
      "true response:  and </s>\n",
      "generate response:  it really doesn ' t seem to be any good to go to a place where you ' re in a small town </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  oh are you </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  uh - huh </s>\n",
      "BLEU1 0.442634, BLEU2 0.358468, BLEU3 0.302032, BLEU4 0.242476, inter_dist1 0.007436, inter_dist2 0.046731 avg_len 15.335340\n",
      " time: 145.6 s\n",
      "Done testing\n",
      "Epoch:  47\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[47 50] loss_ae: 11.928958 loss_g: 76.140757 loss_d: -58.774302 time: 8.3 s\n",
      "[47 100] loss_ae: 13.748794 loss_g: 72.074594 loss_d: -62.168183 time: 8.2 s\n",
      "[47 150] loss_ae: 14.210888 loss_g: 72.227140 loss_d: -62.014313 time: 8.2 s\n",
      "[47 200] loss_ae: 12.916629 loss_g: 69.303563 loss_d: -58.187553 time: 8.2 s\n",
      "[47 250] loss_ae: 13.724021 loss_g: 81.011899 loss_d: -64.348385 time: 8.1 s\n",
      "[47 300] loss_ae: 12.771561 loss_g: 89.411650 loss_d: -77.540292 time: 8.2 s\n",
      "[47 350] loss_ae: 12.134694 loss_g: 75.489219 loss_d: -60.041023 time: 8.1 s\n",
      "[47 400] loss_ae: 13.801188 loss_g: 74.389001 loss_d: -58.736537 time: 8.2 s\n",
      "[47 450] loss_ae: 12.286916 loss_g: 73.562931 loss_d: -62.214484 time: 8.4 s\n",
      "[47 500] loss_ae: 14.675947 loss_g: 74.575194 loss_d: -59.958280 time: 8.1 s\n",
      "[47 550] loss_ae: 13.799398 loss_g: 76.402339 loss_d: -53.590222 time: 8.2 s\n",
      "[47 600] loss_ae: 12.440799 loss_g: 71.651089 loss_d: -59.469666 time: 8.2 s\n",
      "[47 650] loss_ae: 13.518386 loss_g: 77.882412 loss_d: -69.210471 time: 8.2 s\n",
      "[47 700] loss_ae: 15.897808 loss_g: 70.456620 loss_d: -59.095914 time: 8.1 s\n",
      "[47 750] loss_ae: 14.758749 loss_g: 87.087274 loss_d: -71.016235 time: 8.2 s\n",
      "[47 800] loss_ae: 14.225439 loss_g: 72.382261 loss_d: -62.787938 time: 8.2 s\n",
      "[47 850] loss_ae: 13.103824 loss_g: 85.765844 loss_d: -72.492909 time: 8.2 s\n",
      "[47 900] loss_ae: 13.829405 loss_g: 87.728147 loss_d: -67.455267 time: 8.3 s\n",
      "[47 950] loss_ae: 12.838831 loss_g: 83.071981 loss_d: -60.747758 time: 8.2 s\n",
      "[47 1000] loss_ae: 12.224535 loss_g: 84.563805 loss_d: -66.702356 time: 8.0 s\n",
      "[47 1050] loss_ae: 12.947214 loss_g: 83.743924 loss_d: -67.435456 time: 8.2 s\n",
      "[47 1100] loss_ae: 12.456489 loss_g: 85.484533 loss_d: -67.484515 time: 8.2 s\n",
      "[47 1150] loss_ae: 13.512388 loss_g: 64.974459 loss_d: -55.280642 time: 8.1 s\n",
      "[47 1200] loss_ae: 13.271627 loss_g: 78.583508 loss_d: -63.024364 time: 8.2 s\n",
      "[47 1250] loss_ae: 11.718838 loss_g: 76.185940 loss_d: -58.569950 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  they just they just they just they just they just don ' t want to do it and i think they ' re going to have to </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  they do have to do a lot of things i mean i mean i ' ve been real lucky i ' ve </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  i </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  you know you just can ' t get a job in there and </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  if it is </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  oh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  to do that but that ' s i don ' t know i don ' t know if you ' ve ever heard of it but </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  they ' re just they ' re just they ' re just they ' re just a little <unk> they ' re just a <unk> </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  you know and they have to have to have a certain amount of time to do it </s>\n",
      "true response:  but they </s>\n",
      "generate response:  but but if you can put them in a separate room and they put it in a <unk> and they ' re not going to </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  oh </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  we i ' ve had a we have a we have a we have a we have a we have a we have a </s>\n",
      "true response:  and </s>\n",
      "generate response:  and you don ' t you don ' t you don ' t have to you have to have a you have to have a you </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  oh really is </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  uh - huh </s>\n",
      "BLEU1 0.434206, BLEU2 0.351562, BLEU3 0.295628, BLEU4 0.236951, inter_dist1 0.007817, inter_dist2 0.049738 avg_len 15.054735\n",
      " time: 145.6 s\n",
      "Done testing\n",
      "Epoch:  48\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[48 50] loss_ae: 12.259489 loss_g: 78.323255 loss_d: -64.407822 time: 8.2 s\n",
      "[48 100] loss_ae: 13.068021 loss_g: 81.267274 loss_d: -65.317593 time: 8.1 s\n",
      "[48 150] loss_ae: 14.176778 loss_g: 80.890445 loss_d: -69.663657 time: 8.3 s\n",
      "[48 200] loss_ae: 12.830095 loss_g: 81.873289 loss_d: -66.140457 time: 8.2 s\n",
      "[48 250] loss_ae: 12.985912 loss_g: 81.948631 loss_d: -63.418703 time: 8.2 s\n",
      "[48 300] loss_ae: 14.205576 loss_g: 81.249763 loss_d: -66.542924 time: 8.3 s\n",
      "[48 350] loss_ae: 13.750451 loss_g: 85.193034 loss_d: -67.387636 time: 8.1 s\n",
      "[48 400] loss_ae: 12.460976 loss_g: 77.912147 loss_d: -63.216571 time: 8.3 s\n",
      "[48 450] loss_ae: 13.849053 loss_g: 71.377259 loss_d: -58.257960 time: 8.2 s\n",
      "[48 500] loss_ae: 15.253766 loss_g: 75.546009 loss_d: -59.697773 time: 8.2 s\n",
      "[48 550] loss_ae: 13.129711 loss_g: 77.485668 loss_d: -61.417159 time: 8.2 s\n",
      "[48 600] loss_ae: 11.972710 loss_g: 73.865225 loss_d: -59.372336 time: 8.2 s\n",
      "[48 650] loss_ae: 12.396833 loss_g: 80.501636 loss_d: -58.464338 time: 8.1 s\n",
      "[48 700] loss_ae: 12.280770 loss_g: 86.827041 loss_d: -68.657332 time: 8.1 s\n",
      "[48 750] loss_ae: 14.058444 loss_g: 73.406799 loss_d: -64.455035 time: 8.1 s\n",
      "[48 800] loss_ae: 13.893520 loss_g: 85.727538 loss_d: -69.950884 time: 8.2 s\n",
      "[48 850] loss_ae: 14.271424 loss_g: 80.989888 loss_d: -63.745242 time: 8.2 s\n",
      "[48 900] loss_ae: 14.222426 loss_g: 77.389419 loss_d: -68.366939 time: 8.3 s\n",
      "[48 950] loss_ae: 12.425576 loss_g: 77.652949 loss_d: -63.952903 time: 8.2 s\n",
      "[48 1000] loss_ae: 13.592269 loss_g: 65.872966 loss_d: -56.853261 time: 8.1 s\n",
      "[48 1050] loss_ae: 12.261891 loss_g: 76.705598 loss_d: -63.890797 time: 8.3 s\n",
      "[48 1100] loss_ae: 14.009752 loss_g: 80.598869 loss_d: -66.322738 time: 8.3 s\n",
      "[48 1150] loss_ae: 13.096360 loss_g: 69.195193 loss_d: -51.810947 time: 8.0 s\n",
      "[48 1200] loss_ae: 13.207499 loss_g: 78.292895 loss_d: -62.115370 time: 8.2 s\n",
      "[48 1250] loss_ae: 12.948549 loss_g: 83.618917 loss_d: -69.626000 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and you know they have they have a they have a <unk> they have a <unk> and they have a <unk> and they </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  have they have they have a big problem with their kids </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  they have to have to have a <unk> they have to have a <unk> they have to have a <unk> they have to have a </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  something like that </s>\n",
      "generate response:  and </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  if they if they if they don ' t do that they ' ll do that they ' ll do that they ' ll do that they ' ll </s>\n",
      "true response:  oh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  to you know to go to you know to go to a you know a <unk> or a <unk> or something like that </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  i don ' t know i don ' t know if you ' ve ever heard of it but they ' re not going to be good </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  but they really are and it </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  you know like you say you have to have a you have a you have a you have a you have a you have a </s>\n",
      "true response:  but they </s>\n",
      "generate response:  and oh </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know i mean i mean i think it ' s a lot of fun i think it ' s a lot of fun </s>\n",
      "true response:  and </s>\n",
      "generate response:  we have a lot of <unk> <unk> we have a we have a we have a <unk> a <unk> a <unk> <unk> <unk> <unk> </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  uh - huh </s>\n",
      "BLEU1 0.440026, BLEU2 0.357487, BLEU3 0.302498, BLEU4 0.243557, inter_dist1 0.007122, inter_dist2 0.045529 avg_len 15.549352\n",
      " time: 144.8 s\n",
      "Done testing\n",
      "Epoch:  49\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[49 50] loss_ae: 13.025449 loss_g: 68.995613 loss_d: -59.559747 time: 8.2 s\n",
      "[49 100] loss_ae: 13.896219 loss_g: 76.798141 loss_d: -56.546535 time: 8.1 s\n",
      "[49 150] loss_ae: 13.239897 loss_g: 71.526010 loss_d: -60.211267 time: 8.4 s\n",
      "[49 200] loss_ae: 13.051421 loss_g: 76.501016 loss_d: -63.350809 time: 8.4 s\n",
      "[49 250] loss_ae: 15.002699 loss_g: 67.111073 loss_d: -53.099264 time: 8.2 s\n",
      "[49 300] loss_ae: 12.764444 loss_g: 77.128961 loss_d: -63.562391 time: 8.2 s\n",
      "[49 350] loss_ae: 13.177270 loss_g: 78.967850 loss_d: -59.944488 time: 8.2 s\n",
      "[49 400] loss_ae: 12.995368 loss_g: 65.426804 loss_d: -58.770138 time: 8.2 s\n",
      "[49 450] loss_ae: 13.595467 loss_g: 74.237958 loss_d: -59.033982 time: 8.3 s\n",
      "[49 500] loss_ae: 13.085505 loss_g: 66.795156 loss_d: -56.966521 time: 8.2 s\n",
      "[49 550] loss_ae: 12.863366 loss_g: 85.590377 loss_d: -71.972264 time: 8.3 s\n",
      "[49 600] loss_ae: 12.375020 loss_g: 84.442874 loss_d: -68.926091 time: 8.2 s\n",
      "[49 650] loss_ae: 14.560000 loss_g: 82.564929 loss_d: -66.983504 time: 8.1 s\n",
      "[49 700] loss_ae: 14.230530 loss_g: 82.946599 loss_d: -63.828737 time: 8.2 s\n",
      "[49 750] loss_ae: 12.307745 loss_g: 72.953934 loss_d: -63.563941 time: 8.1 s\n",
      "[49 800] loss_ae: 11.801013 loss_g: 82.372808 loss_d: -61.920249 time: 8.2 s\n",
      "[49 850] loss_ae: 12.446883 loss_g: 76.052870 loss_d: -65.026715 time: 8.3 s\n",
      "[49 900] loss_ae: 11.358160 loss_g: 70.649772 loss_d: -57.726121 time: 8.2 s\n",
      "[49 950] loss_ae: 12.984770 loss_g: 77.658583 loss_d: -53.899224 time: 8.3 s\n",
      "[49 1000] loss_ae: 15.603082 loss_g: 65.325887 loss_d: -50.641395 time: 8.0 s\n",
      "[49 1050] loss_ae: 12.275515 loss_g: 74.187638 loss_d: -63.082305 time: 8.2 s\n",
      "[49 1100] loss_ae: 14.302152 loss_g: 81.349911 loss_d: -72.763340 time: 8.2 s\n",
      "[49 1150] loss_ae: 12.294822 loss_g: 82.000154 loss_d: -65.586251 time: 8.3 s\n",
      "[49 1200] loss_ae: 13.748262 loss_g: 82.341075 loss_d: -59.372067 time: 8.3 s\n",
      "[49 1250] loss_ae: 12.860304 loss_g: 62.043466 loss_d: -48.240760 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  or if there is a problem with the school system and the kids are very active in the school and the kids are very young and they </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  hello </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  but they do have to do that and i think that ' s a good idea i don ' t think i ' d ever </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  and we have a lot of the <unk> and the <unk> and the <unk> and the <unk> and the <unk> and the <unk> and the </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  oh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  have some nice little <unk> and it ' s kind of a nice place to go to the beach and the other one is </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  in fact they had a <unk> and they had a <unk> of a <unk> and they had a <unk> </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and it ' s it it ' s it it ' s it it ' s it it ' s it it ' s it it ' </s>\n",
      "true response:  but they </s>\n",
      "generate response:  i you know i ' m i ' m i i ' m i i ' m i i ' m i i ' m i i ' m </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  uh - huh well they have a they have a they have a they have a they have a <unk> they have a <unk> they have a </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know that ' s what i ' m saying i ' m not sure if i ' m going to be able to do </s>\n",
      "true response:  and </s>\n",
      "generate response:  to do </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  there ' ll do you do you do you live in the house or </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  not really not really <unk> </s>\n",
      "BLEU1 0.446498, BLEU2 0.361921, BLEU3 0.305708, BLEU4 0.245936, inter_dist1 0.007062, inter_dist2 0.045384 avg_len 15.785076\n",
      " time: 145.6 s\n",
      "Done testing\n",
      "Epoch:  50\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[50 50] loss_ae: 13.832522 loss_g: 75.363244 loss_d: -53.658423 time: 8.1 s\n",
      "[50 100] loss_ae: 13.192661 loss_g: 69.673687 loss_d: -54.405798 time: 8.2 s\n",
      "[50 150] loss_ae: 14.403085 loss_g: 73.453421 loss_d: -66.910120 time: 8.1 s\n",
      "[50 200] loss_ae: 13.908051 loss_g: 69.331311 loss_d: -49.292111 time: 8.1 s\n",
      "[50 250] loss_ae: 13.507006 loss_g: 70.933503 loss_d: -60.090812 time: 8.2 s\n",
      "[50 300] loss_ae: 12.418712 loss_g: 75.941829 loss_d: -59.600536 time: 8.2 s\n",
      "[50 350] loss_ae: 12.551524 loss_g: 76.083695 loss_d: -61.197847 time: 8.2 s\n",
      "[50 400] loss_ae: 13.537399 loss_g: 72.329342 loss_d: -54.389645 time: 8.2 s\n",
      "[50 450] loss_ae: 13.936479 loss_g: 80.475144 loss_d: -67.534541 time: 8.1 s\n",
      "[50 500] loss_ae: 14.574496 loss_g: 76.905538 loss_d: -68.283138 time: 8.2 s\n",
      "[50 550] loss_ae: 13.004104 loss_g: 74.302567 loss_d: -53.296770 time: 8.2 s\n",
      "[50 600] loss_ae: 12.708316 loss_g: 72.844867 loss_d: -57.161706 time: 8.2 s\n",
      "[50 650] loss_ae: 11.924522 loss_g: 81.186498 loss_d: -59.677978 time: 8.2 s\n",
      "[50 700] loss_ae: 12.201766 loss_g: 74.379383 loss_d: -61.131114 time: 8.2 s\n",
      "[50 750] loss_ae: 13.803957 loss_g: 81.459302 loss_d: -65.149974 time: 8.1 s\n",
      "[50 800] loss_ae: 14.286215 loss_g: 79.715366 loss_d: -70.033691 time: 8.3 s\n",
      "[50 850] loss_ae: 13.736544 loss_g: 80.607629 loss_d: -66.480481 time: 8.3 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50 900] loss_ae: 14.192778 loss_g: 72.534483 loss_d: -59.234739 time: 8.1 s\n",
      "[50 950] loss_ae: 12.405971 loss_g: 83.628374 loss_d: -68.843308 time: 8.2 s\n",
      "[50 1000] loss_ae: 13.799872 loss_g: 80.137710 loss_d: -67.054188 time: 8.2 s\n",
      "[50 1050] loss_ae: 12.006888 loss_g: 88.816797 loss_d: -72.749808 time: 8.2 s\n",
      "[50 1100] loss_ae: 15.091746 loss_g: 85.541025 loss_d: -66.427308 time: 8.3 s\n",
      "[50 1150] loss_ae: 13.384642 loss_g: 78.558480 loss_d: -68.254283 time: 8.1 s\n",
      "[50 1200] loss_ae: 12.056616 loss_g: 81.226975 loss_d: -62.200268 time: 8.3 s\n",
      "[50 1250] loss_ae: 12.585144 loss_g: 77.659974 loss_d: -64.676538 time: 8.3 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  have been up to a university of texas and i ' m in the university of texas and i ' m in the university of </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  but what are the what kind of things do you like to do </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  they ' ve got they ' ve got a lot of good players </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  or if it doesn ' t do it if they do it they don ' t do it they don ' t do it they don ' t </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  they have a lot of <unk> they have a they have a <unk> they have a <unk> they have a <unk> they have a <unk> </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  oh i don ' t i don ' t i don ' t i don ' t i don ' t like the way i like i like i like\n",
      "true response:  oh </s>\n",
      "generate response:  they have a <unk> <unk> in <unk> and they have a <unk> and they have a <unk> and they have a <unk> and they have a <unk> and they have\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh and you have to have a child to be in a nursing home </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  you know you can ' t you can ' t tell you they ' ll just tell you they ' ll tell you if they if they don ' </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  you know it ' s not it ' s not it ' s not it ' s not it ' s not really it ' s not </s>\n",
      "true response:  but they </s>\n",
      "generate response:  but no nobody wants to go to the store </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know i don ' t know i don ' t know i don ' t know if you ' ve ever heard of a </s>\n",
      "true response:  and </s>\n",
      "generate response:  to do because i think that ' s the only thing i ' ve ever done is the fact that i ' ve been in </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  i don ' t </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  <unk> or </s>\n",
      "BLEU1 0.434591, BLEU2 0.351532, BLEU3 0.296103, BLEU4 0.237716, inter_dist1 0.007748, inter_dist2 0.048483 avg_len 15.471264\n",
      " time: 144.6 s\n",
      "Done testing\n",
      "Epoch:  51\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[51 50] loss_ae: 13.955990 loss_g: 84.474428 loss_d: -72.237109 time: 8.3 s\n",
      "[51 100] loss_ae: 13.557473 loss_g: 74.093107 loss_d: -54.219151 time: 8.3 s\n",
      "[51 150] loss_ae: 12.644958 loss_g: 78.996482 loss_d: -57.888068 time: 8.3 s\n",
      "[51 200] loss_ae: 12.430664 loss_g: 73.655903 loss_d: -61.023497 time: 8.2 s\n",
      "[51 250] loss_ae: 11.758868 loss_g: 82.867035 loss_d: -68.449658 time: 8.3 s\n",
      "[51 300] loss_ae: 13.653166 loss_g: 79.710155 loss_d: -61.713177 time: 8.1 s\n",
      "[51 350] loss_ae: 12.677278 loss_g: 83.897974 loss_d: -71.125729 time: 8.1 s\n",
      "[51 400] loss_ae: 12.635920 loss_g: 83.106783 loss_d: -70.591532 time: 8.3 s\n",
      "[51 450] loss_ae: 13.703094 loss_g: 76.188751 loss_d: -67.448191 time: 8.1 s\n",
      "[51 500] loss_ae: 14.651369 loss_g: 73.727448 loss_d: -65.275655 time: 8.2 s\n",
      "[51 550] loss_ae: 13.445168 loss_g: 86.777958 loss_d: -72.514615 time: 8.3 s\n",
      "[51 600] loss_ae: 13.111062 loss_g: 84.947852 loss_d: -71.266003 time: 8.1 s\n",
      "[51 650] loss_ae: 12.675959 loss_g: 94.483642 loss_d: -80.047929 time: 8.2 s\n",
      "[51 700] loss_ae: 13.101248 loss_g: 81.118246 loss_d: -67.139092 time: 8.0 s\n",
      "[51 750] loss_ae: 10.928049 loss_g: 94.697696 loss_d: -77.518697 time: 8.3 s\n",
      "[51 800] loss_ae: 14.211193 loss_g: 85.367025 loss_d: -63.260175 time: 8.2 s\n",
      "[51 850] loss_ae: 12.255172 loss_g: 80.457597 loss_d: -60.666132 time: 8.1 s\n",
      "[51 900] loss_ae: 12.100296 loss_g: 78.648481 loss_d: -62.992876 time: 8.2 s\n",
      "[51 950] loss_ae: 13.353441 loss_g: 82.350161 loss_d: -67.057833 time: 8.2 s\n",
      "[51 1000] loss_ae: 13.314285 loss_g: 80.206473 loss_d: -61.000776 time: 8.1 s\n",
      "[51 1050] loss_ae: 13.920871 loss_g: 83.249782 loss_d: -66.278374 time: 8.3 s\n",
      "[51 1100] loss_ae: 12.847056 loss_g: 78.463698 loss_d: -71.243756 time: 8.1 s\n",
      "[51 1150] loss_ae: 15.265460 loss_g: 73.124754 loss_d: -62.822565 time: 8.1 s\n",
      "[51 1200] loss_ae: 12.731109 loss_g: 78.731003 loss_d: -67.211726 time: 8.3 s\n",
      "[51 1250] loss_ae: 14.275457 loss_g: 76.746942 loss_d: -59.971826 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  you know he can do the job and he can do that he can do that he can do </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  and they have they have a <unk> <unk> they have a <unk> <unk> in a restaurant </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  you can have a lot of <unk> and you can ' t you can ' t you can ' t you can ' t <unk> </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  they were </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  they have to have you have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  oh </s>\n",
      "generate response:  you know you ' re you know you ' re just you ' re just you ' re just you ' re just a <unk> you ' re </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  they have you know you have to have a you have a you have a you have a you have a you have a <unk> you have to have </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  that ' s right it ' s it ' s it ' s not it ' s not it ' s not like the <unk> of the </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  you know you have to have a you have a you have a you have a you have a you have a <unk> you have to have a </s>\n",
      "true response:  but they </s>\n",
      "generate response:  you bet so </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know you know it ' s not like you ' re not you know you ' re not you ' re not you ' re not </s>\n",
      "true response:  and </s>\n",
      "generate response:  we you know we ' ve got a lot of <unk> and we ' ve got a lot of <unk> and we ' ve got a </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  uh - huh </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.447572, BLEU2 0.362907, BLEU3 0.306056, BLEU4 0.245864, inter_dist1 0.007187, inter_dist2 0.045483 avg_len 15.738186\n",
      " time: 144.8 s\n",
      "Done testing\n",
      "Epoch:  52\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[52 50] loss_ae: 14.095118 loss_g: 99.914553 loss_d: -77.452876 time: 8.2 s\n",
      "[52 100] loss_ae: 13.280411 loss_g: 81.249697 loss_d: -69.552970 time: 8.2 s\n",
      "[52 150] loss_ae: 11.475578 loss_g: 82.947726 loss_d: -65.522666 time: 8.1 s\n",
      "[52 200] loss_ae: 13.339310 loss_g: 78.157746 loss_d: -63.331024 time: 8.2 s\n",
      "[52 250] loss_ae: 12.024824 loss_g: 76.857448 loss_d: -67.727753 time: 8.3 s\n",
      "[52 300] loss_ae: 13.295323 loss_g: 84.269418 loss_d: -68.299925 time: 8.1 s\n",
      "[52 350] loss_ae: 12.981093 loss_g: 80.989807 loss_d: -63.951020 time: 8.2 s\n",
      "[52 400] loss_ae: 12.884617 loss_g: 88.877777 loss_d: -68.491301 time: 8.1 s\n",
      "[52 450] loss_ae: 11.835911 loss_g: 86.743360 loss_d: -69.990732 time: 8.2 s\n",
      "[52 500] loss_ae: 12.933242 loss_g: 75.443866 loss_d: -61.557735 time: 8.2 s\n",
      "[52 550] loss_ae: 13.923757 loss_g: 90.755632 loss_d: -71.698003 time: 8.0 s\n",
      "[52 600] loss_ae: 12.459752 loss_g: 76.803619 loss_d: -56.073567 time: 8.3 s\n",
      "[52 650] loss_ae: 12.379945 loss_g: 81.106671 loss_d: -65.479563 time: 8.2 s\n",
      "[52 700] loss_ae: 13.816659 loss_g: 83.835264 loss_d: -69.425071 time: 8.1 s\n",
      "[52 750] loss_ae: 13.590904 loss_g: 78.966104 loss_d: -64.433374 time: 8.3 s\n",
      "[52 800] loss_ae: 12.105343 loss_g: 81.818044 loss_d: -66.092040 time: 8.2 s\n",
      "[52 850] loss_ae: 13.466085 loss_g: 74.952435 loss_d: -62.557511 time: 8.4 s\n",
      "[52 900] loss_ae: 12.966489 loss_g: 81.507550 loss_d: -65.597571 time: 8.1 s\n",
      "[52 950] loss_ae: 13.521909 loss_g: 75.734006 loss_d: -61.098695 time: 8.1 s\n",
      "[52 1000] loss_ae: 12.740266 loss_g: 84.151083 loss_d: -64.882163 time: 8.3 s\n",
      "[52 1050] loss_ae: 12.958793 loss_g: 78.001285 loss_d: -61.455101 time: 8.1 s\n",
      "[52 1100] loss_ae: 13.402481 loss_g: 84.408390 loss_d: -70.417709 time: 8.1 s\n",
      "[52 1150] loss_ae: 12.520197 loss_g: 70.365158 loss_d: -48.404294 time: 8.3 s\n",
      "[52 1200] loss_ae: 13.565719 loss_g: 69.375065 loss_d: -54.197114 time: 8.1 s\n",
      "[52 1250] loss_ae: 12.279808 loss_g: 83.215312 loss_d: -67.237649 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and they are very young and he is he is a very good <unk> and he ' s a very good school and he ' s a </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  is it really is it ' s not a real <unk> type of thing but you know i mean it </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  they have all the people who are going to be <unk> and they ' re not going to do that but they ' re </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  well the </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  where you ' re from you ' re from texas and you ' re not going to have to do it but you ' re </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  they do that they ' re not they ' re not they ' re not they ' re not they ' re not they ' re not </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh well that ' s good yeah that ' s good yeah </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and it it ' s it it it it it it it it it it it it it it it it it it it it it it it it it\n",
      "true response:  uh - huh </s>\n",
      "generate response:  right </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  they have you have to have a you have a you have a you have a you have a you have a you have a </s>\n",
      "true response:  but they </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  sure </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and they are you know they ' re <unk> and they ' re you know they ' re just they ' re just they </s>\n",
      "true response:  and </s>\n",
      "generate response:  have to be a lot more <unk> than to do that </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  oh you don ' t do you do you do you do you do you do you do you do you do you do you do you do you do\n",
      "true response:  um - hum </s>\n",
      "generate response:  they </s>\n",
      "BLEU1 0.450679, BLEU2 0.366070, BLEU3 0.309096, BLEU4 0.248559, inter_dist1 0.006924, inter_dist2 0.042448 avg_len 15.731436\n",
      " time: 144.7 s\n",
      "Done testing\n",
      "Epoch:  53\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[53 50] loss_ae: 13.574580 loss_g: 77.629834 loss_d: -58.282929 time: 8.1 s\n",
      "[53 100] loss_ae: 12.836614 loss_g: 84.838882 loss_d: -67.648312 time: 8.1 s\n",
      "[53 150] loss_ae: 13.196733 loss_g: 74.494445 loss_d: -57.275795 time: 8.3 s\n",
      "[53 200] loss_ae: 11.594896 loss_g: 75.207725 loss_d: -58.730836 time: 8.2 s\n",
      "[53 250] loss_ae: 12.897687 loss_g: 80.259538 loss_d: -65.468744 time: 8.2 s\n",
      "[53 300] loss_ae: 13.219822 loss_g: 79.993289 loss_d: -63.208699 time: 8.2 s\n",
      "[53 350] loss_ae: 13.277706 loss_g: 84.883883 loss_d: -65.574499 time: 8.2 s\n",
      "[53 400] loss_ae: 12.523103 loss_g: 70.068552 loss_d: -53.023974 time: 8.2 s\n",
      "[53 450] loss_ae: 13.777619 loss_g: 69.806389 loss_d: -58.945105 time: 8.2 s\n",
      "[53 500] loss_ae: 13.533747 loss_g: 86.056462 loss_d: -68.711530 time: 8.2 s\n",
      "[53 550] loss_ae: 13.463779 loss_g: 82.667802 loss_d: -68.052127 time: 8.2 s\n",
      "[53 600] loss_ae: 13.127251 loss_g: 82.421769 loss_d: -64.963339 time: 8.2 s\n",
      "[53 650] loss_ae: 12.426905 loss_g: 89.237215 loss_d: -70.476215 time: 8.1 s\n",
      "[53 700] loss_ae: 12.922252 loss_g: 82.825523 loss_d: -58.335229 time: 8.2 s\n",
      "[53 750] loss_ae: 13.482201 loss_g: 75.417616 loss_d: -63.591630 time: 8.2 s\n",
      "[53 800] loss_ae: 14.851429 loss_g: 77.155331 loss_d: -62.572883 time: 8.2 s\n",
      "[53 850] loss_ae: 10.416041 loss_g: 88.756693 loss_d: -77.427571 time: 8.2 s\n",
      "[53 900] loss_ae: 11.119532 loss_g: 85.674672 loss_d: -60.794593 time: 8.2 s\n",
      "[53 950] loss_ae: 12.729550 loss_g: 71.639827 loss_d: -61.812375 time: 8.1 s\n",
      "[53 1000] loss_ae: 11.322686 loss_g: 70.599559 loss_d: -62.702739 time: 8.3 s\n",
      "[53 1050] loss_ae: 13.331471 loss_g: 78.717940 loss_d: -63.465060 time: 8.2 s\n",
      "[53 1100] loss_ae: 11.834732 loss_g: 71.694019 loss_d: -58.179425 time: 8.1 s\n",
      "[53 1150] loss_ae: 12.848808 loss_g: 77.350881 loss_d: -61.458564 time: 8.3 s\n",
      "[53 1200] loss_ae: 11.110448 loss_g: 76.753958 loss_d: -54.650959 time: 8.2 s\n",
      "[53 1250] loss_ae: 12.894345 loss_g: 81.731943 loss_d: -63.810666 time: 7.8 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  are not that there ' s no reason for them to go to school and go to school and go to school and go to school and </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  you a lot of people have a lot of people have a lot of people and they have a lot of </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  it would be interesting to see how the war is that it ' s going to be a lot of the people that are going to </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  and they have to have to have a <unk> and they have a they have a <unk> and they have a <unk> and they have </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  they ' ll </s>\n",
      "true response:  oh </s>\n",
      "generate response:  they ' re they ' re they ' re they ' re they ' re they ' re they ' re they ' re they ' </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  you know you can you can you can have a you have a you have a you have a you have a you have a </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  you do it </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  right </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  to be <unk> to be honest with you i ' m not sure that we ' re not going to do it but i think </s>\n",
      "true response:  but they </s>\n",
      "generate response:  if you if you don ' t like it you can ' t </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  trying to think it ' s hard to do that because i ' ve been to the point where i ' ve been to the point where </s>\n",
      "true response:  and </s>\n",
      "generate response:  and i think that it that it that it that it that it that it that it that it that it that it that it that it that it </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  and we do that too we have a lot of <unk> and </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  uh - huh </s>\n",
      "BLEU1 0.432377, BLEU2 0.351899, BLEU3 0.297190, BLEU4 0.238805, inter_dist1 0.007245, inter_dist2 0.045433 avg_len 15.889436\n",
      " time: 162.0 s\n",
      "Done testing\n",
      "Epoch:  54\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[54 50] loss_ae: 12.846474 loss_g: 80.959828 loss_d: -66.636064 time: 7.7 s\n",
      "[54 100] loss_ae: 12.588335 loss_g: 84.945338 loss_d: -72.774070 time: 8.2 s\n",
      "[54 150] loss_ae: 13.467953 loss_g: 76.161479 loss_d: -59.708533 time: 8.2 s\n",
      "[54 200] loss_ae: 12.433416 loss_g: 70.127760 loss_d: -61.845875 time: 8.1 s\n",
      "[54 250] loss_ae: 12.828807 loss_g: 70.191077 loss_d: -57.358587 time: 8.3 s\n",
      "[54 300] loss_ae: 12.449620 loss_g: 70.608001 loss_d: -55.219754 time: 8.2 s\n",
      "[54 350] loss_ae: 13.643152 loss_g: 74.111007 loss_d: -58.497081 time: 8.2 s\n",
      "[54 400] loss_ae: 12.958641 loss_g: 83.951261 loss_d: -66.331000 time: 8.2 s\n",
      "[54 450] loss_ae: 14.073056 loss_g: 76.540264 loss_d: -67.871747 time: 8.3 s\n",
      "[54 500] loss_ae: 11.836745 loss_g: 82.370151 loss_d: -70.551342 time: 8.3 s\n",
      "[54 550] loss_ae: 12.402874 loss_g: 84.527565 loss_d: -70.127589 time: 8.2 s\n",
      "[54 600] loss_ae: 13.884009 loss_g: 79.053650 loss_d: -64.682038 time: 8.1 s\n",
      "[54 650] loss_ae: 12.512210 loss_g: 77.977435 loss_d: -61.103474 time: 8.1 s\n",
      "[54 700] loss_ae: 14.518528 loss_g: 84.722402 loss_d: -66.299857 time: 8.3 s\n",
      "[54 750] loss_ae: 11.211742 loss_g: 72.799526 loss_d: -60.182392 time: 8.2 s\n",
      "[54 800] loss_ae: 14.900344 loss_g: 93.976647 loss_d: -77.255591 time: 8.2 s\n",
      "[54 850] loss_ae: 13.076447 loss_g: 80.298340 loss_d: -62.742810 time: 8.1 s\n",
      "[54 900] loss_ae: 12.438262 loss_g: 77.678515 loss_d: -64.298540 time: 8.1 s\n",
      "[54 950] loss_ae: 12.674163 loss_g: 88.285621 loss_d: -71.236694 time: 8.3 s\n",
      "[54 1000] loss_ae: 12.417249 loss_g: 74.295867 loss_d: -62.189466 time: 8.2 s\n",
      "[54 1050] loss_ae: 12.976345 loss_g: 67.697156 loss_d: -61.852799 time: 8.2 s\n",
      "[54 1100] loss_ae: 12.501904 loss_g: 75.884333 loss_d: -57.011219 time: 8.2 s\n",
      "[54 1150] loss_ae: 11.991352 loss_g: 70.572978 loss_d: -55.036418 time: 8.2 s\n",
      "[54 1200] loss_ae: 12.392402 loss_g: 72.268865 loss_d: -61.948980 time: 8.3 s\n",
      "[54 1250] loss_ae: 12.753267 loss_g: 64.876778 loss_d: -52.334779 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  with <unk> and he ' s a he ' s a <unk> and he ' s a <unk> and he ' s a very </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and you know and they just have a <unk> and they have to have a <unk> and they have to have a </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  that they have the right to the <unk> and the other one is the one that i ' ve seen that i ' ve seen </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and you can you can go to the <unk> and you can ' t get the <unk> and the <unk> and the <unk> and the <unk> and </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  you do that you do that you do you do </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  really what </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  you know they are they are they are they are they are they are they are you know they are they are they are you know they are they are\n",
      "true response:  but they </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and you have to have you have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  and </s>\n",
      "generate response:  you can you can you can you can you can you can you can you can you can you can you can you can you can you can you can\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  you probably do the same thing i ' ve been camping in the mountains </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  uh - huh </s>\n",
      "BLEU1 0.459887, BLEU2 0.374468, BLEU3 0.317583, BLEU4 0.256195, inter_dist1 0.006723, inter_dist2 0.041900 avg_len 16.473636\n",
      " time: 144.7 s\n",
      "Done testing\n",
      "Epoch:  55\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[55 50] loss_ae: 12.576555 loss_g: 77.850681 loss_d: -62.785861 time: 8.1 s\n",
      "[55 100] loss_ae: 12.183467 loss_g: 76.233412 loss_d: -61.385534 time: 8.2 s\n",
      "[55 150] loss_ae: 14.036638 loss_g: 71.255328 loss_d: -58.706057 time: 8.2 s\n",
      "[55 200] loss_ae: 11.898795 loss_g: 84.239916 loss_d: -69.582163 time: 8.2 s\n",
      "[55 250] loss_ae: 12.850240 loss_g: 92.705502 loss_d: -71.197372 time: 8.2 s\n",
      "[55 300] loss_ae: 13.801929 loss_g: 76.015425 loss_d: -60.150336 time: 8.0 s\n",
      "[55 350] loss_ae: 12.534909 loss_g: 72.368359 loss_d: -61.001722 time: 8.2 s\n",
      "[55 400] loss_ae: 12.121463 loss_g: 73.667076 loss_d: -58.083717 time: 8.1 s\n",
      "[55 450] loss_ae: 12.110928 loss_g: 75.192940 loss_d: -62.603635 time: 8.1 s\n",
      "[55 500] loss_ae: 13.835984 loss_g: 86.565205 loss_d: -65.140325 time: 8.2 s\n",
      "[55 550] loss_ae: 11.532139 loss_g: 85.656167 loss_d: -66.382427 time: 8.2 s\n",
      "[55 600] loss_ae: 14.084064 loss_g: 68.779295 loss_d: -59.961382 time: 8.2 s\n",
      "[55 650] loss_ae: 12.340327 loss_g: 75.753183 loss_d: -66.088709 time: 8.2 s\n",
      "[55 700] loss_ae: 12.576464 loss_g: 75.789638 loss_d: -66.118302 time: 8.2 s\n",
      "[55 750] loss_ae: 13.362471 loss_g: 68.167510 loss_d: -63.145003 time: 8.2 s\n",
      "[55 800] loss_ae: 13.575317 loss_g: 85.949393 loss_d: -65.964542 time: 8.1 s\n",
      "[55 850] loss_ae: 14.327461 loss_g: 75.364538 loss_d: -63.322163 time: 8.2 s\n",
      "[55 900] loss_ae: 11.515853 loss_g: 74.479514 loss_d: -59.508651 time: 8.1 s\n",
      "[55 950] loss_ae: 11.492875 loss_g: 64.999894 loss_d: -52.324930 time: 8.2 s\n",
      "[55 1000] loss_ae: 12.699244 loss_g: 79.921130 loss_d: -64.993727 time: 8.2 s\n",
      "[55 1050] loss_ae: 11.800592 loss_g: 81.255914 loss_d: -63.733956 time: 8.2 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55 1100] loss_ae: 13.647359 loss_g: 76.044138 loss_d: -58.373056 time: 8.2 s\n",
      "[55 1150] loss_ae: 14.747439 loss_g: 88.668172 loss_d: -72.303770 time: 8.1 s\n",
      "[55 1200] loss_ae: 13.302446 loss_g: 77.852413 loss_d: -61.663929 time: 8.1 s\n",
      "[55 1250] loss_ae: 13.973475 loss_g: 61.796522 loss_d: -49.698402 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  they do that there ' s no way they can do that they can do that they can do that they can do that they can do that </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  there are a lot of people that are in the in the in the </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  oh yeah you ' re right there </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  they get the death penalty for the death penalty for the death penalty </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  the only thing that i ' ve read about the other thing is that i have to say i ' m a <unk> and i </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  oh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  you know you have to have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  you know and they just they just they just they just love it and they just they just love it and they just love it and it ' </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  they have a lot of <unk> and they have a </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and you know the people in the country that are in the same place and they ' re not going to be in the middle of </s>\n",
      "true response:  but they </s>\n",
      "generate response:  if they do that they ' ll do that they ' ll do that they ' ll do that they ' ll do that they ' ll do that </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  or do it hum </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know just to get to get a little more of a <unk> <unk> than you </s>\n",
      "true response:  and </s>\n",
      "generate response:  i don ' t have to have to have a <unk> to have a <unk> you know they have a <unk> they have a <unk> </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  oh we have we have a lot of snow in the mountains and the mountains and the mountains and the mountains and the mountains and the mountains and </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  oh </s>\n",
      "BLEU1 0.440612, BLEU2 0.356688, BLEU3 0.300646, BLEU4 0.241428, inter_dist1 0.006771, inter_dist2 0.044693 avg_len 15.548623\n",
      " time: 144.3 s\n",
      "Done testing\n",
      "Epoch:  56\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[56 50] loss_ae: 12.787183 loss_g: 79.751142 loss_d: -71.791890 time: 8.1 s\n",
      "[56 100] loss_ae: 12.934755 loss_g: 82.861508 loss_d: -65.939737 time: 8.3 s\n",
      "[56 150] loss_ae: 13.601249 loss_g: 77.037027 loss_d: -65.343212 time: 8.2 s\n",
      "[56 200] loss_ae: 12.167785 loss_g: 73.791896 loss_d: -55.208247 time: 8.2 s\n",
      "[56 250] loss_ae: 14.551815 loss_g: 72.567057 loss_d: -59.362709 time: 8.2 s\n",
      "[56 300] loss_ae: 12.651768 loss_g: 72.764610 loss_d: -53.986328 time: 8.1 s\n",
      "[56 350] loss_ae: 13.975415 loss_g: 78.951820 loss_d: -62.765683 time: 8.3 s\n",
      "[56 400] loss_ae: 11.468182 loss_g: 71.903241 loss_d: -64.078043 time: 8.1 s\n",
      "[56 450] loss_ae: 11.995236 loss_g: 75.325935 loss_d: -56.065994 time: 8.0 s\n",
      "[56 500] loss_ae: 11.484842 loss_g: 77.127993 loss_d: -61.498661 time: 8.2 s\n",
      "[56 550] loss_ae: 13.514147 loss_g: 84.016066 loss_d: -65.886723 time: 8.3 s\n",
      "[56 600] loss_ae: 12.835948 loss_g: 74.931846 loss_d: -58.603557 time: 8.1 s\n",
      "[56 650] loss_ae: 13.637326 loss_g: 82.912999 loss_d: -74.627759 time: 8.2 s\n",
      "[56 700] loss_ae: 13.207489 loss_g: 66.932674 loss_d: -52.815990 time: 8.2 s\n",
      "[56 750] loss_ae: 12.389881 loss_g: 80.855152 loss_d: -63.898141 time: 8.1 s\n",
      "[56 800] loss_ae: 12.896437 loss_g: 69.887775 loss_d: -53.419123 time: 8.3 s\n",
      "[56 850] loss_ae: 15.123265 loss_g: 81.318043 loss_d: -64.618845 time: 8.1 s\n",
      "[56 900] loss_ae: 14.902914 loss_g: 90.894535 loss_d: -76.369744 time: 8.2 s\n",
      "[56 950] loss_ae: 12.858724 loss_g: 74.795244 loss_d: -56.281356 time: 8.2 s\n",
      "[56 1000] loss_ae: 11.778863 loss_g: 70.306042 loss_d: -56.663887 time: 8.1 s\n",
      "[56 1050] loss_ae: 13.667777 loss_g: 73.274620 loss_d: -57.534789 time: 8.1 s\n",
      "[56 1100] loss_ae: 12.539135 loss_g: 72.200315 loss_d: -59.286746 time: 8.3 s\n",
      "[56 1150] loss_ae: 13.697387 loss_g: 80.496134 loss_d: -56.756177 time: 8.2 s\n",
      "[56 1200] loss_ae: 13.037218 loss_g: 64.987089 loss_d: -52.019013 time: 8.2 s\n",
      "[56 1250] loss_ae: 12.456323 loss_g: 67.298183 loss_d: -50.489072 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  they do but they do have the same thing that ' s the only thing that i ' ve ever done is i ' ve </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  don ' t you don ' t you don ' t have to have a you have to have a you have a </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  i know </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  in a <unk> and they have to have a <unk> <unk> and they have to have a </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  oh they ' ll they ' ll do that i don ' t know if they ' ll ever get a gun or a <unk> or something </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  don ' t do you </s>\n",
      "true response:  oh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  or they would be better if they were to do that they would </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  do you do you do you like to do that you do you do you do you like to do you do you do you like to do it or\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  you know you can have you can have a you have a you have a you have a you have a you have a </s>\n",
      "true response:  but they </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  to you know they have to have a <unk> they have to have a <unk> they have to have a </s>\n",
      "true response:  and </s>\n",
      "generate response:  you know there is a lot of people that are in the in the in the in the in the in the in the in the </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  yeah that does it </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  i don ' t know </s>\n",
      "BLEU1 0.448081, BLEU2 0.364881, BLEU3 0.308466, BLEU4 0.248118, inter_dist1 0.006822, inter_dist2 0.043036 avg_len 15.671410\n",
      " time: 145.1 s\n",
      "Done testing\n",
      "Epoch:  57\n",
      "Train begins with 6398 batches with 12 left over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57 50] loss_ae: 12.223073 loss_g: 79.790001 loss_d: -62.076813 time: 8.2 s\n",
      "[57 100] loss_ae: 11.457471 loss_g: 75.727042 loss_d: -55.834300 time: 8.2 s\n",
      "[57 150] loss_ae: 12.895528 loss_g: 69.229018 loss_d: -46.253623 time: 8.2 s\n",
      "[57 200] loss_ae: 11.770202 loss_g: 87.299253 loss_d: -71.904751 time: 8.2 s\n",
      "[57 250] loss_ae: 12.928410 loss_g: 75.082807 loss_d: -56.548256 time: 8.1 s\n",
      "[57 300] loss_ae: 10.846453 loss_g: 73.676974 loss_d: -59.564926 time: 8.2 s\n",
      "[57 350] loss_ae: 11.555902 loss_g: 67.789981 loss_d: -61.378743 time: 8.1 s\n",
      "[57 400] loss_ae: 13.858761 loss_g: 77.773745 loss_d: -66.424566 time: 8.1 s\n",
      "[57 450] loss_ae: 13.860472 loss_g: 83.544292 loss_d: -61.742488 time: 8.2 s\n",
      "[57 500] loss_ae: 13.333420 loss_g: 78.807991 loss_d: -69.921781 time: 8.1 s\n",
      "[57 550] loss_ae: 12.168274 loss_g: 76.900504 loss_d: -59.951826 time: 8.0 s\n",
      "[57 600] loss_ae: 14.801496 loss_g: 73.985469 loss_d: -63.641567 time: 8.1 s\n",
      "[57 650] loss_ae: 13.518975 loss_g: 77.123277 loss_d: -64.433099 time: 8.3 s\n",
      "[57 700] loss_ae: 11.332537 loss_g: 80.942879 loss_d: -62.559998 time: 8.1 s\n",
      "[57 750] loss_ae: 13.930121 loss_g: 73.531173 loss_d: -60.243644 time: 8.3 s\n",
      "[57 800] loss_ae: 13.440868 loss_g: 84.064670 loss_d: -65.664759 time: 8.1 s\n",
      "[57 850] loss_ae: 13.485243 loss_g: 79.987189 loss_d: -62.316586 time: 8.1 s\n",
      "[57 900] loss_ae: 11.956033 loss_g: 81.155064 loss_d: -67.927557 time: 8.2 s\n",
      "[57 950] loss_ae: 12.595544 loss_g: 88.277431 loss_d: -66.851018 time: 8.1 s\n",
      "[57 1000] loss_ae: 12.243845 loss_g: 83.505861 loss_d: -68.224950 time: 8.2 s\n",
      "[57 1050] loss_ae: 13.549510 loss_g: 75.270432 loss_d: -56.992090 time: 8.3 s\n",
      "[57 1100] loss_ae: 11.351700 loss_g: 84.244723 loss_d: -68.839990 time: 8.2 s\n",
      "[57 1150] loss_ae: 12.063198 loss_g: 72.444601 loss_d: -50.040846 time: 8.3 s\n",
      "[57 1200] loss_ae: 13.673142 loss_g: 76.494376 loss_d: -58.372756 time: 8.1 s\n",
      "[57 1250] loss_ae: 13.315305 loss_g: 73.522209 loss_d: -63.312886 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and so that he has a lot of <unk> and i ' m not sure that he ' s been doing that for a long time </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  but most of the time </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  it </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and you know you can ' t you can ' t you can ' t you can ' t you can ' t you can ' t </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  he has to have a <unk> and he ' s a <unk> and he ' s a <unk> and he </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  it really is nice to go to the beach and go out and play and you know go out and play and you know go out </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and you know there ' s a lot of things that are really good </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  right </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  you know they have you have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  but they </s>\n",
      "generate response:  if it ' s if they do they do they do they do they do they do it they do they do it they do they do it they do\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know they can you can you can you can you can you can you can you can you can you can you can you can you can </s>\n",
      "true response:  and </s>\n",
      "generate response:  you know <unk> so </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  that ' s not too bad i mean </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  i mean it ' s not really not really that bad but it ' s not really that bad but it ' </s>\n",
      "BLEU1 0.437932, BLEU2 0.353244, BLEU3 0.296946, BLEU4 0.238142, inter_dist1 0.007665, inter_dist2 0.051332 avg_len 14.471994\n",
      " time: 145.0 s\n",
      "Done testing\n",
      "Epoch:  58\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[58 50] loss_ae: 13.852556 loss_g: 74.582822 loss_d: -58.239253 time: 8.4 s\n",
      "[58 100] loss_ae: 12.463705 loss_g: 66.992334 loss_d: -55.247119 time: 8.1 s\n",
      "[58 150] loss_ae: 12.480697 loss_g: 74.810129 loss_d: -64.155597 time: 8.3 s\n",
      "[58 200] loss_ae: 11.992730 loss_g: 75.106579 loss_d: -50.384713 time: 8.2 s\n",
      "[58 250] loss_ae: 12.438958 loss_g: 74.763832 loss_d: -58.791180 time: 8.1 s\n",
      "[58 300] loss_ae: 11.920381 loss_g: 71.740516 loss_d: -66.122767 time: 8.2 s\n",
      "[58 350] loss_ae: 11.227524 loss_g: 72.038104 loss_d: -58.898967 time: 8.3 s\n",
      "[58 400] loss_ae: 12.134335 loss_g: 68.121040 loss_d: -51.233790 time: 8.3 s\n",
      "[58 450] loss_ae: 12.574726 loss_g: 73.599990 loss_d: -57.449246 time: 8.2 s\n",
      "[58 500] loss_ae: 11.716372 loss_g: 68.381301 loss_d: -58.217891 time: 8.2 s\n",
      "[58 550] loss_ae: 14.573785 loss_g: 73.357406 loss_d: -63.345389 time: 8.1 s\n",
      "[58 600] loss_ae: 13.683421 loss_g: 81.640749 loss_d: -62.667992 time: 8.2 s\n",
      "[58 650] loss_ae: 12.158678 loss_g: 72.715824 loss_d: -63.542169 time: 8.0 s\n",
      "[58 700] loss_ae: 11.519291 loss_g: 76.042505 loss_d: -62.426076 time: 8.2 s\n",
      "[58 750] loss_ae: 12.015497 loss_g: 79.231510 loss_d: -62.018304 time: 8.2 s\n",
      "[58 800] loss_ae: 12.333241 loss_g: 79.097258 loss_d: -65.041844 time: 8.1 s\n",
      "[58 850] loss_ae: 14.135373 loss_g: 73.524536 loss_d: -61.827166 time: 8.1 s\n",
      "[58 900] loss_ae: 13.188806 loss_g: 69.913553 loss_d: -56.041985 time: 8.2 s\n",
      "[58 950] loss_ae: 12.492769 loss_g: 74.676556 loss_d: -57.797686 time: 8.1 s\n",
      "[58 1000] loss_ae: 14.854408 loss_g: 67.309412 loss_d: -53.459482 time: 8.3 s\n",
      "[58 1050] loss_ae: 12.278929 loss_g: 76.686738 loss_d: -62.702103 time: 8.4 s\n",
      "[58 1100] loss_ae: 13.366469 loss_g: 72.676922 loss_d: -61.651949 time: 7.9 s\n",
      "[58 1150] loss_ae: 12.859088 loss_g: 70.417416 loss_d: -56.818661 time: 7.9 s\n",
      "[58 1200] loss_ae: 12.887794 loss_g: 77.684439 loss_d: -60.628427 time: 7.9 s\n",
      "[58 1250] loss_ae: 14.150187 loss_g: 77.425926 loss_d: -62.161156 time: 7.8 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  they have they have they have you know they have a they have a <unk> they have a <unk> they have a <unk> they have a </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  are they really are they are they ' re not supposed to be as good as they used to be in the </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  some of those things that happen and it ' s not the people that are in the military and i ' m not sure that </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  they just they just <unk> the <unk> and they ' re not they ' re not <unk> to the <unk> </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  i </s>\n",
      "true response:  oh </s>\n",
      "generate response:  oh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and you can you can you can have a you have a <unk> you have to have a you have a you have a <unk> </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  we do have a lot of <unk> and we have a lot of <unk> and we have </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and you don ' t you don ' t have to do it you don ' t have to do it and you know you </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  uh - huh </s>\n",
      "generate response:  with those two hundred and fifty dollars a year and a half a month to a </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  they have like in the past and they ' ve been there for about twenty years and they ' ve been there for about twenty </s>\n",
      "true response:  but they </s>\n",
      "generate response:  even though it ' s just a matter of fact i ' m not sure what the what the what the what the </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  just to get to the point where you ' re going to be in the same position and you ' re going to have </s>\n",
      "true response:  and </s>\n",
      "generate response:  and it is the </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  they don ' t they don ' t have a they have a <unk> <unk> in the <unk> </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.444771, BLEU2 0.362606, BLEU3 0.306858, BLEU4 0.246921, inter_dist1 0.006909, inter_dist2 0.043052 avg_len 16.531107\n",
      " time: 159.7 s\n",
      "Done testing\n",
      "Epoch:  59\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[59 50] loss_ae: 12.770250 loss_g: 73.087841 loss_d: -60.145557 time: 8.2 s\n",
      "[59 100] loss_ae: 12.732885 loss_g: 80.448045 loss_d: -64.517809 time: 8.1 s\n",
      "[59 150] loss_ae: 12.447296 loss_g: 71.953943 loss_d: -54.615570 time: 8.2 s\n",
      "[59 200] loss_ae: 12.376695 loss_g: 80.921751 loss_d: -68.797428 time: 8.3 s\n",
      "[59 250] loss_ae: 12.926549 loss_g: 76.964022 loss_d: -64.349649 time: 8.1 s\n",
      "[59 300] loss_ae: 12.571838 loss_g: 83.400335 loss_d: -64.367959 time: 8.1 s\n",
      "[59 350] loss_ae: 13.253406 loss_g: 83.522588 loss_d: -66.703579 time: 8.3 s\n",
      "[59 400] loss_ae: 13.787481 loss_g: 73.494973 loss_d: -62.548625 time: 8.1 s\n",
      "[59 450] loss_ae: 12.742439 loss_g: 75.058334 loss_d: -66.139158 time: 8.3 s\n",
      "[59 500] loss_ae: 13.504951 loss_g: 73.568316 loss_d: -58.344803 time: 8.2 s\n",
      "[59 550] loss_ae: 13.365930 loss_g: 77.351083 loss_d: -60.138085 time: 8.1 s\n",
      "[59 600] loss_ae: 14.102675 loss_g: 68.334418 loss_d: -54.410735 time: 8.1 s\n",
      "[59 650] loss_ae: 12.287163 loss_g: 76.658175 loss_d: -63.368243 time: 8.3 s\n",
      "[59 700] loss_ae: 12.661116 loss_g: 80.354142 loss_d: -58.516774 time: 8.2 s\n",
      "[59 750] loss_ae: 13.489520 loss_g: 79.171089 loss_d: -65.425126 time: 8.3 s\n",
      "[59 800] loss_ae: 12.697278 loss_g: 74.519447 loss_d: -56.412687 time: 8.2 s\n",
      "[59 850] loss_ae: 12.361355 loss_g: 83.001071 loss_d: -61.592480 time: 8.1 s\n",
      "[59 900] loss_ae: 13.216739 loss_g: 71.098955 loss_d: -60.507023 time: 8.2 s\n",
      "[59 950] loss_ae: 13.052801 loss_g: 77.707139 loss_d: -65.075365 time: 8.4 s\n",
      "[59 1000] loss_ae: 11.539074 loss_g: 82.075252 loss_d: -63.329196 time: 8.2 s\n",
      "[59 1050] loss_ae: 13.191834 loss_g: 77.068814 loss_d: -64.958557 time: 8.2 s\n",
      "[59 1100] loss_ae: 13.532093 loss_g: 81.363574 loss_d: -64.519902 time: 8.1 s\n",
      "[59 1150] loss_ae: 11.273948 loss_g: 71.167047 loss_d: -55.360441 time: 8.3 s\n",
      "[59 1200] loss_ae: 12.041225 loss_g: 78.349698 loss_d: -63.163409 time: 8.2 s\n",
      "[59 1250] loss_ae: 14.358628 loss_g: 80.946834 loss_d: -62.278624 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  you can you can you can have <unk> <unk> you can you can you can you can you can have a <unk> you can you can </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  are you familiar with the country western country or country western or country </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  and </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and there ' s no reason to go out and do it and i don ' t know if you ' re familiar with the </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  <unk> do you </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  oh </s>\n",
      "generate response:  oh they </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  you know they have to be there and they ' ll have to go back and get it and they ' ll get a little </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  but it ' s it it ' s it it ' s it it ' s it it ' s it it ' s it it </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and you can go to a small town and you can go in and look at the you know the <unk> and the <unk> and the </s>\n",
      "true response:  but they </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  uh - huh well have you ever done a lot of <unk> and you know you don ' t you don ' t have to worry about the <unk> of\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know i mean i mean i know that i know that i was in the first grade i had a little boy i had a </s>\n",
      "true response:  and </s>\n",
      "generate response:  to do it but it doesn ' t seem like it ' s not it ' s not really </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  so do you do that there ' s a big lake there ' s </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  there ' s not there ' s not there ' s not there ' s not there ' s not there ' s not there ' s not there </s>\n",
      "BLEU1 0.438871, BLEU2 0.358029, BLEU3 0.303088, BLEU4 0.243887, inter_dist1 0.006545, inter_dist2 0.041023 avg_len 17.673052\n",
      " time: 145.9 s\n",
      "Done testing\n",
      "Epoch:  60\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[60 50] loss_ae: 13.793394 loss_g: 76.566392 loss_d: -61.443604 time: 8.3 s\n",
      "[60 100] loss_ae: 12.031667 loss_g: 84.928888 loss_d: -72.348403 time: 8.1 s\n",
      "[60 150] loss_ae: 12.357565 loss_g: 81.451906 loss_d: -63.260973 time: 8.2 s\n",
      "[60 200] loss_ae: 11.764811 loss_g: 75.494987 loss_d: -67.020097 time: 8.2 s\n",
      "[60 250] loss_ae: 11.517682 loss_g: 81.323339 loss_d: -61.580391 time: 8.2 s\n",
      "[60 300] loss_ae: 13.232665 loss_g: 83.033926 loss_d: -68.087782 time: 8.2 s\n",
      "[60 350] loss_ae: 13.880847 loss_g: 81.071483 loss_d: -63.014152 time: 8.2 s\n",
      "[60 400] loss_ae: 12.430789 loss_g: 87.242076 loss_d: -81.010774 time: 8.3 s\n",
      "[60 450] loss_ae: 13.053975 loss_g: 74.620451 loss_d: -61.994211 time: 8.2 s\n",
      "[60 500] loss_ae: 11.966634 loss_g: 65.139843 loss_d: -58.532214 time: 8.1 s\n",
      "[60 550] loss_ae: 11.592014 loss_g: 74.951250 loss_d: -57.830458 time: 8.2 s\n",
      "[60 600] loss_ae: 12.913719 loss_g: 76.091722 loss_d: -62.166236 time: 8.1 s\n",
      "[60 650] loss_ae: 12.779108 loss_g: 66.784056 loss_d: -55.558956 time: 8.1 s\n",
      "[60 700] loss_ae: 10.826164 loss_g: 78.515278 loss_d: -64.995739 time: 8.2 s\n",
      "[60 750] loss_ae: 13.221803 loss_g: 67.279338 loss_d: -55.068114 time: 8.3 s\n",
      "[60 800] loss_ae: 13.301668 loss_g: 70.669226 loss_d: -58.400102 time: 8.0 s\n",
      "[60 850] loss_ae: 11.956173 loss_g: 72.178511 loss_d: -62.813713 time: 8.0 s\n",
      "[60 900] loss_ae: 13.781149 loss_g: 67.134568 loss_d: -56.922566 time: 8.2 s\n",
      "[60 950] loss_ae: 12.292764 loss_g: 70.088097 loss_d: -59.766258 time: 8.2 s\n",
      "[60 1000] loss_ae: 14.455604 loss_g: 74.104766 loss_d: -56.990629 time: 8.1 s\n",
      "[60 1050] loss_ae: 13.716351 loss_g: 80.417143 loss_d: -69.580509 time: 8.3 s\n",
      "[60 1100] loss_ae: 13.256262 loss_g: 72.837045 loss_d: -56.872858 time: 8.3 s\n",
      "[60 1150] loss_ae: 14.046013 loss_g: 77.916037 loss_d: -66.610684 time: 8.1 s\n",
      "[60 1200] loss_ae: 11.563176 loss_g: 78.113649 loss_d: -60.803678 time: 8.5 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60 1250] loss_ae: 12.081774 loss_g: 78.154992 loss_d: -60.600941 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  not but it doesn ' t do it but it ' s not it ' s not it ' s not it ' s not a very good </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  we have to have to have a we have a we have a we have a we have a <unk> a </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  you know you have to have a you have a you have a you have a <unk> you have </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  uh - huh i ' ve heard of them i ' ve heard of them i ' ve </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  oh </s>\n",
      "generate response:  they ' re nice to meet </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  they can they can they can they can they can they can they can they can they can they can they can they can they can they can they can\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah they </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  yeah he ' ll do that he ' ll be able to do it and he ' ll just go out and he ' ll </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  right </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and you have to have you have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  but they </s>\n",
      "generate response:  i don ' t know </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  uh - huh we have we have we have a we have a we have a we have a we have a we have a we have a </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know i have a sister - in - law and i have a sister - in - law who ' s a </s>\n",
      "true response:  and </s>\n",
      "generate response:  but </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  no uh - huh it ' s just it ' s just a matter of fact i ' ve got a friend of mine </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  uh - huh </s>\n",
      "BLEU1 0.431482, BLEU2 0.351312, BLEU3 0.296080, BLEU4 0.237611, inter_dist1 0.006783, inter_dist2 0.044152 avg_len 15.519431\n",
      " time: 143.8 s\n",
      "Done testing\n",
      "Epoch:  61\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[61 50] loss_ae: 12.412833 loss_g: 75.822194 loss_d: -64.666038 time: 8.2 s\n",
      "[61 100] loss_ae: 12.666625 loss_g: 73.947981 loss_d: -59.652696 time: 8.2 s\n",
      "[61 150] loss_ae: 14.039486 loss_g: 68.386487 loss_d: -57.401759 time: 8.2 s\n",
      "[61 200] loss_ae: 13.746840 loss_g: 79.502631 loss_d: -66.233612 time: 8.1 s\n",
      "[61 250] loss_ae: 12.272416 loss_g: 78.450314 loss_d: -57.214479 time: 8.2 s\n",
      "[61 300] loss_ae: 13.445983 loss_g: 76.197509 loss_d: -59.172759 time: 8.2 s\n",
      "[61 350] loss_ae: 11.998398 loss_g: 80.709665 loss_d: -62.082984 time: 8.2 s\n",
      "[61 400] loss_ae: 11.751654 loss_g: 71.224860 loss_d: -57.449081 time: 8.2 s\n",
      "[61 450] loss_ae: 12.626481 loss_g: 75.011463 loss_d: -59.864064 time: 8.2 s\n",
      "[61 500] loss_ae: 12.797692 loss_g: 74.687323 loss_d: -64.783102 time: 8.2 s\n",
      "[61 550] loss_ae: 15.397898 loss_g: 85.567740 loss_d: -68.492631 time: 8.2 s\n",
      "[61 600] loss_ae: 10.888141 loss_g: 81.395079 loss_d: -67.812551 time: 8.2 s\n",
      "[61 650] loss_ae: 14.304514 loss_g: 83.097869 loss_d: -69.599871 time: 8.2 s\n",
      "[61 700] loss_ae: 13.890592 loss_g: 65.698677 loss_d: -52.476049 time: 8.4 s\n",
      "[61 750] loss_ae: 12.273350 loss_g: 62.475494 loss_d: -54.072645 time: 8.1 s\n",
      "[61 800] loss_ae: 12.016127 loss_g: 80.040230 loss_d: -66.966090 time: 8.0 s\n",
      "[61 850] loss_ae: 13.535590 loss_g: 77.805072 loss_d: -62.452401 time: 8.2 s\n",
      "[61 900] loss_ae: 14.251499 loss_g: 84.646745 loss_d: -65.193839 time: 8.2 s\n",
      "[61 950] loss_ae: 12.855043 loss_g: 80.894395 loss_d: -60.207147 time: 8.1 s\n",
      "[61 1000] loss_ae: 12.786186 loss_g: 74.507466 loss_d: -64.642983 time: 8.2 s\n",
      "[61 1050] loss_ae: 11.759992 loss_g: 83.664425 loss_d: -63.482496 time: 8.2 s\n",
      "[61 1100] loss_ae: 12.983680 loss_g: 72.194692 loss_d: -55.132780 time: 8.2 s\n",
      "[61 1150] loss_ae: 12.589694 loss_g: 79.370810 loss_d: -63.514837 time: 8.2 s\n",
      "[61 1200] loss_ae: 13.886632 loss_g: 78.634150 loss_d: -66.518286 time: 8.3 s\n",
      "[61 1250] loss_ae: 12.029239 loss_g: 67.748600 loss_d: -53.284444 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and you know a lot of people have you know a lot of people have a lot of people have a lot of people in the </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  and you know i like the idea of having a lot of <unk> and i think that ' s a good idea </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  or with somebody else ' s going to be a <unk> and i think that ' s a big problem i think they ' re </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  i just don ' t have a whole lot of <unk> i don ' t know if you ' ve heard of it or not </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  oh you like the <unk> uh - huh yeah </s>\n",
      "true response:  oh </s>\n",
      "generate response:  and we ' re </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  they have a they have a they have a they have a they have a they have a <unk> they have a <unk> they have a </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  you do </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  i don ' t you don ' t have to have to have a <unk> you know you have to have a <unk> you know you </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  but you know there ' s a lot of things that we can ' t do that i mean i ' m not sure that we </s>\n",
      "true response:  but they </s>\n",
      "generate response:  <unk> </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah yeah i ' ve i ' ve you have you have you have you have you have you have you have you have you have you have you </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you have to have a you have a you have a you have a you have a you have a you have a <unk> </s>\n",
      "true response:  and </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  it doesn ' t seem to do anything to do it but it ' s not too bad </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  yeah </s>\n",
      "BLEU1 0.452632, BLEU2 0.371126, BLEU3 0.314832, BLEU4 0.253672, inter_dist1 0.006273, inter_dist2 0.038109 avg_len 16.839080\n",
      " time: 144.6 s\n",
      "Done testing\n",
      "Epoch:  62\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[62 50] loss_ae: 12.531452 loss_g: 79.682534 loss_d: -57.820227 time: 8.2 s\n",
      "[62 100] loss_ae: 12.920811 loss_g: 78.309814 loss_d: -75.339580 time: 8.2 s\n",
      "[62 150] loss_ae: 12.585467 loss_g: 92.244779 loss_d: -65.331511 time: 8.1 s\n",
      "[62 200] loss_ae: 11.239191 loss_g: 78.899299 loss_d: -64.544210 time: 8.2 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62 250] loss_ae: 12.051957 loss_g: 70.859047 loss_d: -53.304384 time: 8.2 s\n",
      "[62 300] loss_ae: 11.251465 loss_g: 69.772708 loss_d: -55.981528 time: 8.2 s\n",
      "[62 350] loss_ae: 11.992822 loss_g: 67.894737 loss_d: -57.569265 time: 8.3 s\n",
      "[62 400] loss_ae: 13.053799 loss_g: 71.124266 loss_d: -70.531081 time: 8.2 s\n",
      "[62 450] loss_ae: 11.619828 loss_g: 70.544205 loss_d: -59.296688 time: 8.2 s\n",
      "[62 500] loss_ae: 13.439958 loss_g: 70.728218 loss_d: -56.198434 time: 8.2 s\n",
      "[62 550] loss_ae: 11.276034 loss_g: 78.389010 loss_d: -57.740609 time: 8.3 s\n",
      "[62 600] loss_ae: 13.195666 loss_g: 72.123023 loss_d: -55.883611 time: 8.3 s\n",
      "[62 650] loss_ae: 14.200044 loss_g: 74.670223 loss_d: -63.546931 time: 8.3 s\n",
      "[62 700] loss_ae: 13.135586 loss_g: 65.070586 loss_d: -52.474919 time: 8.1 s\n",
      "[62 750] loss_ae: 12.069705 loss_g: 73.214118 loss_d: -66.098064 time: 8.2 s\n",
      "[62 800] loss_ae: 11.623887 loss_g: 74.244401 loss_d: -60.576637 time: 8.1 s\n",
      "[62 850] loss_ae: 13.888992 loss_g: 87.184770 loss_d: -74.288508 time: 8.1 s\n",
      "[62 900] loss_ae: 12.510018 loss_g: 75.877638 loss_d: -62.538981 time: 8.3 s\n",
      "[62 950] loss_ae: 13.591798 loss_g: 80.521999 loss_d: -69.053157 time: 8.1 s\n",
      "[62 1000] loss_ae: 12.721020 loss_g: 74.128389 loss_d: -58.475710 time: 8.0 s\n",
      "[62 1050] loss_ae: 13.543783 loss_g: 73.232933 loss_d: -59.671827 time: 8.3 s\n",
      "[62 1100] loss_ae: 11.023913 loss_g: 80.021984 loss_d: -65.258454 time: 8.2 s\n",
      "[62 1150] loss_ae: 12.033779 loss_g: 74.558501 loss_d: -58.009593 time: 8.0 s\n",
      "[62 1200] loss_ae: 13.902760 loss_g: 79.589321 loss_d: -63.679772 time: 8.3 s\n",
      "[62 1250] loss_ae: 11.906977 loss_g: 74.531526 loss_d: -58.699528 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  you have you have you have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  and you can do a lot of things with the little bit of things and you know what </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  you know they ' re they have a they have a <unk> they have a <unk> they have a <unk> they have a </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and they have to have to have a <unk> they have to have a <unk> they have to have a <unk> they have to have a </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  they have they have you have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  yeah well that ' s what do you do </s>\n",
      "true response:  oh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and but i just i just love to go to the beach and the <unk> and the <unk> and the <unk> and the <unk> and the <unk> </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  they have they have a they have a they have a they have a they have a they have a they have a they have a </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  they just they just they just they just they just they just they just they just they just they just they just <unk> and they ' re just they '\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and you know we have we have a lot of people who are in the military and i don ' t know if you ' </s>\n",
      "true response:  but they </s>\n",
      "generate response:  oh yeah </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  oh that ' s nice </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and you have to you have to have a you have a you have a you have a you have a you have a <unk> </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  yeah well it ' s not too bad it ' s not too bad because you have a lot of rain and you know you can get a lot </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  if she doesn ' t make it a little bit more </s>\n",
      "BLEU1 0.428427, BLEU2 0.349717, BLEU3 0.295953, BLEU4 0.238124, inter_dist1 0.006120, inter_dist2 0.038470 avg_len 17.172414\n",
      " time: 144.8 s\n",
      "Done testing\n",
      "Epoch:  63\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[63 50] loss_ae: 9.683833 loss_g: 65.076947 loss_d: -55.379437 time: 8.1 s\n",
      "[63 100] loss_ae: 12.536852 loss_g: 70.253546 loss_d: -57.675269 time: 8.2 s\n",
      "[63 150] loss_ae: 12.123104 loss_g: 76.038738 loss_d: -61.265098 time: 8.2 s\n",
      "[63 200] loss_ae: 12.090601 loss_g: 64.574216 loss_d: -54.435027 time: 8.2 s\n",
      "[63 250] loss_ae: 11.675896 loss_g: 70.196185 loss_d: -55.525985 time: 8.3 s\n",
      "[63 300] loss_ae: 12.918085 loss_g: 73.522712 loss_d: -56.645997 time: 8.2 s\n",
      "[63 350] loss_ae: 11.119261 loss_g: 63.030968 loss_d: -48.694413 time: 8.3 s\n",
      "[63 400] loss_ae: 13.694528 loss_g: 81.682121 loss_d: -61.977174 time: 8.2 s\n",
      "[63 450] loss_ae: 13.326234 loss_g: 64.906400 loss_d: -52.444904 time: 8.2 s\n",
      "[63 500] loss_ae: 13.799601 loss_g: 78.630883 loss_d: -65.666590 time: 8.2 s\n",
      "[63 550] loss_ae: 12.524143 loss_g: 64.258779 loss_d: -49.047287 time: 8.2 s\n",
      "[63 600] loss_ae: 12.329272 loss_g: 76.473462 loss_d: -63.844469 time: 8.2 s\n",
      "[63 650] loss_ae: 12.779852 loss_g: 65.857530 loss_d: -53.487426 time: 8.1 s\n",
      "[63 700] loss_ae: 11.220101 loss_g: 72.169706 loss_d: -52.829668 time: 8.3 s\n",
      "[63 750] loss_ae: 12.444179 loss_g: 57.152692 loss_d: -43.966976 time: 8.2 s\n",
      "[63 800] loss_ae: 13.872659 loss_g: 71.518444 loss_d: -62.968468 time: 8.2 s\n",
      "[63 850] loss_ae: 11.575037 loss_g: 56.542187 loss_d: -36.453840 time: 8.5 s\n",
      "[63 900] loss_ae: 12.369958 loss_g: 73.733314 loss_d: -62.667872 time: 8.4 s\n",
      "[63 950] loss_ae: 11.831304 loss_g: 70.450763 loss_d: -49.652763 time: 8.0 s\n",
      "[63 1000] loss_ae: 13.949575 loss_g: 72.209597 loss_d: -59.649901 time: 7.9 s\n",
      "[63 1050] loss_ae: 13.733686 loss_g: 60.750017 loss_d: -45.535131 time: 7.8 s\n",
      "[63 1100] loss_ae: 13.423224 loss_g: 71.465473 loss_d: -57.810901 time: 8.0 s\n",
      "[63 1150] loss_ae: 13.739807 loss_g: 74.150617 loss_d: -55.603857 time: 7.8 s\n",
      "[63 1200] loss_ae: 13.150188 loss_g: 70.417883 loss_d: -63.253099 time: 7.9 s\n",
      "[63 1250] loss_ae: 12.952077 loss_g: 68.165652 loss_d: -58.584664 time: 8.0 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and yeah and it ' s not a good idea to me that ' s a good </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  and but they do have a lot of things that i like to do and i have a lot of <unk> </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  and </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  they they just they just don ' t want to be <unk> and i think that ' s the way it is i think </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  have <unk> and they have a they have a <unk> <unk> they have a <unk> they have a <unk> <unk> they have a <unk> </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  i really don ' t </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  you know they just they just they ' re not going to be the ones that you know are going to be the ones that are </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and you know they just they just they just they just love it and they just love it and </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh they have they have you have you have you have you have you have you have you have you have you have you have you </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  with you know we ' ve got a lot of people that are in the in the in the in the in the in the in the </s>\n",
      "true response:  but they </s>\n",
      "generate response:  you know </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  um - huh </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and we have a lot of people that have a lot of kids in the family and we ' re </s>\n",
      "true response:  and </s>\n",
      "generate response:  oh it ' s not too bad i mean you know the </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  yeah that ' s true that ' s true that ' s true i ' ve never heard of that i ' ve never been there </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  you would be really </s>\n",
      "BLEU1 0.456618, BLEU2 0.374027, BLEU3 0.317326, BLEU4 0.255620, inter_dist1 0.006524, inter_dist2 0.042430 avg_len 16.948185\n",
      " time: 157.5 s\n",
      "Done testing\n",
      "Epoch:  64\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[64 50] loss_ae: 12.042223 loss_g: 77.222317 loss_d: -68.700788 time: 8.1 s\n",
      "[64 100] loss_ae: 12.169305 loss_g: 77.585717 loss_d: -67.266299 time: 8.2 s\n",
      "[64 150] loss_ae: 11.805988 loss_g: 74.161525 loss_d: -63.910356 time: 8.2 s\n",
      "[64 200] loss_ae: 12.341633 loss_g: 70.940682 loss_d: -57.978184 time: 8.2 s\n",
      "[64 250] loss_ae: 13.354373 loss_g: 71.802499 loss_d: -51.222028 time: 8.1 s\n",
      "[64 300] loss_ae: 11.904675 loss_g: 75.953522 loss_d: -57.167137 time: 8.3 s\n",
      "[64 350] loss_ae: 12.348614 loss_g: 61.005108 loss_d: -50.239903 time: 8.1 s\n",
      "[64 400] loss_ae: 14.635133 loss_g: 65.989423 loss_d: -59.635178 time: 8.3 s\n",
      "[64 450] loss_ae: 11.405986 loss_g: 61.414726 loss_d: -51.374413 time: 8.2 s\n",
      "[64 500] loss_ae: 13.337050 loss_g: 62.783651 loss_d: -51.352036 time: 8.2 s\n",
      "[64 550] loss_ae: 11.505562 loss_g: 73.003185 loss_d: -60.156247 time: 8.3 s\n",
      "[64 600] loss_ae: 13.498654 loss_g: 79.577756 loss_d: -62.663317 time: 8.3 s\n",
      "[64 650] loss_ae: 12.043454 loss_g: 58.767621 loss_d: -51.203431 time: 8.3 s\n",
      "[64 700] loss_ae: 13.369039 loss_g: 67.267584 loss_d: -51.256896 time: 8.2 s\n",
      "[64 750] loss_ae: 13.436549 loss_g: 73.453994 loss_d: -67.298045 time: 8.3 s\n",
      "[64 800] loss_ae: 13.968848 loss_g: 61.281306 loss_d: -46.956992 time: 8.3 s\n",
      "[64 850] loss_ae: 12.214507 loss_g: 69.038464 loss_d: -60.617936 time: 8.1 s\n",
      "[64 900] loss_ae: 12.125965 loss_g: 64.138985 loss_d: -50.589383 time: 8.2 s\n",
      "[64 950] loss_ae: 12.519679 loss_g: 61.600063 loss_d: -51.125499 time: 8.0 s\n",
      "[64 1000] loss_ae: 12.055367 loss_g: 70.387684 loss_d: -56.221832 time: 8.3 s\n",
      "[64 1050] loss_ae: 11.296784 loss_g: 74.801500 loss_d: -53.748110 time: 8.1 s\n",
      "[64 1100] loss_ae: 12.613205 loss_g: 71.554751 loss_d: -56.231884 time: 8.2 s\n",
      "[64 1150] loss_ae: 12.307754 loss_g: 72.082224 loss_d: -60.332002 time: 8.2 s\n",
      "[64 1200] loss_ae: 13.325654 loss_g: 63.977606 loss_d: -49.421737 time: 8.2 s\n",
      "[64 1250] loss_ae: 12.682014 loss_g: 58.077319 loss_d: -50.916514 time: 8.3 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  they have you have you have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  they say that the <unk> are really </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  have they have you have you have you have you have you have you have you have you have you have you have you have you have you </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and you know with the people with the people that are in the military and the people in the world and they ' re </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  uh - huh no he ' s always been in the same time he ' s </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  oh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and you can you can you can you can you can you can you can you can you can you can you can you can you can you can </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  you know they just they just they just they just love it and everything </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  we had had we had a had a <unk> in the in the in the in the in the in the in the in the in the in the in\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  if they do not have the time to go to school and go to school and go to school and go to school and go to </s>\n",
      "true response:  but they </s>\n",
      "generate response:  you know you can have a you have a you have a you have a you have a you have a you have a </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know have you ever been to a day care center or something </s>\n",
      "true response:  and </s>\n",
      "generate response:  you have to have you have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  yeah oh huh </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  they ' ll you know they ' ll </s>\n",
      "BLEU1 0.431428, BLEU2 0.351061, BLEU3 0.295543, BLEU4 0.236875, inter_dist1 0.006597, inter_dist2 0.041611 avg_len 15.929028\n",
      " time: 145.5 s\n",
      "Done testing\n",
      "Epoch:  65\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[65 50] loss_ae: 12.256543 loss_g: 70.068827 loss_d: -53.182480 time: 8.3 s\n",
      "[65 100] loss_ae: 12.697127 loss_g: 64.994741 loss_d: -57.333039 time: 8.0 s\n",
      "[65 150] loss_ae: 11.540684 loss_g: 63.819038 loss_d: -53.717642 time: 8.2 s\n",
      "[65 200] loss_ae: 13.630872 loss_g: 64.659903 loss_d: -52.208536 time: 8.2 s\n",
      "[65 250] loss_ae: 12.523539 loss_g: 73.323127 loss_d: -52.716510 time: 8.2 s\n",
      "[65 300] loss_ae: 13.282588 loss_g: 65.826116 loss_d: -49.289564 time: 8.2 s\n",
      "[65 350] loss_ae: 13.018536 loss_g: 67.832357 loss_d: -53.573525 time: 8.1 s\n",
      "[65 400] loss_ae: 14.951292 loss_g: 79.935630 loss_d: -62.403234 time: 8.2 s\n",
      "[65 450] loss_ae: 12.942991 loss_g: 73.513927 loss_d: -54.353554 time: 8.2 s\n",
      "[65 500] loss_ae: 12.845605 loss_g: 71.059265 loss_d: -52.930199 time: 8.1 s\n",
      "[65 550] loss_ae: 12.135660 loss_g: 70.678140 loss_d: -56.369918 time: 8.3 s\n",
      "[65 600] loss_ae: 12.863364 loss_g: 72.761038 loss_d: -55.822287 time: 8.1 s\n",
      "[65 650] loss_ae: 11.403504 loss_g: 60.046303 loss_d: -47.750109 time: 8.2 s\n",
      "[65 700] loss_ae: 11.526723 loss_g: 65.949832 loss_d: -53.990716 time: 8.2 s\n",
      "[65 750] loss_ae: 12.363574 loss_g: 75.672481 loss_d: -59.850016 time: 8.1 s\n",
      "[65 800] loss_ae: 11.242878 loss_g: 72.687125 loss_d: -56.416560 time: 8.2 s\n",
      "[65 850] loss_ae: 12.051435 loss_g: 69.440895 loss_d: -59.004990 time: 8.2 s\n",
      "[65 900] loss_ae: 13.580180 loss_g: 73.645184 loss_d: -55.750635 time: 8.3 s\n",
      "[65 950] loss_ae: 12.638233 loss_g: 69.861646 loss_d: -66.353180 time: 8.2 s\n",
      "[65 1000] loss_ae: 12.143198 loss_g: 67.573302 loss_d: -50.328128 time: 8.2 s\n",
      "[65 1050] loss_ae: 11.428654 loss_g: 76.923791 loss_d: -57.822319 time: 8.3 s\n",
      "[65 1100] loss_ae: 12.459457 loss_g: 72.305373 loss_d: -55.608389 time: 8.1 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65 1150] loss_ae: 11.924389 loss_g: 68.105197 loss_d: -53.056934 time: 8.1 s\n",
      "[65 1200] loss_ae: 13.485151 loss_g: 77.659790 loss_d: -62.825544 time: 8.2 s\n",
      "[65 1250] loss_ae: 13.002628 loss_g: 77.823783 loss_d: -59.326178 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  but it ' s not it ' s not it ' s not really it ' s not really it ' s not really a big <unk> </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  right but what ' s </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  oh do you really </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and they are trying to get a gun and they ' re not going to be able to do it you know they ' re </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  do you have any other things that you can do with it you know with the </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  oh do you do you do you like to do you like to go to the park or something like that you </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and you don ' t you don ' t have to have a <unk> or </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  and there ' s not enough to be there </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  it ' ll do not do it if they do they do they do they do it they do they do it or do you do they do it </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  oh </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  but do you do you do you do you do you do you do you do you do you do you do you do you do you do you do\n",
      "true response:  but they </s>\n",
      "generate response:  but they don ' t do anything they can do they can do it they don ' t they don ' t </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and you know </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  yeah you have to you have to have a you have a you have a you have a you have a you have a you have a </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  they have they have you have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "BLEU1 0.423534, BLEU2 0.345766, BLEU3 0.292198, BLEU4 0.234837, inter_dist1 0.006104, inter_dist2 0.037211 avg_len 16.081919\n",
      " time: 144.8 s\n",
      "Done testing\n",
      "Epoch:  66\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[66 50] loss_ae: 12.440079 loss_g: 80.421587 loss_d: -62.624367 time: 8.2 s\n",
      "[66 100] loss_ae: 13.372299 loss_g: 72.622300 loss_d: -54.036271 time: 8.3 s\n",
      "[66 150] loss_ae: 12.364754 loss_g: 75.820849 loss_d: -64.287944 time: 8.2 s\n",
      "[66 200] loss_ae: 11.882409 loss_g: 71.866474 loss_d: -50.238601 time: 8.3 s\n",
      "[66 250] loss_ae: 12.282739 loss_g: 69.953544 loss_d: -62.596737 time: 8.2 s\n",
      "[66 300] loss_ae: 12.199617 loss_g: 80.970551 loss_d: -58.202919 time: 8.3 s\n",
      "[66 350] loss_ae: 12.667464 loss_g: 68.993444 loss_d: -51.662675 time: 8.2 s\n",
      "[66 400] loss_ae: 11.977562 loss_g: 79.241897 loss_d: -66.489304 time: 8.3 s\n",
      "[66 450] loss_ae: 12.930763 loss_g: 70.717961 loss_d: -49.846984 time: 8.2 s\n",
      "[66 500] loss_ae: 11.098069 loss_g: 79.125382 loss_d: -61.901441 time: 8.1 s\n",
      "[66 550] loss_ae: 11.531659 loss_g: 63.006126 loss_d: -46.376108 time: 8.2 s\n",
      "[66 600] loss_ae: 15.779414 loss_g: 74.101685 loss_d: -59.379203 time: 8.2 s\n",
      "[66 650] loss_ae: 12.547986 loss_g: 70.072184 loss_d: -63.791904 time: 8.1 s\n",
      "[66 700] loss_ae: 14.205798 loss_g: 75.877277 loss_d: -59.128183 time: 8.2 s\n",
      "[66 750] loss_ae: 11.773715 loss_g: 75.381254 loss_d: -60.254291 time: 8.2 s\n",
      "[66 800] loss_ae: 11.659816 loss_g: 65.919911 loss_d: -49.701522 time: 8.2 s\n",
      "[66 850] loss_ae: 12.140870 loss_g: 70.319456 loss_d: -63.007562 time: 8.3 s\n",
      "[66 900] loss_ae: 12.351303 loss_g: 74.126498 loss_d: -56.336742 time: 8.2 s\n",
      "[66 950] loss_ae: 13.398726 loss_g: 70.411651 loss_d: -62.775722 time: 8.3 s\n",
      "[66 1000] loss_ae: 13.135626 loss_g: 71.304563 loss_d: -61.775849 time: 8.2 s\n",
      "[66 1050] loss_ae: 12.796194 loss_g: 67.008088 loss_d: -59.906717 time: 8.3 s\n",
      "[66 1100] loss_ae: 13.400928 loss_g: 74.287387 loss_d: -56.324466 time: 8.2 s\n",
      "[66 1150] loss_ae: 13.998455 loss_g: 77.733052 loss_d: -61.610783 time: 8.1 s\n",
      "[66 1200] loss_ae: 14.172125 loss_g: 76.944038 loss_d: -63.866986 time: 8.2 s\n",
      "[66 1250] loss_ae: 10.247842 loss_g: 74.056558 loss_d: -56.501699 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and and they don ' t they don ' t have you don ' t have to have a <unk> they don ' t have to </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  mostly we do have a lot of camping and we have a lot of camping in </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  and when they do it ' s like it ' s like you know the <unk> and the <unk> and the <unk> and the <unk> and </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  has been <unk> in the last couple of years and i ' ve been in the military and i ' ve been </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  oh that ' s interesting do you do you do you do you do you do you do you do you do you do you do you do you do\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  you know when you get to do that you don ' t have to do that and you don ' t have to worry about it </s>\n",
      "true response:  oh </s>\n",
      "generate response:  they ' ve been there </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and with a with a <unk> <unk> and a <unk> and a <unk> and a <unk> and a <unk> <unk> </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  with that ' s what i ' ve seen and i ' ve heard of it i ' ve heard that it was really good </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  sure </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  if they don ' t do it they don ' t they don ' t do it they don ' t do it they don ' t do it but\n",
      "true response:  but they </s>\n",
      "generate response:  that ' s right </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  you don ' t you don ' t you have you have you have you have you have you have you have you have you have you have you </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know if you can get a job you can get a job you can do it </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  right </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  have been real interesting because i was in the same position and i was in a in a in a in a in a in a in a </s>\n",
      "BLEU1 0.445544, BLEU2 0.363150, BLEU3 0.307374, BLEU4 0.247423, inter_dist1 0.006272, inter_dist2 0.040460 avg_len 16.376026\n",
      " time: 144.8 s\n",
      "Done testing\n",
      "Epoch:  67\n",
      "Train begins with 6398 batches with 12 left over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67 50] loss_ae: 14.896230 loss_g: 74.727844 loss_d: -62.691887 time: 8.2 s\n",
      "[67 100] loss_ae: 15.641166 loss_g: 88.205675 loss_d: -69.641473 time: 8.1 s\n",
      "[67 150] loss_ae: 12.584902 loss_g: 70.644036 loss_d: -57.236436 time: 8.2 s\n",
      "[67 200] loss_ae: 13.074956 loss_g: 69.481959 loss_d: -55.986193 time: 8.1 s\n",
      "[67 250] loss_ae: 12.481743 loss_g: 71.794504 loss_d: -58.184425 time: 8.1 s\n",
      "[67 300] loss_ae: 12.939551 loss_g: 64.198185 loss_d: -55.908227 time: 8.3 s\n",
      "[67 350] loss_ae: 11.268201 loss_g: 68.411702 loss_d: -56.809882 time: 8.2 s\n",
      "[67 400] loss_ae: 12.236228 loss_g: 71.022211 loss_d: -66.633551 time: 8.2 s\n",
      "[67 450] loss_ae: 12.446792 loss_g: 67.618921 loss_d: -52.356744 time: 8.2 s\n",
      "[67 500] loss_ae: 13.663639 loss_g: 64.998892 loss_d: -47.411358 time: 8.1 s\n",
      "[67 550] loss_ae: 12.118422 loss_g: 81.588069 loss_d: -66.486620 time: 8.2 s\n",
      "[67 600] loss_ae: 12.865875 loss_g: 70.897570 loss_d: -55.036958 time: 8.2 s\n",
      "[67 650] loss_ae: 13.366248 loss_g: 78.219786 loss_d: -57.737946 time: 8.2 s\n",
      "[67 700] loss_ae: 12.414665 loss_g: 66.566938 loss_d: -61.695426 time: 8.3 s\n",
      "[67 750] loss_ae: 12.840691 loss_g: 66.147422 loss_d: -60.531768 time: 8.3 s\n",
      "[67 800] loss_ae: 12.157266 loss_g: 65.484985 loss_d: -49.594686 time: 8.2 s\n",
      "[67 850] loss_ae: 13.730950 loss_g: 69.608827 loss_d: -53.361826 time: 8.2 s\n",
      "[67 900] loss_ae: 12.225776 loss_g: 67.157772 loss_d: -48.150816 time: 8.1 s\n",
      "[67 950] loss_ae: 14.531027 loss_g: 67.479566 loss_d: -55.216348 time: 8.3 s\n",
      "[67 1000] loss_ae: 12.881826 loss_g: 62.551431 loss_d: -49.002615 time: 8.3 s\n",
      "[67 1050] loss_ae: 12.261695 loss_g: 63.884651 loss_d: -53.361114 time: 8.1 s\n",
      "[67 1100] loss_ae: 11.795793 loss_g: 59.255420 loss_d: -53.369834 time: 8.3 s\n",
      "[67 1150] loss_ae: 12.825151 loss_g: 66.963224 loss_d: -53.951232 time: 8.1 s\n",
      "[67 1200] loss_ae: 13.753907 loss_g: 70.790119 loss_d: -55.931853 time: 8.1 s\n",
      "[67 1250] loss_ae: 11.531776 loss_g: 89.195910 loss_d: -69.051336 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and he ' ll have to you know he ' ll have to have a <unk> and he ' s a <unk> and he ' s a </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  they just have to be <unk> and stuff like that </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  you ' ll have to have you have you have you have you have you have you have you have you have you have you have you have you </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  or they can do that they can do that they can do that they can do that they </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  and you ' re not going to have to be in the car </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  and they just </s>\n",
      "true response:  oh </s>\n",
      "generate response:  oh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  to do you know like i say i ' ve been to the <unk> and i ' ve been to the point where i ' ve </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  but i don ' t know that ' s what i ' m saying i don ' t know if you ' re familiar with the </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  sure </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  they they have they have you know they have a they have a <unk> you know they have a they have a <unk> you know they </s>\n",
      "true response:  but they </s>\n",
      "generate response:  are not a <unk> you know but it ' s not </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know they have to do something to you or do you know you do it and you do it and you do it yourself </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  and you don ' t do that don ' t you do you do you do you do you do you do you do you </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  uh - huh </s>\n",
      "BLEU1 0.438475, BLEU2 0.357778, BLEU3 0.302816, BLEU4 0.243562, inter_dist1 0.006708, inter_dist2 0.042522 avg_len 16.102354\n",
      " time: 144.4 s\n",
      "Done testing\n",
      "Epoch:  68\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[68 50] loss_ae: 12.313147 loss_g: 69.695149 loss_d: -50.797336 time: 8.2 s\n",
      "[68 100] loss_ae: 11.140796 loss_g: 76.698078 loss_d: -66.023709 time: 8.3 s\n",
      "[68 150] loss_ae: 11.794160 loss_g: 72.082314 loss_d: -50.413599 time: 8.4 s\n",
      "[68 200] loss_ae: 12.860344 loss_g: 64.571346 loss_d: -53.587010 time: 8.1 s\n",
      "[68 250] loss_ae: 12.519744 loss_g: 62.398462 loss_d: -50.615465 time: 8.2 s\n",
      "[68 300] loss_ae: 12.237092 loss_g: 77.609257 loss_d: -67.110493 time: 8.1 s\n",
      "[68 350] loss_ae: 11.643651 loss_g: 61.597148 loss_d: -54.932299 time: 8.4 s\n",
      "[68 400] loss_ae: 12.169762 loss_g: 72.019334 loss_d: -66.919789 time: 8.2 s\n",
      "[68 450] loss_ae: 12.951796 loss_g: 70.493550 loss_d: -56.115198 time: 8.3 s\n",
      "[68 500] loss_ae: 13.402243 loss_g: 77.320153 loss_d: -55.897990 time: 8.2 s\n",
      "[68 550] loss_ae: 12.762896 loss_g: 76.203171 loss_d: -67.409868 time: 8.2 s\n",
      "[68 600] loss_ae: 14.153946 loss_g: 77.491279 loss_d: -62.555903 time: 8.1 s\n",
      "[68 650] loss_ae: 13.082538 loss_g: 62.020424 loss_d: -52.560313 time: 8.1 s\n",
      "[68 700] loss_ae: 12.923059 loss_g: 75.403683 loss_d: -55.685801 time: 8.5 s\n",
      "[68 750] loss_ae: 12.689767 loss_g: 73.091241 loss_d: -54.199560 time: 8.5 s\n",
      "[68 800] loss_ae: 13.314763 loss_g: 65.364220 loss_d: -55.786432 time: 7.8 s\n",
      "[68 850] loss_ae: 13.471448 loss_g: 74.823793 loss_d: -64.602824 time: 8.1 s\n",
      "[68 900] loss_ae: 11.503477 loss_g: 75.655853 loss_d: -60.217584 time: 7.9 s\n",
      "[68 950] loss_ae: 12.513064 loss_g: 82.186110 loss_d: -66.190049 time: 7.8 s\n",
      "[68 1000] loss_ae: 12.779188 loss_g: 79.565004 loss_d: -58.857971 time: 8.0 s\n",
      "[68 1050] loss_ae: 12.700693 loss_g: 86.564417 loss_d: -68.562417 time: 8.3 s\n",
      "[68 1100] loss_ae: 13.001870 loss_g: 73.556045 loss_d: -59.909744 time: 7.9 s\n",
      "[68 1150] loss_ae: 12.364467 loss_g: 75.588554 loss_d: -62.327697 time: 7.9 s\n",
      "[68 1200] loss_ae: 11.599577 loss_g: 82.047451 loss_d: -71.930533 time: 7.8 s\n",
      "[68 1250] loss_ae: 11.777002 loss_g: 80.063196 loss_d: -67.127656 time: 8.0 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  you don ' t you don ' t really do anything to do anything but you know there ' s a lot of things that </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  you know you have to have a <unk> you have to have a <unk> you have to have a <unk> you have to </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  they have a they have a lot of <unk> and they ' re just too </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and they are all for the same <unk> of their </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  we don ' t have to do anything about it i don ' t </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  you don ' t you don ' t have to have to have a <unk> you have to have a you have a you have </s>\n",
      "true response:  oh </s>\n",
      "generate response:  i don ' t know i just don ' t have a lot of <unk> i just </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  yeah what do you do you do you do you do you do you do you do you do you do you do you do you do you do you\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and that ' s what i ' ve seen and i ' ve heard of it i ' ve heard of it that ' s </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  there ' s they don ' t do they do they do they do they do they do they do they do they do they do it they do they\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  they have they have they have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  but they </s>\n",
      "generate response:  uh - huh no they have </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know when you do get to work you ' re not going to do it you know you ' re going to do it you know </s>\n",
      "true response:  and </s>\n",
      "generate response:  you know and they have a they have a <unk> they have a <unk> they have a <unk> they have a <unk> they have a </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  are you in texas where the weather is in the mountains </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  uh - huh </s>\n",
      "BLEU1 0.443096, BLEU2 0.363430, BLEU3 0.308260, BLEU4 0.248152, inter_dist1 0.006408, inter_dist2 0.040227 avg_len 16.825762\n",
      " time: 160.3 s\n",
      "Done testing\n",
      "Epoch:  69\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[69 50] loss_ae: 13.363203 loss_g: 72.324148 loss_d: -56.304031 time: 8.3 s\n",
      "[69 100] loss_ae: 12.487147 loss_g: 77.961499 loss_d: -59.474326 time: 8.3 s\n",
      "[69 150] loss_ae: 13.258550 loss_g: 76.537218 loss_d: -56.524911 time: 8.2 s\n",
      "[69 200] loss_ae: 11.787000 loss_g: 71.436180 loss_d: -61.729385 time: 8.3 s\n",
      "[69 250] loss_ae: 12.778707 loss_g: 75.636747 loss_d: -56.774692 time: 8.2 s\n",
      "[69 300] loss_ae: 12.015169 loss_g: 72.848470 loss_d: -66.069933 time: 8.3 s\n",
      "[69 350] loss_ae: 13.629900 loss_g: 72.385924 loss_d: -62.975415 time: 8.3 s\n",
      "[69 400] loss_ae: 13.389713 loss_g: 72.519007 loss_d: -62.270487 time: 8.1 s\n",
      "[69 450] loss_ae: 12.386425 loss_g: 75.681958 loss_d: -61.517275 time: 8.3 s\n",
      "[69 500] loss_ae: 12.887346 loss_g: 76.943990 loss_d: -63.305573 time: 8.1 s\n",
      "[69 550] loss_ae: 13.655240 loss_g: 78.572185 loss_d: -62.171205 time: 8.2 s\n",
      "[69 600] loss_ae: 13.127960 loss_g: 76.500887 loss_d: -67.531654 time: 8.2 s\n",
      "[69 650] loss_ae: 11.881526 loss_g: 72.776795 loss_d: -54.460424 time: 8.2 s\n",
      "[69 700] loss_ae: 13.585261 loss_g: 73.040247 loss_d: -62.071953 time: 8.3 s\n",
      "[69 750] loss_ae: 13.013928 loss_g: 75.337146 loss_d: -65.188813 time: 8.1 s\n",
      "[69 800] loss_ae: 15.486364 loss_g: 82.773814 loss_d: -70.356976 time: 8.3 s\n",
      "[69 850] loss_ae: 12.497124 loss_g: 63.702969 loss_d: -54.373049 time: 8.3 s\n",
      "[69 900] loss_ae: 13.466218 loss_g: 73.383247 loss_d: -69.692787 time: 8.3 s\n",
      "[69 950] loss_ae: 11.513113 loss_g: 80.238433 loss_d: -64.738528 time: 8.2 s\n",
      "[69 1000] loss_ae: 11.569176 loss_g: 85.730631 loss_d: -60.359375 time: 8.3 s\n",
      "[69 1050] loss_ae: 12.774646 loss_g: 69.702108 loss_d: -61.242309 time: 8.2 s\n",
      "[69 1100] loss_ae: 12.353858 loss_g: 65.665004 loss_d: -54.366305 time: 8.2 s\n",
      "[69 1150] loss_ae: 11.334091 loss_g: 71.973215 loss_d: -58.157546 time: 8.2 s\n",
      "[69 1200] loss_ae: 12.568461 loss_g: 73.758119 loss_d: -62.335936 time: 8.2 s\n",
      "[69 1250] loss_ae: 12.227357 loss_g: 77.584100 loss_d: -62.160987 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  to they have to go to a you know they have a <unk> and they have a <unk> and they have a <unk> and they have a </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  and i don ' t have any children i have a sister who lives in the house and she ' </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  right </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  they don ' t they don ' t they don ' t they don ' t they don ' t they don ' </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  sure </s>\n",
      "true response:  oh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  you know they have they have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  they do not do it but i don ' t think they ' ll do it but they don ' t do it they don ' </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and you can have you can have a you have a <unk> you have to have a you have a you have a <unk> you have </s>\n",
      "true response:  but they </s>\n",
      "generate response:  you have to have you have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  but it ' s it doesn ' t it doesn ' t it doesn ' t do it you know it ' s like it ' </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  you can you can go to <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  right </s>\n",
      "BLEU1 0.413585, BLEU2 0.337012, BLEU3 0.284344, BLEU4 0.228325, inter_dist1 0.005948, inter_dist2 0.035590 avg_len 16.840358\n",
      " time: 144.8 s\n",
      "Done testing\n",
      "Epoch:  70\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[70 50] loss_ae: 13.255342 loss_g: 73.001340 loss_d: -55.986610 time: 8.2 s\n",
      "[70 100] loss_ae: 12.834642 loss_g: 79.043053 loss_d: -63.073692 time: 8.3 s\n",
      "[70 150] loss_ae: 12.020642 loss_g: 60.370314 loss_d: -52.117838 time: 8.4 s\n",
      "[70 200] loss_ae: 12.742874 loss_g: 60.353277 loss_d: -43.047629 time: 8.3 s\n",
      "[70 250] loss_ae: 13.486791 loss_g: 69.892494 loss_d: -62.505743 time: 8.2 s\n",
      "[70 300] loss_ae: 14.119045 loss_g: 71.661910 loss_d: -48.712977 time: 8.2 s\n",
      "[70 350] loss_ae: 13.660946 loss_g: 72.806042 loss_d: -65.344050 time: 7.6 s\n",
      "[70 400] loss_ae: 13.034635 loss_g: 69.298838 loss_d: -44.357210 time: 7.8 s\n",
      "[70 450] loss_ae: 11.692647 loss_g: 70.018410 loss_d: -61.284513 time: 8.0 s\n",
      "[70 500] loss_ae: 13.970336 loss_g: 73.853408 loss_d: -59.063056 time: 7.3 s\n",
      "[70 550] loss_ae: 13.094958 loss_g: 61.924068 loss_d: -54.057574 time: 7.3 s\n",
      "[70 600] loss_ae: 12.134862 loss_g: 64.177665 loss_d: -50.498863 time: 7.5 s\n",
      "[70 650] loss_ae: 11.808280 loss_g: 76.233854 loss_d: -57.075841 time: 7.2 s\n",
      "[70 700] loss_ae: 12.530757 loss_g: 75.118400 loss_d: -64.805319 time: 7.3 s\n",
      "[70 750] loss_ae: 11.922506 loss_g: 71.366254 loss_d: -62.847152 time: 7.2 s\n",
      "[70 800] loss_ae: 13.440949 loss_g: 88.520712 loss_d: -70.271176 time: 7.1 s\n",
      "[70 850] loss_ae: 13.948242 loss_g: 73.406097 loss_d: -53.813718 time: 7.1 s\n",
      "[70 900] loss_ae: 12.759353 loss_g: 82.508130 loss_d: -62.116849 time: 7.4 s\n",
      "[70 950] loss_ae: 12.769600 loss_g: 87.124508 loss_d: -74.681113 time: 7.2 s\n",
      "[70 1000] loss_ae: 13.819340 loss_g: 87.242334 loss_d: -64.715495 time: 7.1 s\n",
      "[70 1050] loss_ae: 11.400184 loss_g: 80.447151 loss_d: -64.489136 time: 7.1 s\n",
      "[70 1100] loss_ae: 11.238064 loss_g: 66.634373 loss_d: -57.347164 time: 7.1 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70 1150] loss_ae: 12.464300 loss_g: 78.396345 loss_d: -66.578576 time: 7.1 s\n",
      "[70 1200] loss_ae: 12.712506 loss_g: 75.273353 loss_d: -63.164849 time: 7.4 s\n",
      "[70 1250] loss_ae: 13.552194 loss_g: 69.668520 loss_d: -59.320515 time: 7.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and they are they are very <unk> and they ' re very <unk> and they ' re very <unk> and they ' re very <unk> </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  you know </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  they have they have they have you have you have you have you have you have you have you have you </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and not even though they ' re not going to be in the military but they </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  it ' s not like they ' re not going to do it i mean they ' re not they ' re not they </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  oh </s>\n",
      "generate response:  if it ' s a good place to go to the beach or </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  you get to get out there and get a lot of <unk> and stuff and then you can ' t get the <unk> and </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  that ' s right </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  they have they have you know they have a they have a they have a they have a <unk> and they have a they have </s>\n",
      "true response:  but they </s>\n",
      "generate response:  just to get out of the car and they ' re not going to be </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  oh they ' ll have to you know they ' ll have to do the same thing and then they ' ll put it in the back of the </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you ' ll have to do it you don ' t you don ' t want to do it or do you do it yourself or </s>\n",
      "true response:  and </s>\n",
      "generate response:  i ' ve we have a we have a we have a we have a we have a <unk> a <unk> a <unk> <unk> <unk> </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  no there not there there there there there ' s not there ' s not there ' s not </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.441904, BLEU2 0.361832, BLEU3 0.306811, BLEU4 0.247071, inter_dist1 0.006265, inter_dist2 0.040614 avg_len 16.978471\n",
      " time: 136.2 s\n",
      "Done testing\n",
      "Epoch:  71\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[71 50] loss_ae: 12.582484 loss_g: 76.899246 loss_d: -59.501308 time: 7.3 s\n",
      "[71 100] loss_ae: 13.003122 loss_g: 76.731423 loss_d: -62.822301 time: 7.1 s\n",
      "[71 150] loss_ae: 12.820812 loss_g: 69.942994 loss_d: -55.179724 time: 7.9 s\n",
      "[71 200] loss_ae: 13.647429 loss_g: 71.814311 loss_d: -53.509727 time: 8.2 s\n",
      "[71 250] loss_ae: 12.184696 loss_g: 77.659895 loss_d: -58.660274 time: 8.2 s\n",
      "[71 300] loss_ae: 12.477445 loss_g: 73.238887 loss_d: -58.348240 time: 8.3 s\n",
      "[71 350] loss_ae: 13.480616 loss_g: 75.852176 loss_d: -58.336778 time: 8.1 s\n",
      "[71 400] loss_ae: 12.829421 loss_g: 72.219339 loss_d: -61.194341 time: 8.3 s\n",
      "[71 450] loss_ae: 13.551098 loss_g: 76.251412 loss_d: -59.456031 time: 8.3 s\n",
      "[71 500] loss_ae: 13.203990 loss_g: 77.275662 loss_d: -62.688600 time: 8.4 s\n",
      "[71 550] loss_ae: 13.925574 loss_g: 81.013956 loss_d: -63.312231 time: 8.2 s\n",
      "[71 600] loss_ae: 14.071164 loss_g: 81.916919 loss_d: -69.364619 time: 8.2 s\n",
      "[71 650] loss_ae: 11.651937 loss_g: 68.520420 loss_d: -63.853778 time: 8.1 s\n",
      "[71 700] loss_ae: 12.127022 loss_g: 72.103147 loss_d: -57.477171 time: 8.4 s\n",
      "[71 750] loss_ae: 10.938085 loss_g: 82.707587 loss_d: -63.513217 time: 8.1 s\n",
      "[71 800] loss_ae: 11.535744 loss_g: 64.801921 loss_d: -60.935110 time: 8.3 s\n",
      "[71 850] loss_ae: 12.650619 loss_g: 71.440295 loss_d: -61.132429 time: 8.1 s\n",
      "[71 900] loss_ae: 11.667262 loss_g: 70.286420 loss_d: -53.825411 time: 8.4 s\n",
      "[71 950] loss_ae: 12.303891 loss_g: 72.813931 loss_d: -59.437478 time: 8.2 s\n",
      "[71 1000] loss_ae: 12.961563 loss_g: 73.115068 loss_d: -57.526661 time: 8.4 s\n",
      "[71 1050] loss_ae: 13.124602 loss_g: 71.589690 loss_d: -58.749347 time: 8.2 s\n",
      "[71 1100] loss_ae: 12.450170 loss_g: 78.479496 loss_d: -66.604789 time: 8.3 s\n",
      "[71 1150] loss_ae: 13.194452 loss_g: 80.073136 loss_d: -65.377325 time: 8.3 s\n",
      "[71 1200] loss_ae: 11.465434 loss_g: 74.392170 loss_d: -62.458045 time: 8.2 s\n",
      "[71 1250] loss_ae: 12.864602 loss_g: 71.016924 loss_d: -60.678206 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  i don ' t know if they do they do they do they do they do it they do they do it they do they do it they do they\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  sure </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  a lot more um - hum </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  oh </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  <unk> <unk> or something like that and then you know </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  oh well that ' s nice to do </s>\n",
      "true response:  oh </s>\n",
      "generate response:  you know there ' s always something that you can ' t do that you can ' t do </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  or if they can go to the beach and go to the beach and go out and play and go to the beach and </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  it ' s not a real good movie but </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  so </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  they have to have you have you have you have you have you have you have you have you have you have you have you have you have you </s>\n",
      "true response:  but they </s>\n",
      "generate response:  it doesn ' t do that well i don ' t know if you ' ve heard of it but it ' s not it ' </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah we just have to put up a little bit of a <unk> that ' s </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  there ' s always there ' s always there ' s always there ' s always there ' s always been there for a while </s>\n",
      "true response:  and </s>\n",
      "generate response:  if you don ' t if you don ' t do it you don ' t do it you don ' t do it you don ' t </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  um - hum well there ' s been some interesting things i ' ve noticed lately i ' ve been in the past several years </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.441774, BLEU2 0.358565, BLEU3 0.301744, BLEU4 0.241984, inter_dist1 0.006944, inter_dist2 0.043931 avg_len 15.265645\n",
      " time: 145.9 s\n",
      "Done testing\n",
      "Epoch:  72\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[72 50] loss_ae: 12.927036 loss_g: 72.810303 loss_d: -60.748028 time: 8.3 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72 100] loss_ae: 13.105793 loss_g: 77.959961 loss_d: -60.964555 time: 8.3 s\n",
      "[72 150] loss_ae: 11.784356 loss_g: 69.335640 loss_d: -65.145461 time: 8.3 s\n",
      "[72 200] loss_ae: 12.134576 loss_g: 83.982528 loss_d: -69.355308 time: 8.2 s\n",
      "[72 250] loss_ae: 11.064911 loss_g: 79.223944 loss_d: -63.170125 time: 8.2 s\n",
      "[72 300] loss_ae: 12.106904 loss_g: 73.172250 loss_d: -53.511844 time: 8.1 s\n",
      "[72 350] loss_ae: 12.777995 loss_g: 80.928006 loss_d: -65.296925 time: 8.3 s\n",
      "[72 400] loss_ae: 11.363484 loss_g: 63.738096 loss_d: -53.094576 time: 8.3 s\n",
      "[72 450] loss_ae: 12.418220 loss_g: 73.848645 loss_d: -56.098549 time: 8.2 s\n",
      "[72 500] loss_ae: 13.406568 loss_g: 71.299121 loss_d: -57.746575 time: 8.3 s\n",
      "[72 550] loss_ae: 12.379763 loss_g: 75.457959 loss_d: -63.515713 time: 8.1 s\n",
      "[72 600] loss_ae: 10.829616 loss_g: 68.635186 loss_d: -55.017099 time: 8.2 s\n",
      "[72 650] loss_ae: 13.077646 loss_g: 72.349377 loss_d: -64.510824 time: 8.1 s\n",
      "[72 700] loss_ae: 12.456655 loss_g: 53.010050 loss_d: -54.080053 time: 8.1 s\n",
      "[72 750] loss_ae: 13.217995 loss_g: 75.569919 loss_d: -60.219796 time: 8.3 s\n",
      "[72 800] loss_ae: 13.527322 loss_g: 82.710809 loss_d: -64.534170 time: 8.2 s\n",
      "[72 850] loss_ae: 11.598440 loss_g: 78.685824 loss_d: -59.760068 time: 8.2 s\n",
      "[72 900] loss_ae: 13.175612 loss_g: 70.709911 loss_d: -54.704977 time: 8.4 s\n",
      "[72 950] loss_ae: 12.130588 loss_g: 75.099746 loss_d: -57.196079 time: 8.2 s\n",
      "[72 1000] loss_ae: 13.467455 loss_g: 71.069335 loss_d: -50.813605 time: 8.3 s\n",
      "[72 1050] loss_ae: 12.310637 loss_g: 70.713422 loss_d: -58.831883 time: 8.1 s\n",
      "[72 1100] loss_ae: 12.596866 loss_g: 61.634167 loss_d: -50.969391 time: 8.4 s\n",
      "[72 1150] loss_ae: 12.404497 loss_g: 69.808788 loss_d: -53.969294 time: 8.1 s\n",
      "[72 1200] loss_ae: 11.894085 loss_g: 74.090196 loss_d: -52.680365 time: 8.3 s\n",
      "[72 1250] loss_ae: 13.370485 loss_g: 67.407337 loss_d: -49.845963 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  or they are there are there are there are there are there are there are there are some are the ones that have the <unk> and they ' </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  you know with the <unk> and the <unk> and the <unk> and the <unk> and the <unk> and things like that </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  right </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and you can ' t do it you know you can ' t do it you know you don ' t </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  they have a lot of <unk> they have a lot of <unk> and they have a lot of </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  oh </s>\n",
      "generate response:  and they were just </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and you can go to you can just go to a you know <unk> <unk> and you have to have a <unk> you know </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  that they have to do and they ' re not they ' re not they ' re not they ' re not they ' re not </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  right </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  they have they have a they have a have a <unk> you have to have a you have a you have a you have a </s>\n",
      "true response:  but they </s>\n",
      "generate response:  they are they ' re they have they have they have they have a they have a they have a they have a they have a </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  sure </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  they do have a lot of <unk> and i have a lot of friends that i ' ve had to have a lot of </s>\n",
      "true response:  and </s>\n",
      "generate response:  and you don ' t you don ' t you don ' t have to you have to have a you know you have </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.449616, BLEU2 0.367761, BLEU3 0.311531, BLEU4 0.250850, inter_dist1 0.006468, inter_dist2 0.041384 avg_len 16.416712\n",
      " time: 145.2 s\n",
      "Done testing\n",
      "Epoch:  73\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[73 50] loss_ae: 14.503153 loss_g: 79.152120 loss_d: -61.397491 time: 8.1 s\n",
      "[73 100] loss_ae: 10.967032 loss_g: 75.268054 loss_d: -63.354647 time: 8.1 s\n",
      "[73 150] loss_ae: 11.082258 loss_g: 65.816493 loss_d: -52.623194 time: 8.2 s\n",
      "[73 200] loss_ae: 13.759097 loss_g: 78.982356 loss_d: -59.454845 time: 8.3 s\n",
      "[73 250] loss_ae: 13.093198 loss_g: 72.872346 loss_d: -59.743698 time: 8.3 s\n",
      "[73 300] loss_ae: 13.345996 loss_g: 66.949985 loss_d: -61.843742 time: 8.2 s\n",
      "[73 350] loss_ae: 14.533235 loss_g: 82.096383 loss_d: -62.520988 time: 8.3 s\n",
      "[73 400] loss_ae: 12.905560 loss_g: 68.980570 loss_d: -62.496109 time: 8.2 s\n",
      "[73 450] loss_ae: 11.955399 loss_g: 70.145742 loss_d: -56.312678 time: 8.2 s\n",
      "[73 500] loss_ae: 12.998174 loss_g: 70.086958 loss_d: -54.329312 time: 8.2 s\n",
      "[73 550] loss_ae: 11.325465 loss_g: 72.731846 loss_d: -61.905149 time: 8.3 s\n",
      "[73 600] loss_ae: 12.007646 loss_g: 68.332323 loss_d: -58.201643 time: 8.1 s\n",
      "[73 650] loss_ae: 13.809106 loss_g: 71.230304 loss_d: -55.138359 time: 8.2 s\n",
      "[73 700] loss_ae: 12.778993 loss_g: 64.693311 loss_d: -58.062857 time: 8.3 s\n",
      "[73 750] loss_ae: 13.218797 loss_g: 73.645216 loss_d: -60.781907 time: 8.2 s\n",
      "[73 800] loss_ae: 12.909513 loss_g: 67.595166 loss_d: -44.781166 time: 8.3 s\n",
      "[73 850] loss_ae: 13.813541 loss_g: 58.819886 loss_d: -46.968925 time: 8.1 s\n",
      "[73 900] loss_ae: 12.388396 loss_g: 64.202650 loss_d: -43.581761 time: 8.3 s\n",
      "[73 950] loss_ae: 13.143246 loss_g: 61.213500 loss_d: -51.443900 time: 8.2 s\n",
      "[73 1000] loss_ae: 12.699048 loss_g: 69.405136 loss_d: -57.185489 time: 8.3 s\n",
      "[73 1050] loss_ae: 13.308943 loss_g: 78.335320 loss_d: -58.519231 time: 8.3 s\n",
      "[73 1100] loss_ae: 11.991755 loss_g: 73.536215 loss_d: -62.781582 time: 8.3 s\n",
      "[73 1150] loss_ae: 13.290461 loss_g: 73.307359 loss_d: -48.962123 time: 8.1 s\n",
      "[73 1200] loss_ae: 13.273246 loss_g: 73.112641 loss_d: -59.354294 time: 8.3 s\n",
      "[73 1250] loss_ae: 13.208167 loss_g: 61.465458 loss_d: -49.996462 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  you know they do not have any children but </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  in fact i ' ve been to the <unk> for several years and i ' ve been </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  right but there ' s a lot of people that are in there and i think that ' s a good idea i think it ' </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  they just they just they just don ' t have to worry about it i mean i don ' t think it ' s a </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  they have no idea what </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  oh </s>\n",
      "generate response:  oh you have you have you have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and you know there ' s a lot of places that go to the beach and go out and play a lot of the </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and you can ' t do anything you can do it you can ' t do it you know </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh and they </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  but they do have to do it and they ' ll do it you know they do it they do they do it but they do </s>\n",
      "true response:  but they </s>\n",
      "generate response:  they ' re not even <unk> </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  uh - huh they do they do a lot of that in the summer </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  oh </s>\n",
      "true response:  and </s>\n",
      "generate response:  you know when they when they ' re in the process of the <unk> and the <unk> and the <unk> and the <unk> and the <unk> and </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  there ' s not there ' s not there ' s not there ' s not there ' s not there ' s not there ' s not there </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.447858, BLEU2 0.365969, BLEU3 0.310292, BLEU4 0.249959, inter_dist1 0.006556, inter_dist2 0.043254 avg_len 16.446634\n",
      " time: 149.4 s\n",
      "Done testing\n",
      "Epoch:  74\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[74 50] loss_ae: 12.677806 loss_g: 66.201555 loss_d: -47.228520 time: 8.0 s\n",
      "[74 100] loss_ae: 12.539396 loss_g: 67.707297 loss_d: -46.891045 time: 7.9 s\n",
      "[74 150] loss_ae: 12.091653 loss_g: 67.373445 loss_d: -52.197930 time: 7.9 s\n",
      "[74 200] loss_ae: 12.945922 loss_g: 71.033716 loss_d: -56.765835 time: 7.8 s\n",
      "[74 250] loss_ae: 11.877419 loss_g: 66.552540 loss_d: -56.242069 time: 7.9 s\n",
      "[74 300] loss_ae: 13.144798 loss_g: 61.816864 loss_d: -49.100486 time: 7.8 s\n",
      "[74 350] loss_ae: 12.847340 loss_g: 73.048562 loss_d: -53.590474 time: 7.9 s\n",
      "[74 400] loss_ae: 12.273096 loss_g: 55.368659 loss_d: -36.944140 time: 8.0 s\n",
      "[74 450] loss_ae: 11.652245 loss_g: 61.766213 loss_d: -47.849494 time: 7.8 s\n",
      "[74 500] loss_ae: 14.246884 loss_g: 78.335353 loss_d: -60.539146 time: 7.8 s\n",
      "[74 550] loss_ae: 12.372460 loss_g: 55.988247 loss_d: -47.650123 time: 8.0 s\n",
      "[74 600] loss_ae: 12.062201 loss_g: 61.317401 loss_d: -48.474177 time: 7.8 s\n",
      "[74 650] loss_ae: 12.031674 loss_g: 64.487113 loss_d: -48.901441 time: 7.8 s\n",
      "[74 700] loss_ae: 13.020736 loss_g: 64.329091 loss_d: -48.977164 time: 7.8 s\n",
      "[74 750] loss_ae: 13.351424 loss_g: 64.650552 loss_d: -49.333816 time: 7.9 s\n",
      "[74 800] loss_ae: 13.579718 loss_g: 65.356471 loss_d: -55.261398 time: 7.9 s\n",
      "[74 850] loss_ae: 13.728061 loss_g: 69.697186 loss_d: -56.567848 time: 7.9 s\n",
      "[74 900] loss_ae: 14.014316 loss_g: 64.145577 loss_d: -48.696759 time: 7.8 s\n",
      "[74 950] loss_ae: 12.075635 loss_g: 64.689112 loss_d: -47.883156 time: 7.8 s\n",
      "[74 1000] loss_ae: 12.241940 loss_g: 73.182563 loss_d: -56.412827 time: 7.9 s\n",
      "[74 1050] loss_ae: 13.142152 loss_g: 63.682306 loss_d: -61.306162 time: 7.6 s\n",
      "[74 1100] loss_ae: 12.327498 loss_g: 74.421788 loss_d: -47.863875 time: 7.9 s\n",
      "[74 1150] loss_ae: 15.146729 loss_g: 70.037103 loss_d: -56.891326 time: 7.7 s\n",
      "[74 1200] loss_ae: 12.082807 loss_g: 65.028066 loss_d: -53.610751 time: 8.4 s\n",
      "[74 1250] loss_ae: 13.293432 loss_g: 62.275105 loss_d: -59.229387 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and i have a i have a <unk> i have a <unk> in a <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  right now we ' re kind of <unk> and we ' re not supposed to be as good as it </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  to do it or </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  they have to do it and they ' re not going to do it but i think that ' s a really good </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  those are good </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  you don ' t you don ' t you don ' t have to have you have you have you have you have you have you have you </s>\n",
      "true response:  oh </s>\n",
      "generate response:  and and there ' s a lot of places where you can go and you can ' t get the name of it so </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and you don ' t you don ' t have to have to have a <unk> you know you have to have a you have </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  it doesn ' t do it do you do you do you do you do you do you do you do you do you do you do you do you\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum they have a </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  you can have you can have you can have a you have a you have a you have a you have a you have a </s>\n",
      "true response:  but they </s>\n",
      "generate response:  you have to you have to have to have a you have to have a you have a <unk> </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  and you can do it with you know like you say it ' s like a <unk> and you know it ' s like you know </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know there ' s a lot of things that you can do with it you know you </s>\n",
      "true response:  and </s>\n",
      "generate response:  it really does seem to be too bad that they ' ve done it ' s too bad they ' ve got to do </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  they have a lot of <unk> and they have a lot of <unk> and they have a lot of <unk> </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  oh i just couldn ' t believe that was </s>\n",
      "BLEU1 0.450530, BLEU2 0.367809, BLEU3 0.311303, BLEU4 0.250418, inter_dist1 0.006807, inter_dist2 0.044209 avg_len 16.323481\n",
      " time: 145.1 s\n",
      "Done testing\n",
      "Epoch:  75\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[75 50] loss_ae: 13.650383 loss_g: 70.792238 loss_d: -57.039848 time: 8.1 s\n",
      "[75 100] loss_ae: 12.665654 loss_g: 56.670780 loss_d: -50.280541 time: 8.3 s\n",
      "[75 150] loss_ae: 13.553657 loss_g: 68.806539 loss_d: -51.010441 time: 8.1 s\n",
      "[75 200] loss_ae: 13.306063 loss_g: 70.743135 loss_d: -48.968266 time: 8.3 s\n",
      "[75 250] loss_ae: 12.190115 loss_g: 63.917964 loss_d: -53.109418 time: 8.2 s\n",
      "[75 300] loss_ae: 11.647353 loss_g: 54.775973 loss_d: -35.534577 time: 8.2 s\n",
      "[75 350] loss_ae: 12.878716 loss_g: 57.300218 loss_d: -38.979678 time: 8.1 s\n",
      "[75 400] loss_ae: 12.726396 loss_g: 63.694008 loss_d: -57.209582 time: 8.3 s\n",
      "[75 450] loss_ae: 13.740668 loss_g: 64.188028 loss_d: -53.052267 time: 8.3 s\n",
      "[75 500] loss_ae: 12.555363 loss_g: 62.955183 loss_d: -57.024012 time: 8.3 s\n",
      "[75 550] loss_ae: 12.866202 loss_g: 63.240572 loss_d: -53.631557 time: 8.2 s\n",
      "[75 600] loss_ae: 12.372744 loss_g: 67.057961 loss_d: -59.571000 time: 8.3 s\n",
      "[75 650] loss_ae: 13.241746 loss_g: 63.031616 loss_d: -52.894012 time: 8.3 s\n",
      "[75 700] loss_ae: 11.488758 loss_g: 73.647027 loss_d: -59.153596 time: 8.3 s\n",
      "[75 750] loss_ae: 11.003014 loss_g: 68.348008 loss_d: -55.025248 time: 8.2 s\n",
      "[75 800] loss_ae: 11.335784 loss_g: 59.199844 loss_d: -52.892424 time: 8.2 s\n",
      "[75 850] loss_ae: 12.744033 loss_g: 68.515149 loss_d: -56.032903 time: 8.2 s\n",
      "[75 900] loss_ae: 13.627954 loss_g: 60.947493 loss_d: -40.606403 time: 8.2 s\n",
      "[75 950] loss_ae: 12.659228 loss_g: 65.074431 loss_d: -57.723313 time: 8.3 s\n",
      "[75 1000] loss_ae: 13.928787 loss_g: 62.692516 loss_d: -50.328301 time: 8.1 s\n",
      "[75 1050] loss_ae: 13.216491 loss_g: 64.236458 loss_d: -53.335229 time: 8.3 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75 1100] loss_ae: 12.668672 loss_g: 60.790519 loss_d: -56.247068 time: 8.0 s\n",
      "[75 1150] loss_ae: 11.636281 loss_g: 76.275342 loss_d: -60.539616 time: 8.2 s\n",
      "[75 1200] loss_ae: 12.033682 loss_g: 63.963143 loss_d: -46.745766 time: 8.3 s\n",
      "[75 1250] loss_ae: 11.934740 loss_g: 61.057071 loss_d: -46.652404 time: 8.3 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and the other thing is he ' s a he ' s a <unk> and he ' s a very <unk> <unk> and he ' s </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  no they are they are they are they are they are they are they are they are they are they are they are they are they are they are they\n",
      "true response:  yeah </s>\n",
      "generate response:  when they do have a little bit of a <unk> that ' s kind of a <unk> thing and i think that ' s a good idea </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  you can you can you can you can you can you can you can you can you can you can you can you can you can you can have a\n",
      "true response:  something like that </s>\n",
      "generate response:  well no i don ' t think they have to do it i mean they ' re not going to be as much as they </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  you know if you if you go to a restaurant or </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  they just </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  because if he doesn ' t like it he ' s not really <unk> he doesn ' t like to be in the house </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  it ' s not like a <unk> or something like that you know </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  you know a lot of people that are in the same place and they ' re not going to be you know they ' re </s>\n",
      "true response:  but they </s>\n",
      "generate response:  because they ' ll get you know they ' ll get they ' ll do something they can do they can do it they don ' t </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  and they have they have you have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  because i would have to be able to do that i mean i can ' t even remember what i ' m doing but </s>\n",
      "true response:  and </s>\n",
      "generate response:  right </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  you can get you can get out there and get it done and then </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  yeah </s>\n",
      "BLEU1 0.446806, BLEU2 0.366384, BLEU3 0.310880, BLEU4 0.250359, inter_dist1 0.006377, inter_dist2 0.040858 avg_len 17.481664\n",
      " time: 145.6 s\n",
      "Done testing\n",
      "Epoch:  76\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[76 50] loss_ae: 13.450979 loss_g: 71.061985 loss_d: -57.302478 time: 8.5 s\n",
      "[76 100] loss_ae: 14.140969 loss_g: 56.234440 loss_d: -49.489247 time: 8.2 s\n",
      "[76 150] loss_ae: 12.185801 loss_g: 63.513785 loss_d: -49.411951 time: 8.3 s\n",
      "[76 200] loss_ae: 10.952172 loss_g: 56.545508 loss_d: -51.111436 time: 8.1 s\n",
      "[76 250] loss_ae: 9.821400 loss_g: 56.144213 loss_d: -55.531296 time: 8.2 s\n",
      "[76 300] loss_ae: 12.511995 loss_g: 67.866457 loss_d: -56.030936 time: 8.2 s\n",
      "[76 350] loss_ae: 11.403380 loss_g: 57.614747 loss_d: -44.685343 time: 8.3 s\n",
      "[76 400] loss_ae: 12.944758 loss_g: 57.228112 loss_d: -48.194333 time: 8.3 s\n",
      "[76 450] loss_ae: 12.739038 loss_g: 61.771329 loss_d: -53.311552 time: 8.2 s\n",
      "[76 500] loss_ae: 12.477438 loss_g: 61.254488 loss_d: -48.840625 time: 8.2 s\n",
      "[76 550] loss_ae: 13.081454 loss_g: 61.468910 loss_d: -54.483371 time: 8.2 s\n",
      "[76 600] loss_ae: 14.396526 loss_g: 70.174863 loss_d: -55.390325 time: 8.2 s\n",
      "[76 650] loss_ae: 12.378291 loss_g: 63.784661 loss_d: -51.154920 time: 8.2 s\n",
      "[76 700] loss_ae: 14.278517 loss_g: 66.094996 loss_d: -53.133218 time: 8.3 s\n",
      "[76 750] loss_ae: 13.058815 loss_g: 57.519073 loss_d: -46.016786 time: 8.2 s\n",
      "[76 800] loss_ae: 13.494488 loss_g: 64.108966 loss_d: -59.620239 time: 8.4 s\n",
      "[76 850] loss_ae: 11.927439 loss_g: 60.464133 loss_d: -55.493346 time: 8.1 s\n",
      "[76 900] loss_ae: 12.803322 loss_g: 69.650426 loss_d: -54.599512 time: 8.2 s\n",
      "[76 950] loss_ae: 15.192873 loss_g: 69.331935 loss_d: -52.921863 time: 8.2 s\n",
      "[76 1000] loss_ae: 12.205788 loss_g: 73.279714 loss_d: -57.611961 time: 8.3 s\n",
      "[76 1050] loss_ae: 13.179296 loss_g: 66.515460 loss_d: -51.741689 time: 8.1 s\n",
      "[76 1100] loss_ae: 12.509520 loss_g: 73.706197 loss_d: -60.233727 time: 8.3 s\n",
      "[76 1150] loss_ae: 12.894136 loss_g: 60.218445 loss_d: -45.281180 time: 8.2 s\n",
      "[76 1200] loss_ae: 13.208873 loss_g: 69.769729 loss_d: -62.586033 time: 8.3 s\n",
      "[76 1250] loss_ae: 12.817137 loss_g: 56.884370 loss_d: -41.408548 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  to be able to do it but there ' s not there ' s not there ' s not there </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  you know with you have you have you have you have you have you have you have you have you have you have you have you have you </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  or they would they would you know they ' d have to have a <unk> they have to have a <unk> they have to have </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  if they don ' t do it they don ' t they don ' t they don ' t they don ' t they don ' t </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  oh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  you know they have they don ' t have to have a lot of <unk> and they have a lot of <unk> and they have </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  oh uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  or they ' ll do it but they ' re not they ' re not they ' re not they </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah they were they were they were they were they were they were they were they were they were they were they were </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  but they don ' t do that they don ' t do anything to do they do they do it they do they do it they do </s>\n",
      "true response:  but they </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  they like they have they have you have you have you have you have you have you have you have you have you have you have you have you </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know if </s>\n",
      "true response:  and </s>\n",
      "generate response:  you don ' t have to have to have a you have to have a <unk> you know you have </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  uh </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  i know i don ' t know i just i don ' t know i just don ' t know </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU1 0.437237, BLEU2 0.357669, BLEU3 0.302210, BLEU4 0.242822, inter_dist1 0.006417, inter_dist2 0.040303 avg_len 16.318737\n",
      " time: 146.2 s\n",
      "Done testing\n",
      "Epoch:  77\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[77 50] loss_ae: 13.621265 loss_g: 71.286364 loss_d: -60.201575 time: 8.1 s\n",
      "[77 100] loss_ae: 13.786707 loss_g: 58.637111 loss_d: -45.160833 time: 8.3 s\n",
      "[77 150] loss_ae: 13.636718 loss_g: 56.161177 loss_d: -45.776324 time: 8.1 s\n",
      "[77 200] loss_ae: 12.978549 loss_g: 59.406504 loss_d: -51.797628 time: 8.4 s\n",
      "[77 250] loss_ae: 12.872517 loss_g: 64.067453 loss_d: -48.757173 time: 8.1 s\n",
      "[77 300] loss_ae: 11.937484 loss_g: 61.005506 loss_d: -59.199236 time: 8.3 s\n",
      "[77 350] loss_ae: 12.816769 loss_g: 77.702070 loss_d: -61.540579 time: 8.1 s\n",
      "[77 400] loss_ae: 13.355735 loss_g: 58.189241 loss_d: -47.580624 time: 8.3 s\n",
      "[77 450] loss_ae: 15.762558 loss_g: 59.456516 loss_d: -47.837111 time: 8.1 s\n",
      "[77 500] loss_ae: 11.519977 loss_g: 71.758527 loss_d: -60.206928 time: 8.3 s\n",
      "[77 550] loss_ae: 14.544261 loss_g: 61.277837 loss_d: -50.867103 time: 8.0 s\n",
      "[77 600] loss_ae: 14.195632 loss_g: 59.198567 loss_d: -45.875892 time: 8.3 s\n",
      "[77 650] loss_ae: 13.927759 loss_g: 59.798485 loss_d: -44.215435 time: 8.2 s\n",
      "[77 700] loss_ae: 12.126211 loss_g: 64.626270 loss_d: -52.330975 time: 8.3 s\n",
      "[77 750] loss_ae: 12.460117 loss_g: 60.992558 loss_d: -54.256360 time: 8.1 s\n",
      "[77 800] loss_ae: 12.353965 loss_g: 59.787166 loss_d: -51.066216 time: 8.3 s\n",
      "[77 850] loss_ae: 12.976013 loss_g: 55.029387 loss_d: -46.161184 time: 8.3 s\n",
      "[77 900] loss_ae: 12.386834 loss_g: 64.105453 loss_d: -52.545784 time: 8.3 s\n",
      "[77 950] loss_ae: 12.091343 loss_g: 60.962016 loss_d: -52.867307 time: 8.3 s\n",
      "[77 1000] loss_ae: 12.709144 loss_g: 56.254179 loss_d: -42.533962 time: 8.3 s\n",
      "[77 1050] loss_ae: 12.095261 loss_g: 61.394567 loss_d: -43.773694 time: 8.3 s\n",
      "[77 1100] loss_ae: 12.305001 loss_g: 63.717214 loss_d: -43.455473 time: 8.2 s\n",
      "[77 1150] loss_ae: 11.192207 loss_g: 68.245210 loss_d: -53.163828 time: 8.3 s\n",
      "[77 1200] loss_ae: 12.511519 loss_g: 59.802568 loss_d: -54.979334 time: 8.4 s\n",
      "[77 1250] loss_ae: 11.843087 loss_g: 59.738797 loss_d: -44.922396 time: 8.3 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  they have they have a they have a have a <unk> they have a <unk> <unk> </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  do you do any kind of fishing do you do you like to do that you don ' t have to do that </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  um - hum do you </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  they say that you ' re not going to do it you don ' t have to do anything about it you know </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  yeah i don ' t like to watch </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  you know they have they have they have you have you have you have you have you have you have you have you have you have you have you </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and you can ' t you can ' t have a you can ' t have a <unk> you know you can </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  did they have they have they have they have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  to get out of the home and the kids are in school and they ' re not going to be the ones that are </s>\n",
      "true response:  but they </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and a lot of times when i was in high school i was in </s>\n",
      "true response:  and </s>\n",
      "generate response:  i don ' t know if you ' ve ever heard of </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  and you know you can ' t get out there and get it down there and then </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  and she ' s a <unk> </s>\n",
      "BLEU1 0.445240, BLEU2 0.365415, BLEU3 0.310208, BLEU4 0.249901, inter_dist1 0.006512, inter_dist2 0.041042 avg_len 16.949827\n",
      " time: 146.1 s\n",
      "Done testing\n",
      "Epoch:  78\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[78 50] loss_ae: 12.693424 loss_g: 69.240271 loss_d: -54.504885 time: 8.2 s\n",
      "[78 100] loss_ae: 12.443627 loss_g: 70.702923 loss_d: -53.740098 time: 8.1 s\n",
      "[78 150] loss_ae: 12.974837 loss_g: 59.051693 loss_d: -48.030540 time: 8.4 s\n",
      "[78 200] loss_ae: 11.456264 loss_g: 67.858980 loss_d: -52.951330 time: 8.2 s\n",
      "[78 250] loss_ae: 11.711386 loss_g: 63.108242 loss_d: -50.950111 time: 8.3 s\n",
      "[78 300] loss_ae: 13.454307 loss_g: 69.591689 loss_d: -58.799565 time: 8.1 s\n",
      "[78 350] loss_ae: 11.834589 loss_g: 53.172054 loss_d: -47.278497 time: 8.3 s\n",
      "[78 400] loss_ae: 11.795376 loss_g: 64.787974 loss_d: -55.579306 time: 8.2 s\n",
      "[78 450] loss_ae: 12.036035 loss_g: 70.407986 loss_d: -47.137828 time: 8.2 s\n",
      "[78 500] loss_ae: 13.804408 loss_g: 69.640100 loss_d: -48.252009 time: 8.2 s\n",
      "[78 550] loss_ae: 13.602428 loss_g: 72.338505 loss_d: -56.051913 time: 8.2 s\n",
      "[78 600] loss_ae: 13.805642 loss_g: 72.963146 loss_d: -62.176494 time: 8.3 s\n",
      "[78 650] loss_ae: 12.715076 loss_g: 63.903856 loss_d: -46.448663 time: 8.2 s\n",
      "[78 700] loss_ae: 11.320107 loss_g: 72.147332 loss_d: -59.486673 time: 8.2 s\n",
      "[78 750] loss_ae: 12.411742 loss_g: 62.954682 loss_d: -45.718396 time: 8.2 s\n",
      "[78 800] loss_ae: 12.372181 loss_g: 72.411060 loss_d: -65.521605 time: 8.3 s\n",
      "[78 850] loss_ae: 15.345918 loss_g: 79.470350 loss_d: -60.678743 time: 8.2 s\n",
      "[78 900] loss_ae: 12.745211 loss_g: 64.993313 loss_d: -47.949566 time: 8.2 s\n",
      "[78 950] loss_ae: 12.636708 loss_g: 69.537424 loss_d: -55.795878 time: 8.1 s\n",
      "[78 1000] loss_ae: 13.495788 loss_g: 51.729161 loss_d: -52.025566 time: 8.3 s\n",
      "[78 1050] loss_ae: 11.116768 loss_g: 65.970969 loss_d: -47.654004 time: 8.2 s\n",
      "[78 1100] loss_ae: 12.635219 loss_g: 61.395405 loss_d: -50.527589 time: 8.2 s\n",
      "[78 1150] loss_ae: 12.630528 loss_g: 75.902248 loss_d: -65.971013 time: 8.2 s\n",
      "[78 1200] loss_ae: 14.261294 loss_g: 69.830645 loss_d: -62.364167 time: 8.4 s\n",
      "[78 1250] loss_ae: 12.795447 loss_g: 59.890195 loss_d: -44.321760 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and you don ' t you don ' t have to have to have a <unk> you have to have a you have to have a </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  you know in the middle of it or something like that </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  it ' s not like the </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  but it ' s not it ' s not really it ' s not really it ' s not really the same as it is </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  if you if you have a chance to get a chance to get a chance to </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  oh i know i have to make that one that i ' ve ever done that is really good </s>\n",
      "true response:  oh </s>\n",
      "generate response:  they ' ve been they ' ve been they ' ve been they ' ve been they ' ve been real <unk> for their little <unk> </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and they have they have they have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  are not real good about it but it ' s not it ' s not that it ' s not that we don ' t have </s>\n",
      "true response:  but they </s>\n",
      "generate response:  or if they ' re not going to do it or do they just </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and </s>\n",
      "true response:  and </s>\n",
      "generate response:  you know and they ' re just not <unk> enough </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  oh uh - huh they ' ve got they have a they have a <unk> <unk> they have a </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.430585, BLEU2 0.349864, BLEU3 0.294423, BLEU4 0.235927, inter_dist1 0.007289, inter_dist2 0.047651 avg_len 14.994162\n",
      " time: 153.1 s\n",
      "Done testing\n",
      "Epoch:  79\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[79 50] loss_ae: 11.812340 loss_g: 68.135997 loss_d: -54.350949 time: 7.9 s\n",
      "[79 100] loss_ae: 12.503289 loss_g: 77.261889 loss_d: -54.628258 time: 7.9 s\n",
      "[79 150] loss_ae: 12.466198 loss_g: 62.480322 loss_d: -45.852027 time: 7.8 s\n",
      "[79 200] loss_ae: 13.384125 loss_g: 60.549520 loss_d: -43.283855 time: 7.9 s\n",
      "[79 250] loss_ae: 11.967264 loss_g: 65.724268 loss_d: -49.352628 time: 8.0 s\n",
      "[79 300] loss_ae: 12.033189 loss_g: 74.019450 loss_d: -56.272624 time: 7.9 s\n",
      "[79 350] loss_ae: 13.012910 loss_g: 71.155505 loss_d: -52.840415 time: 7.9 s\n",
      "[79 400] loss_ae: 13.469232 loss_g: 66.024025 loss_d: -51.799282 time: 7.9 s\n",
      "[79 450] loss_ae: 13.512683 loss_g: 58.401061 loss_d: -44.397206 time: 7.9 s\n",
      "[79 500] loss_ae: 12.096269 loss_g: 72.875193 loss_d: -61.071476 time: 7.9 s\n",
      "[79 550] loss_ae: 11.538979 loss_g: 53.282964 loss_d: -48.031570 time: 7.8 s\n",
      "[79 600] loss_ae: 11.766107 loss_g: 66.348643 loss_d: -53.072452 time: 7.8 s\n",
      "[79 650] loss_ae: 13.585668 loss_g: 64.371462 loss_d: -53.593896 time: 7.9 s\n",
      "[79 700] loss_ae: 12.712377 loss_g: 65.571524 loss_d: -48.321734 time: 8.0 s\n",
      "[79 750] loss_ae: 13.585912 loss_g: 64.988870 loss_d: -51.733163 time: 8.0 s\n",
      "[79 800] loss_ae: 12.479661 loss_g: 54.184300 loss_d: -37.097746 time: 8.2 s\n",
      "[79 850] loss_ae: 13.413778 loss_g: 60.752522 loss_d: -42.127505 time: 8.3 s\n",
      "[79 900] loss_ae: 12.753034 loss_g: 67.707447 loss_d: -57.371695 time: 8.2 s\n",
      "[79 950] loss_ae: 11.492174 loss_g: 61.723180 loss_d: -43.114027 time: 8.3 s\n",
      "[79 1000] loss_ae: 14.385154 loss_g: 69.407557 loss_d: -55.649127 time: 8.2 s\n",
      "[79 1050] loss_ae: 12.210257 loss_g: 69.209858 loss_d: -59.056109 time: 8.2 s\n",
      "[79 1100] loss_ae: 13.547177 loss_g: 64.724471 loss_d: -54.442856 time: 8.2 s\n",
      "[79 1150] loss_ae: 12.270039 loss_g: 63.410364 loss_d: -52.404519 time: 8.3 s\n",
      "[79 1200] loss_ae: 12.464911 loss_g: 74.936296 loss_d: -60.679494 time: 8.3 s\n",
      "[79 1250] loss_ae: 12.480975 loss_g: 58.517019 loss_d: -47.597258 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  do they feel that there ' s so much more <unk> to do with the kids and the kids and the kids and the kids and </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  they are in their own home </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  hum </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  they have to have to have a <unk> they have to have a <unk> they have to have a <unk> they have to have a </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  what do they do if they do they do it they do it they do it they do it they do it they do it they do it </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  oh </s>\n",
      "generate response:  they ' ve been they ' ve been they ' ve been real <unk> for their </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  it ' ll be interesting to see if you ' ve heard of it but it ' s not it ' s not like the <unk> of the </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  that they get the little <unk> and they ' re just they ' re just they ' re just they ' re just they ' re </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  you can they can they can they can they can they can they can they can they can they can they can they can they can they can they can\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  because there ' s so many people that are in the middle of the world and i think that the parents are the ones that are </s>\n",
      "true response:  but they </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah if they if they have a good time with them they ' re not </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know she would she would be you know not to be able to do it or do you know what she ' s doing </s>\n",
      "true response:  and </s>\n",
      "generate response:  we ' ll be darned </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  oh yeah </s>\n",
      "BLEU1 0.430437, BLEU2 0.352098, BLEU3 0.298332, BLEU4 0.240271, inter_dist1 0.006959, inter_dist2 0.043250 avg_len 16.097610\n",
      " time: 145.5 s\n",
      "Done testing\n",
      "Epoch:  80\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[80 50] loss_ae: 11.057473 loss_g: 56.060520 loss_d: -46.389683 time: 8.4 s\n",
      "[80 100] loss_ae: 13.430224 loss_g: 54.285096 loss_d: -41.406503 time: 8.1 s\n",
      "[80 150] loss_ae: 12.077763 loss_g: 62.044253 loss_d: -48.518821 time: 8.3 s\n",
      "[80 200] loss_ae: 10.463376 loss_g: 62.789738 loss_d: -52.328072 time: 8.2 s\n",
      "[80 250] loss_ae: 12.755326 loss_g: 59.239720 loss_d: -48.690401 time: 8.2 s\n",
      "[80 300] loss_ae: 12.827332 loss_g: 59.240647 loss_d: -40.408121 time: 8.3 s\n",
      "[80 350] loss_ae: 12.785991 loss_g: 62.770463 loss_d: -45.969774 time: 8.3 s\n",
      "[80 400] loss_ae: 13.392229 loss_g: 54.394632 loss_d: -43.487838 time: 8.3 s\n",
      "[80 450] loss_ae: 12.233980 loss_g: 68.296971 loss_d: -54.737948 time: 8.1 s\n",
      "[80 500] loss_ae: 12.382960 loss_g: 55.768595 loss_d: -49.268166 time: 8.3 s\n",
      "[80 550] loss_ae: 13.120607 loss_g: 66.254897 loss_d: -48.109291 time: 8.3 s\n",
      "[80 600] loss_ae: 12.536003 loss_g: 76.103317 loss_d: -63.923181 time: 8.2 s\n",
      "[80 650] loss_ae: 12.769197 loss_g: 67.003600 loss_d: -53.431334 time: 8.3 s\n",
      "[80 700] loss_ae: 12.948510 loss_g: 64.489043 loss_d: -47.425947 time: 8.3 s\n",
      "[80 750] loss_ae: 12.524872 loss_g: 68.957440 loss_d: -52.942822 time: 8.1 s\n",
      "[80 800] loss_ae: 13.269110 loss_g: 52.866952 loss_d: -47.318506 time: 8.3 s\n",
      "[80 850] loss_ae: 12.819144 loss_g: 54.630342 loss_d: -43.159501 time: 8.2 s\n",
      "[80 900] loss_ae: 13.027575 loss_g: 64.296780 loss_d: -44.511765 time: 8.1 s\n",
      "[80 950] loss_ae: 12.984106 loss_g: 60.020960 loss_d: -47.412039 time: 8.3 s\n",
      "[80 1000] loss_ae: 10.976365 loss_g: 68.420812 loss_d: -57.860501 time: 8.2 s\n",
      "[80 1050] loss_ae: 14.536008 loss_g: 55.724215 loss_d: -45.615387 time: 8.3 s\n",
      "[80 1100] loss_ae: 12.511842 loss_g: 56.915223 loss_d: -46.366637 time: 8.2 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80 1150] loss_ae: 11.785789 loss_g: 63.819946 loss_d: -43.778493 time: 8.4 s\n",
      "[80 1200] loss_ae: 12.655370 loss_g: 59.025192 loss_d: -43.095632 time: 8.1 s\n",
      "[80 1250] loss_ae: 15.028719 loss_g: 54.557121 loss_d: -41.527669 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  have been out of school for two years and he ' s been there for a long time and he was a little bit </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  have been <unk> for about ten years and have a </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  right </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  you can you can you can you can you can have <unk> you can you can you can you can you can have a <unk> you have to </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  the only thing i can see is the ones that i ' ve seen in the past i ' ve seen a lot of them </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  it doesn ' t do it doesn ' t do it you don ' t do it or do you do it </s>\n",
      "true response:  oh </s>\n",
      "generate response:  yes </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  but i don ' t know that ' s a good idea to do it but i think it ' s a lot of fun </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  there there ' s not there ' s not there ' s not there ' s not there ' s not there ' s not there ' s not there\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  they they have you know they have a they have a they have a they have a they have a they have a they have a </s>\n",
      "true response:  but they </s>\n",
      "generate response:  and you know there ' s a lot of people that work for ti i ' ve had to work for a company company and i </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  oh you bet </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know you have to have a you have a you have a you have a you have a you have a you have a </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah there ' s so many people that don ' t have to worry about that because they ' re not going to be </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  oh i ' ll bet </s>\n",
      "BLEU1 0.434882, BLEU2 0.356756, BLEU3 0.302731, BLEU4 0.243945, inter_dist1 0.006298, inter_dist2 0.039165 avg_len 16.656997\n",
      " time: 145.9 s\n",
      "Done testing\n",
      "Epoch:  81\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[81 50] loss_ae: 12.363324 loss_g: 59.050311 loss_d: -43.652317 time: 8.1 s\n",
      "[81 100] loss_ae: 13.188998 loss_g: 56.992004 loss_d: -40.526406 time: 8.3 s\n",
      "[81 150] loss_ae: 11.831253 loss_g: 69.512977 loss_d: -47.041630 time: 8.2 s\n",
      "[81 200] loss_ae: 12.317189 loss_g: 63.585460 loss_d: -45.080276 time: 8.2 s\n",
      "[81 250] loss_ae: 11.021792 loss_g: 63.520584 loss_d: -50.790123 time: 8.2 s\n",
      "[81 300] loss_ae: 12.056705 loss_g: 61.155400 loss_d: -49.179518 time: 8.3 s\n",
      "[81 350] loss_ae: 15.466968 loss_g: 65.196343 loss_d: -55.795124 time: 8.3 s\n",
      "[81 400] loss_ae: 13.018409 loss_g: 62.373844 loss_d: -43.378312 time: 8.2 s\n",
      "[81 450] loss_ae: 12.627615 loss_g: 59.397418 loss_d: -43.881167 time: 8.2 s\n",
      "[81 500] loss_ae: 11.756189 loss_g: 72.970471 loss_d: -53.517298 time: 8.3 s\n",
      "[81 550] loss_ae: 13.328838 loss_g: 71.968863 loss_d: -52.884058 time: 8.3 s\n",
      "[81 600] loss_ae: 12.619546 loss_g: 58.351215 loss_d: -42.267633 time: 8.3 s\n",
      "[81 650] loss_ae: 11.970305 loss_g: 64.293108 loss_d: -51.344237 time: 8.3 s\n",
      "[81 700] loss_ae: 12.301291 loss_g: 61.592480 loss_d: -47.676623 time: 8.1 s\n",
      "[81 750] loss_ae: 13.022114 loss_g: 65.371357 loss_d: -45.022480 time: 8.5 s\n",
      "[81 800] loss_ae: 13.963272 loss_g: 62.772645 loss_d: -57.556037 time: 8.2 s\n",
      "[81 850] loss_ae: 13.638702 loss_g: 67.796598 loss_d: -51.327041 time: 8.2 s\n",
      "[81 900] loss_ae: 13.233465 loss_g: 57.126434 loss_d: -46.983764 time: 8.3 s\n",
      "[81 950] loss_ae: 13.083128 loss_g: 58.810236 loss_d: -50.752940 time: 8.2 s\n",
      "[81 1000] loss_ae: 12.592589 loss_g: 65.200780 loss_d: -54.018889 time: 8.2 s\n",
      "[81 1050] loss_ae: 12.275932 loss_g: 73.103118 loss_d: -56.612985 time: 8.3 s\n",
      "[81 1100] loss_ae: 13.227427 loss_g: 59.377829 loss_d: -51.506495 time: 8.3 s\n",
      "[81 1150] loss_ae: 12.733213 loss_g: 67.873753 loss_d: -50.903826 time: 8.2 s\n",
      "[81 1200] loss_ae: 13.675047 loss_g: 55.170141 loss_d: -50.994603 time: 8.4 s\n",
      "[81 1250] loss_ae: 12.337288 loss_g: 59.192198 loss_d: -51.868140 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and and they have they have a they have a <unk> they have a <unk> they have a <unk> they have a <unk> they have a </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  and if </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  don ' t do any good job i mean i think it ' s a good idea </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  they they have they have you have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  something like that </s>\n",
      "generate response:  you know they ' re trying to get the money from the other end of the year and i think it ' s a lot better </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  oh </s>\n",
      "generate response:  and you know the <unk> are the same way to go and </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  if they if they don ' t like they don ' t they don ' t they don ' t they don ' </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum they were talking about the same thing about the <unk> </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  to have a <unk> we have a we have a we have a <unk> a <unk> a <unk> a <unk> <unk> <unk> <unk> </s>\n",
      "true response:  but they </s>\n",
      "generate response:  and you know if you ' re in the same area you ' re not going to have to pay for it but it ' s </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  oh </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and they have you have you have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  and </s>\n",
      "generate response:  but there ' s a lot of places where you can go to the beach and go out and do it again </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  to do you know where the weather is so much of a difference between the two of the homes and the area </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  oh </s>\n",
      "BLEU1 0.419798, BLEU2 0.345012, BLEU3 0.292525, BLEU4 0.235493, inter_dist1 0.005490, inter_dist2 0.034810 avg_len 18.212188\n",
      " time: 146.3 s\n",
      "Done testing\n",
      "Epoch:  82\n",
      "Train begins with 6398 batches with 12 left over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82 50] loss_ae: 13.548893 loss_g: 67.156386 loss_d: -55.775310 time: 8.1 s\n",
      "[82 100] loss_ae: 11.885604 loss_g: 58.889361 loss_d: -44.256339 time: 8.3 s\n",
      "[82 150] loss_ae: 13.677795 loss_g: 56.947170 loss_d: -41.550723 time: 8.3 s\n",
      "[82 200] loss_ae: 12.465453 loss_g: 60.742114 loss_d: -48.050526 time: 8.3 s\n",
      "[82 250] loss_ae: 13.588757 loss_g: 60.690032 loss_d: -42.241471 time: 8.2 s\n",
      "[82 300] loss_ae: 13.548787 loss_g: 56.969866 loss_d: -44.877773 time: 8.3 s\n",
      "[82 350] loss_ae: 12.995280 loss_g: 50.936880 loss_d: -38.901051 time: 8.2 s\n",
      "[82 400] loss_ae: 10.754140 loss_g: 51.626108 loss_d: -40.840349 time: 8.4 s\n",
      "[82 450] loss_ae: 12.603563 loss_g: 58.945602 loss_d: -45.674091 time: 8.3 s\n",
      "[82 500] loss_ae: 12.489509 loss_g: 69.248028 loss_d: -51.847644 time: 8.2 s\n",
      "[82 550] loss_ae: 13.502976 loss_g: 64.469981 loss_d: -46.662682 time: 8.3 s\n",
      "[82 600] loss_ae: 11.240563 loss_g: 54.385782 loss_d: -44.170971 time: 8.2 s\n",
      "[82 650] loss_ae: 12.969768 loss_g: 65.676757 loss_d: -52.101055 time: 8.2 s\n",
      "[82 700] loss_ae: 12.435560 loss_g: 55.829394 loss_d: -45.544265 time: 8.2 s\n",
      "[82 750] loss_ae: 12.450976 loss_g: 51.784296 loss_d: -40.517394 time: 8.2 s\n",
      "[82 800] loss_ae: 13.811340 loss_g: 52.958562 loss_d: -41.680393 time: 8.2 s\n",
      "[82 850] loss_ae: 13.167377 loss_g: 51.101861 loss_d: -39.540942 time: 8.2 s\n",
      "[82 900] loss_ae: 11.960691 loss_g: 55.190618 loss_d: -43.892893 time: 8.2 s\n",
      "[82 950] loss_ae: 14.812051 loss_g: 55.502166 loss_d: -41.838847 time: 8.2 s\n",
      "[82 1000] loss_ae: 13.350200 loss_g: 56.951306 loss_d: -46.840446 time: 8.3 s\n",
      "[82 1050] loss_ae: 12.369371 loss_g: 50.916547 loss_d: -40.839422 time: 8.2 s\n",
      "[82 1100] loss_ae: 13.037340 loss_g: 57.999327 loss_d: -42.793709 time: 8.3 s\n",
      "[82 1150] loss_ae: 13.364952 loss_g: 53.389761 loss_d: -38.931068 time: 8.3 s\n",
      "[82 1200] loss_ae: 11.024945 loss_g: 43.552453 loss_d: -36.528971 time: 8.2 s\n",
      "[82 1250] loss_ae: 12.270633 loss_g: 59.328879 loss_d: -47.015362 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  with a family and you ' re saying you ' re not going to be able to do it you know you ' re not </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  have you have you have you have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  yeah </s>\n",
      "generate response:  you know you can ' t get a lot of information from that and </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  whether they ' re going to do it but they do have to do it with the people who </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  you know they have they have you have you have you have you have you have you have you have you have you have you have you have you </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  and oh they don ' t they don ' t they don ' t they don ' t they don ' t </s>\n",
      "true response:  oh </s>\n",
      "generate response:  oh you know we had a lot of snow and all that and all that stuff </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  you would have to have you have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  uh - huh </s>\n",
      "generate response:  right </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  i can ' t remember the name of it but </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  well i mean they were they were they were they were they were they were they were they were they were they were they were they were they were </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and they have they have they have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  but they </s>\n",
      "generate response:  they have a they have a they have a they have a they have a they have a they have a they have a </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  oh i can ' t </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  are you going to get a little more of a you know you ' re going to have to do something about </s>\n",
      "true response:  and </s>\n",
      "generate response:  oh yeah it ' s not too bad it ' s not too bad it ' s not really it ' s not really </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  when they have little kids they ' re just they ' re just they ' re just they ' re just they ' re </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.432490, BLEU2 0.353209, BLEU3 0.298864, BLEU4 0.240429, inter_dist1 0.006547, inter_dist2 0.040446 avg_len 16.246670\n",
      " time: 145.7 s\n",
      "Done testing\n",
      "Epoch:  83\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[83 50] loss_ae: 13.956840 loss_g: 54.342740 loss_d: -40.995691 time: 8.2 s\n",
      "[83 100] loss_ae: 13.082180 loss_g: 57.729531 loss_d: -49.806968 time: 8.2 s\n",
      "[83 150] loss_ae: 14.937087 loss_g: 65.292859 loss_d: -44.791091 time: 8.2 s\n",
      "[83 200] loss_ae: 13.781640 loss_g: 64.707329 loss_d: -48.820093 time: 8.3 s\n",
      "[83 250] loss_ae: 14.268789 loss_g: 56.680387 loss_d: -40.694260 time: 8.2 s\n",
      "[83 300] loss_ae: 13.409434 loss_g: 48.499318 loss_d: -36.027962 time: 8.3 s\n",
      "[83 350] loss_ae: 13.356677 loss_g: 53.721488 loss_d: -40.250821 time: 8.2 s\n",
      "[83 400] loss_ae: 11.871506 loss_g: 54.701652 loss_d: -42.693234 time: 8.2 s\n",
      "[83 450] loss_ae: 11.906219 loss_g: 57.312323 loss_d: -39.775264 time: 8.1 s\n",
      "[83 500] loss_ae: 13.096564 loss_g: 53.300148 loss_d: -35.675629 time: 8.4 s\n",
      "[83 550] loss_ae: 12.485038 loss_g: 55.201311 loss_d: -47.266131 time: 8.1 s\n",
      "[83 600] loss_ae: 13.181898 loss_g: 46.550849 loss_d: -39.422789 time: 8.3 s\n",
      "[83 650] loss_ae: 11.063795 loss_g: 54.847271 loss_d: -42.507686 time: 8.2 s\n",
      "[83 700] loss_ae: 11.185053 loss_g: 55.126377 loss_d: -48.703639 time: 8.3 s\n",
      "[83 750] loss_ae: 12.443746 loss_g: 50.466676 loss_d: -41.121578 time: 8.2 s\n",
      "[83 800] loss_ae: 13.063536 loss_g: 63.406860 loss_d: -55.274928 time: 8.1 s\n",
      "[83 850] loss_ae: 13.341054 loss_g: 53.273271 loss_d: -40.244838 time: 8.3 s\n",
      "[83 900] loss_ae: 11.654502 loss_g: 66.256075 loss_d: -57.124615 time: 8.1 s\n",
      "[83 950] loss_ae: 13.500778 loss_g: 45.928481 loss_d: -37.358041 time: 8.3 s\n",
      "[83 1000] loss_ae: 14.870820 loss_g: 60.584208 loss_d: -52.385392 time: 8.2 s\n",
      "[83 1050] loss_ae: 12.991495 loss_g: 49.937164 loss_d: -39.892431 time: 8.4 s\n",
      "[83 1100] loss_ae: 13.374709 loss_g: 51.581932 loss_d: -41.057073 time: 8.2 s\n",
      "[83 1150] loss_ae: 11.712400 loss_g: 61.483389 loss_d: -46.250283 time: 8.3 s\n",
      "[83 1200] loss_ae: 11.375771 loss_g: 44.627458 loss_d: -28.692933 time: 8.2 s\n",
      "[83 1250] loss_ae: 12.899788 loss_g: 60.057210 loss_d: -42.158116 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  with their children there are very few women in the family that are in the </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  you know as far </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  to be <unk> if there ' s a lot of people that are there are there are some people who are just not </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  i don ' t think they ' re really not that ' s the way they ' re going to do it because they ' re </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  you don ' t you don ' t have to have you have you have you have you have you have you have you have you have you </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  oh </s>\n",
      "generate response:  oh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  oh they are </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  but they do have to do that i mean i ' ve never been there for a while and i ' ve been there </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and you can get there ' s a lot of people that are in the in the in the in the community and they ' re </s>\n",
      "true response:  but they </s>\n",
      "generate response:  it doesn ' t seem to do anything to do with it but they don ' t do it they don ' t do anything to </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  oh </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  oh do you </s>\n",
      "true response:  and </s>\n",
      "generate response:  it ' ll do not do it if they do they do they do they do they do they do it they do they do it they do they do\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  so it has to be a lot of fun to go to the beach and the mountains and the mountains and the mountains and the </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  oh well there ' s not a whole lot of things that they do </s>\n",
      "BLEU1 0.426293, BLEU2 0.349413, BLEU3 0.295871, BLEU4 0.238076, inter_dist1 0.005943, inter_dist2 0.037827 avg_len 17.254333\n",
      " time: 159.1 s\n",
      "Done testing\n",
      "Epoch:  84\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[84 50] loss_ae: 12.866156 loss_g: 59.538092 loss_d: -47.136251 time: 8.0 s\n",
      "[84 100] loss_ae: 13.915798 loss_g: 57.848132 loss_d: -49.006330 time: 8.0 s\n",
      "[84 150] loss_ae: 12.965671 loss_g: 51.836731 loss_d: -42.088862 time: 7.9 s\n",
      "[84 200] loss_ae: 14.958714 loss_g: 57.199473 loss_d: -43.737846 time: 7.8 s\n",
      "[84 250] loss_ae: 13.321140 loss_g: 57.849036 loss_d: -42.397640 time: 8.3 s\n",
      "[84 300] loss_ae: 13.897773 loss_g: 49.877240 loss_d: -41.405917 time: 8.3 s\n",
      "[84 350] loss_ae: 12.185249 loss_g: 62.444771 loss_d: -49.272740 time: 8.1 s\n",
      "[84 400] loss_ae: 11.068143 loss_g: 55.981660 loss_d: -47.964750 time: 8.3 s\n",
      "[84 450] loss_ae: 12.605308 loss_g: 54.012974 loss_d: -48.779474 time: 8.3 s\n",
      "[84 500] loss_ae: 13.617525 loss_g: 49.285177 loss_d: -39.741135 time: 8.4 s\n",
      "[84 550] loss_ae: 13.583885 loss_g: 55.678507 loss_d: -44.647103 time: 8.0 s\n",
      "[84 600] loss_ae: 11.142020 loss_g: 50.557495 loss_d: -38.597808 time: 8.3 s\n",
      "[84 650] loss_ae: 13.564406 loss_g: 45.320219 loss_d: -37.077929 time: 8.2 s\n",
      "[84 700] loss_ae: 12.045442 loss_g: 49.506165 loss_d: -41.609583 time: 8.2 s\n",
      "[84 750] loss_ae: 12.519399 loss_g: 49.724215 loss_d: -36.157905 time: 8.2 s\n",
      "[84 800] loss_ae: 12.543312 loss_g: 52.079410 loss_d: -44.959641 time: 8.3 s\n",
      "[84 850] loss_ae: 13.652405 loss_g: 62.424429 loss_d: -48.691123 time: 8.2 s\n",
      "[84 900] loss_ae: 12.212248 loss_g: 49.561338 loss_d: -38.329652 time: 8.2 s\n",
      "[84 950] loss_ae: 11.772288 loss_g: 49.589085 loss_d: -45.632558 time: 8.3 s\n",
      "[84 1000] loss_ae: 12.591915 loss_g: 46.691240 loss_d: -38.251478 time: 8.2 s\n",
      "[84 1050] loss_ae: 13.933796 loss_g: 57.651782 loss_d: -47.357853 time: 8.4 s\n",
      "[84 1100] loss_ae: 12.601497 loss_g: 42.226184 loss_d: -36.881468 time: 8.1 s\n",
      "[84 1150] loss_ae: 12.857185 loss_g: 42.078888 loss_d: -38.895239 time: 8.4 s\n",
      "[84 1200] loss_ae: 12.656564 loss_g: 49.590818 loss_d: -38.512288 time: 8.1 s\n",
      "[84 1250] loss_ae: 14.125751 loss_g: 51.119055 loss_d: -37.107610 time: 8.4 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  they do not have the time </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  and they have </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  you have to have to have a you have a you have a you have a you have a you have a <unk> you have to have a </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  i haven ' t seen any of </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  oh </s>\n",
      "generate response:  oh you are a lot of fun too you know </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  they have they have they have they have you have to have a lot of <unk> and they have </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  if they have a chance to see it i think </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  right </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and you have to have to have a you have to have a you have to have a you have to have a </s>\n",
      "true response:  but they </s>\n",
      "generate response:  yeah well that ' s true that ' s true that ' s true that ' s true that ' s true that ' s true that ' s </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  oh uh - huh you don ' t you don ' t you don ' t you don ' t have to have a you </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you ' ll have to you have to do it yourself and you know and then you do it </s>\n",
      "true response:  and </s>\n",
      "generate response:  i ' ve just they have they have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  well do you think it ' s going to be a lot of fun </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  oh i ' m sorry that </s>\n",
      "BLEU1 0.437789, BLEU2 0.356166, BLEU3 0.300495, BLEU4 0.241374, inter_dist1 0.007120, inter_dist2 0.045389 avg_len 15.503010\n",
      " time: 145.6 s\n",
      "Done testing\n",
      "Epoch:  85\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[85 50] loss_ae: 12.442739 loss_g: 55.260149 loss_d: -42.008542 time: 8.3 s\n",
      "[85 100] loss_ae: 13.347019 loss_g: 50.055015 loss_d: -42.574887 time: 8.2 s\n",
      "[85 150] loss_ae: 14.741657 loss_g: 55.400035 loss_d: -38.434333 time: 8.2 s\n",
      "[85 200] loss_ae: 10.668452 loss_g: 61.050401 loss_d: -48.187584 time: 8.2 s\n",
      "[85 250] loss_ae: 13.551784 loss_g: 53.389562 loss_d: -38.549846 time: 8.2 s\n",
      "[85 300] loss_ae: 11.936903 loss_g: 58.472284 loss_d: -44.646676 time: 8.3 s\n",
      "[85 350] loss_ae: 12.277457 loss_g: 52.176548 loss_d: -45.252378 time: 8.4 s\n",
      "[85 400] loss_ae: 12.334506 loss_g: 47.084654 loss_d: -41.308677 time: 8.2 s\n",
      "[85 450] loss_ae: 13.415621 loss_g: 43.458909 loss_d: -31.625860 time: 8.2 s\n",
      "[85 500] loss_ae: 11.960414 loss_g: 49.206503 loss_d: -37.136708 time: 8.3 s\n",
      "[85 550] loss_ae: 14.366389 loss_g: 52.370626 loss_d: -47.075787 time: 8.3 s\n",
      "[85 600] loss_ae: 14.378840 loss_g: 67.658891 loss_d: -46.109855 time: 8.1 s\n",
      "[85 650] loss_ae: 13.015922 loss_g: 51.837772 loss_d: -42.928042 time: 8.4 s\n",
      "[85 700] loss_ae: 13.216996 loss_g: 59.403283 loss_d: -46.535632 time: 8.2 s\n",
      "[85 750] loss_ae: 13.034801 loss_g: 52.085775 loss_d: -42.875589 time: 8.3 s\n",
      "[85 800] loss_ae: 13.417686 loss_g: 49.018539 loss_d: -35.185118 time: 8.2 s\n",
      "[85 850] loss_ae: 13.679377 loss_g: 56.625316 loss_d: -37.558984 time: 8.3 s\n",
      "[85 900] loss_ae: 14.518570 loss_g: 62.463201 loss_d: -44.662152 time: 8.1 s\n",
      "[85 950] loss_ae: 12.995176 loss_g: 60.658662 loss_d: -52.741244 time: 8.3 s\n",
      "[85 1000] loss_ae: 12.114173 loss_g: 57.022955 loss_d: -47.623742 time: 8.1 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85 1050] loss_ae: 12.734057 loss_g: 53.015645 loss_d: -44.383823 time: 8.3 s\n",
      "[85 1100] loss_ae: 12.507245 loss_g: 53.289615 loss_d: -43.987784 time: 8.1 s\n",
      "[85 1150] loss_ae: 12.557768 loss_g: 68.004674 loss_d: -49.234351 time: 8.3 s\n",
      "[85 1200] loss_ae: 12.447689 loss_g: 53.511334 loss_d: -47.564232 time: 8.1 s\n",
      "[85 1250] loss_ae: 12.152066 loss_g: 57.938966 loss_d: -49.139457 time: 8.3 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and they are very young so they ' re not in the in the in the in the in the in the in the home </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  you can do you do you do you have any kind of a garden or a or a pet or a <unk> </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  they really i think they </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  if there ' s not going to do anything there to do it i mean i don ' t think that it ' s a </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  you can you can do you can you can do that with the <unk> and the <unk> and the <unk> and the <unk> and </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  yes </s>\n",
      "true response:  oh </s>\n",
      "generate response:  i don ' t know </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  they have to do that but i think it ' s a lot of fun because you know they ' re all <unk> and </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  yeah i don ' t have to have to have to have a <unk> you know you have to have a you have to have a you </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  they have they have you have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  they have to have to have a they have a they have a have a <unk> they have a they have a </s>\n",
      "true response:  but they </s>\n",
      "generate response:  and if they do that they ' ll do that they ' ll do that they will </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  pardon </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  i think they ' re getting better for the kids to be able to </s>\n",
      "true response:  and </s>\n",
      "generate response:  and they have they have they have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  do you do you do you do you do you do you do you do you do you do you do you do you do you do you do you\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.427342, BLEU2 0.349916, BLEU3 0.295658, BLEU4 0.237404, inter_dist1 0.006790, inter_dist2 0.042963 avg_len 16.122058\n",
      " time: 144.9 s\n",
      "Done testing\n",
      "Epoch:  86\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[86 50] loss_ae: 11.993078 loss_g: 51.479330 loss_d: -41.552922 time: 8.3 s\n",
      "[86 100] loss_ae: 12.708657 loss_g: 50.318950 loss_d: -40.517043 time: 8.2 s\n",
      "[86 150] loss_ae: 12.389767 loss_g: 53.657485 loss_d: -47.133601 time: 8.4 s\n",
      "[86 200] loss_ae: 12.592543 loss_g: 62.059802 loss_d: -44.001177 time: 8.3 s\n",
      "[86 250] loss_ae: 13.037002 loss_g: 54.741518 loss_d: -46.178869 time: 8.3 s\n",
      "[86 300] loss_ae: 11.997985 loss_g: 46.454427 loss_d: -40.529281 time: 8.4 s\n",
      "[86 350] loss_ae: 12.762036 loss_g: 51.079854 loss_d: -45.056732 time: 8.2 s\n",
      "[86 400] loss_ae: 12.268621 loss_g: 49.735492 loss_d: -39.008364 time: 8.3 s\n",
      "[86 450] loss_ae: 14.045446 loss_g: 54.230846 loss_d: -41.798705 time: 8.3 s\n",
      "[86 500] loss_ae: 13.606106 loss_g: 57.009021 loss_d: -43.091591 time: 8.2 s\n",
      "[86 550] loss_ae: 13.565973 loss_g: 55.805711 loss_d: -49.922369 time: 8.3 s\n",
      "[86 600] loss_ae: 12.746705 loss_g: 54.548704 loss_d: -39.665562 time: 8.3 s\n",
      "[86 650] loss_ae: 12.195703 loss_g: 63.328846 loss_d: -50.185503 time: 8.2 s\n",
      "[86 700] loss_ae: 12.266710 loss_g: 47.389533 loss_d: -33.364047 time: 8.4 s\n",
      "[86 750] loss_ae: 13.328219 loss_g: 54.311331 loss_d: -53.782213 time: 8.2 s\n",
      "[86 800] loss_ae: 14.000935 loss_g: 52.394008 loss_d: -39.199312 time: 8.2 s\n",
      "[86 850] loss_ae: 13.296867 loss_g: 55.281777 loss_d: -44.091943 time: 8.1 s\n",
      "[86 900] loss_ae: 12.981966 loss_g: 56.689125 loss_d: -42.617173 time: 8.3 s\n",
      "[86 950] loss_ae: 12.622666 loss_g: 49.832973 loss_d: -37.242754 time: 8.1 s\n",
      "[86 1000] loss_ae: 12.590892 loss_g: 51.819698 loss_d: -40.740516 time: 8.3 s\n",
      "[86 1050] loss_ae: 12.298685 loss_g: 62.792902 loss_d: -49.521236 time: 8.1 s\n",
      "[86 1100] loss_ae: 10.838799 loss_g: 56.121682 loss_d: -38.390970 time: 8.3 s\n",
      "[86 1150] loss_ae: 14.338188 loss_g: 62.238508 loss_d: -50.376959 time: 8.2 s\n",
      "[86 1200] loss_ae: 13.348151 loss_g: 56.972525 loss_d: -48.397428 time: 8.3 s\n",
      "[86 1250] loss_ae: 13.348523 loss_g: 58.863879 loss_d: -53.084167 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and they ' re not they ' re not they ' re not they ' re not they ' re not they ' re not they ' re not </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  and i ' m trying to </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  oh uh - huh they ' ll have to go to the store and they ' ll go and see what they ' re doing </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  which they ' ve been doing and they ' ve been doing that </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  and they can ' t they can ' t they can ' t they can ' t they can ' t they can </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  especially if you like to do a lot of the <unk> and stuff like that and i mean it ' s just it ' s </s>\n",
      "true response:  oh </s>\n",
      "generate response:  well we have a lot of <unk> and we have a we have a we have a we have a we have a <unk> a <unk> a </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and they have a lot of <unk> and they have a lot of <unk> and they have a lot of <unk> and they have a </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and you can you can you can you can you can you can you can you can you can you can have you can you have you have you </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  or they ' re not they ' re not going to do it but they do </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  to have they have they have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  but they </s>\n",
      "generate response:  uh - huh we have we have we have a we have a we have a we have a we have a we have a have a </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  they get real expensive and </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  no </s>\n",
      "true response:  and </s>\n",
      "generate response:  they are they are they are they are they are they are they are they are they are they are they are </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  you know they have they have they have they have you have you have you have you have you have you have you have you have you have you </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  um - hum </s>\n",
      "generate response:  and i ' ve heard of her i have a friend who ' s a nurse </s>\n",
      "BLEU1 0.423745, BLEU2 0.346878, BLEU3 0.294325, BLEU4 0.237146, inter_dist1 0.006520, inter_dist2 0.042179 avg_len 16.230432\n",
      " time: 145.1 s\n",
      "Done testing\n",
      "Epoch:  87\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[87 50] loss_ae: 13.300895 loss_g: 51.790411 loss_d: -49.556982 time: 8.2 s\n",
      "[87 100] loss_ae: 12.003135 loss_g: 65.460683 loss_d: -53.411483 time: 8.2 s\n",
      "[87 150] loss_ae: 12.352210 loss_g: 54.056063 loss_d: -40.861381 time: 8.2 s\n",
      "[87 200] loss_ae: 14.034116 loss_g: 58.461886 loss_d: -48.570011 time: 8.3 s\n",
      "[87 250] loss_ae: 12.385987 loss_g: 41.477191 loss_d: -33.148976 time: 8.1 s\n",
      "[87 300] loss_ae: 12.855967 loss_g: 47.866682 loss_d: -41.227888 time: 8.3 s\n",
      "[87 350] loss_ae: 11.621316 loss_g: 55.645201 loss_d: -44.659942 time: 8.2 s\n",
      "[87 400] loss_ae: 14.186161 loss_g: 55.312325 loss_d: -50.865803 time: 8.3 s\n",
      "[87 450] loss_ae: 12.971264 loss_g: 46.206063 loss_d: -39.857484 time: 8.2 s\n",
      "[87 500] loss_ae: 12.720410 loss_g: 64.460564 loss_d: -44.158653 time: 8.4 s\n",
      "[87 550] loss_ae: 13.770079 loss_g: 53.503211 loss_d: -38.570177 time: 8.1 s\n",
      "[87 600] loss_ae: 12.561451 loss_g: 52.056772 loss_d: -39.570186 time: 8.4 s\n",
      "[87 650] loss_ae: 13.106712 loss_g: 57.073968 loss_d: -47.541219 time: 8.2 s\n",
      "[87 700] loss_ae: 14.994128 loss_g: 46.982248 loss_d: -45.292709 time: 8.4 s\n",
      "[87 750] loss_ae: 10.530760 loss_g: 64.140742 loss_d: -56.293462 time: 8.2 s\n",
      "[87 800] loss_ae: 13.129654 loss_g: 65.750076 loss_d: -41.929599 time: 8.3 s\n",
      "[87 850] loss_ae: 13.387330 loss_g: 51.134317 loss_d: -41.494451 time: 8.1 s\n",
      "[87 900] loss_ae: 13.534866 loss_g: 53.357149 loss_d: -40.622020 time: 8.2 s\n",
      "[87 950] loss_ae: 14.339750 loss_g: 58.289854 loss_d: -51.463826 time: 8.2 s\n",
      "[87 1000] loss_ae: 12.416287 loss_g: 52.233900 loss_d: -39.963621 time: 8.4 s\n",
      "[87 1050] loss_ae: 12.932723 loss_g: 53.000868 loss_d: -40.892767 time: 8.2 s\n",
      "[87 1100] loss_ae: 12.945410 loss_g: 56.274030 loss_d: -35.898040 time: 8.2 s\n",
      "[87 1150] loss_ae: 11.703303 loss_g: 55.524152 loss_d: -47.912266 time: 8.3 s\n",
      "[87 1200] loss_ae: 13.565258 loss_g: 70.491901 loss_d: -54.820534 time: 8.1 s\n",
      "[87 1250] loss_ae: 12.573825 loss_g: 49.676639 loss_d: -45.417388 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  it ' s they don ' t do any more but they do they do they do it but they do they do it they do it by hand </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  you know i have a we have a we have a we have a we have a we have a we have a </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  you can get a little bit of <unk> and you know you can ' t get a chance to get out of it </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  they have to do something about it i think they should be </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  they do have a lot of <unk> and they have a lot of <unk> and </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  yeah really do </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  but they do not do anything but they do they do they do it they do they do it they do they do it they do they do </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  well there ' s been some really good things that </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  they were talking about how they were going to do it i mean they were just talking about the <unk> of the </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  you can you can you can have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  but they </s>\n",
      "generate response:  you can you can you can you can you can you can have a you can have a you have a <unk> </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  they have they have you have you have you have you have you have you have you have you have you have you have you have you have you </s>\n",
      "true response:  and </s>\n",
      "generate response:  oh they do they have they have they have you have you have you have you have you have you have you had any </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  oh they ' ve they ' ve been to <unk> <unk> <unk> </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  yeah </s>\n",
      "BLEU1 0.421120, BLEU2 0.345855, BLEU3 0.293392, BLEU4 0.236195, inter_dist1 0.006388, inter_dist2 0.041596 avg_len 16.566320\n",
      " time: 145.1 s\n",
      "Done testing\n",
      "Epoch:  88\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[88 50] loss_ae: 13.001774 loss_g: 54.225317 loss_d: -45.596452 time: 8.3 s\n",
      "[88 100] loss_ae: 13.040589 loss_g: 47.250353 loss_d: -44.560183 time: 8.3 s\n",
      "[88 150] loss_ae: 13.512435 loss_g: 59.427346 loss_d: -46.654192 time: 8.4 s\n",
      "[88 200] loss_ae: 12.802549 loss_g: 55.103265 loss_d: -46.405570 time: 8.2 s\n",
      "[88 250] loss_ae: 12.689629 loss_g: 56.057112 loss_d: -47.164688 time: 8.2 s\n",
      "[88 300] loss_ae: 11.131719 loss_g: 54.956127 loss_d: -45.223686 time: 8.1 s\n",
      "[88 350] loss_ae: 12.443025 loss_g: 53.137298 loss_d: -43.427365 time: 8.3 s\n",
      "[88 400] loss_ae: 13.257345 loss_g: 48.640633 loss_d: -41.861002 time: 8.1 s\n",
      "[88 450] loss_ae: 14.586403 loss_g: 44.880891 loss_d: -34.907770 time: 8.3 s\n",
      "[88 500] loss_ae: 13.564360 loss_g: 55.397878 loss_d: -47.877670 time: 8.2 s\n",
      "[88 550] loss_ae: 10.917973 loss_g: 59.805504 loss_d: -43.311669 time: 8.2 s\n",
      "[88 600] loss_ae: 12.915505 loss_g: 52.805323 loss_d: -44.529905 time: 8.2 s\n",
      "[88 650] loss_ae: 12.276726 loss_g: 59.105361 loss_d: -49.993178 time: 8.3 s\n",
      "[88 700] loss_ae: 14.057440 loss_g: 59.036875 loss_d: -56.086641 time: 8.2 s\n",
      "[88 750] loss_ae: 13.461350 loss_g: 54.301159 loss_d: -44.897975 time: 8.3 s\n",
      "[88 800] loss_ae: 13.008784 loss_g: 56.505526 loss_d: -49.805399 time: 8.1 s\n",
      "[88 850] loss_ae: 16.144988 loss_g: 63.889877 loss_d: -51.398502 time: 8.2 s\n",
      "[88 900] loss_ae: 13.525574 loss_g: 61.799758 loss_d: -51.170761 time: 8.4 s\n",
      "[88 950] loss_ae: 12.326880 loss_g: 55.708036 loss_d: -41.564755 time: 8.3 s\n",
      "[88 1000] loss_ae: 12.585958 loss_g: 48.288495 loss_d: -41.370501 time: 7.9 s\n",
      "[88 1050] loss_ae: 13.539196 loss_g: 58.036407 loss_d: -46.739228 time: 7.9 s\n",
      "[88 1100] loss_ae: 12.735496 loss_g: 39.055497 loss_d: -29.247857 time: 7.9 s\n",
      "[88 1150] loss_ae: 13.030633 loss_g: 46.974564 loss_d: -38.859727 time: 7.8 s\n",
      "[88 1200] loss_ae: 12.296875 loss_g: 45.809950 loss_d: -35.855127 time: 7.9 s\n",
      "[88 1250] loss_ae: 11.888653 loss_g: 41.564150 loss_d: -30.224315 time: 7.8 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  about two years ago and he was in school and he was in college and he was in college and he was in college and he </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  you do </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  yeah it ' ll do it if you don ' t if you don ' t like it you don ' t want to go out </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  you don ' t you don ' t you don ' t you don ' t you don ' t you don ' t have you have you </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  yeah you don ' t </s>\n",
      "true response:  oh </s>\n",
      "generate response:  they are just really good </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  you have to have you have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh and you have to say </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  right now we ' ve got to go to the <unk> ' s not too bad but it ' s not a good movie but </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  but it ' s still been nice </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  you know with <unk> and they have they have a <unk> they have a <unk> <unk> you have to have </s>\n",
      "true response:  but they </s>\n",
      "generate response:  so </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  i ' ve you have to have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  well they ' re fun though </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  you know they are they ' re like they ' re </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  yeah </s>\n",
      "BLEU1 0.414832, BLEU2 0.339639, BLEU3 0.286704, BLEU4 0.230111, inter_dist1 0.005909, inter_dist2 0.038299 avg_len 16.458311\n",
      " time: 157.2 s\n",
      "Done testing\n",
      "Epoch:  89\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[89 50] loss_ae: 13.979659 loss_g: 56.770737 loss_d: -53.903331 time: 8.2 s\n",
      "[89 100] loss_ae: 13.625650 loss_g: 68.838490 loss_d: -54.319529 time: 8.2 s\n",
      "[89 150] loss_ae: 12.906908 loss_g: 55.495554 loss_d: -46.428930 time: 8.3 s\n",
      "[89 200] loss_ae: 12.154923 loss_g: 42.536171 loss_d: -32.663851 time: 8.2 s\n",
      "[89 250] loss_ae: 12.892202 loss_g: 50.824818 loss_d: -36.643300 time: 8.5 s\n",
      "[89 300] loss_ae: 14.210435 loss_g: 54.523208 loss_d: -41.417407 time: 8.0 s\n",
      "[89 350] loss_ae: 13.526819 loss_g: 56.374267 loss_d: -32.209609 time: 8.4 s\n",
      "[89 400] loss_ae: 13.607579 loss_g: 53.578818 loss_d: -47.067136 time: 8.2 s\n",
      "[89 450] loss_ae: 13.744388 loss_g: 56.806101 loss_d: -46.399389 time: 8.2 s\n",
      "[89 500] loss_ae: 12.074795 loss_g: 53.183756 loss_d: -40.482111 time: 8.2 s\n",
      "[89 550] loss_ae: 11.899900 loss_g: 47.960846 loss_d: -35.805428 time: 8.3 s\n",
      "[89 600] loss_ae: 13.294938 loss_g: 49.613372 loss_d: -41.587923 time: 8.4 s\n",
      "[89 650] loss_ae: 14.001245 loss_g: 43.936588 loss_d: -33.668420 time: 8.3 s\n",
      "[89 700] loss_ae: 15.289016 loss_g: 46.200388 loss_d: -41.272109 time: 8.3 s\n",
      "[89 750] loss_ae: 12.726254 loss_g: 57.603717 loss_d: -46.198112 time: 8.3 s\n",
      "[89 800] loss_ae: 13.243764 loss_g: 50.375782 loss_d: -43.091711 time: 8.3 s\n",
      "[89 850] loss_ae: 12.270198 loss_g: 48.636756 loss_d: -40.465428 time: 8.4 s\n",
      "[89 900] loss_ae: 12.961670 loss_g: 53.393846 loss_d: -42.395314 time: 8.1 s\n",
      "[89 950] loss_ae: 13.736006 loss_g: 56.503745 loss_d: -41.336389 time: 8.2 s\n",
      "[89 1000] loss_ae: 12.465295 loss_g: 55.379768 loss_d: -43.072996 time: 8.2 s\n",
      "[89 1050] loss_ae: 11.740763 loss_g: 59.828047 loss_d: -48.941515 time: 8.2 s\n",
      "[89 1100] loss_ae: 11.639260 loss_g: 57.228685 loss_d: -41.043299 time: 8.2 s\n",
      "[89 1150] loss_ae: 13.019400 loss_g: 51.180266 loss_d: -42.595333 time: 8.3 s\n",
      "[89 1200] loss_ae: 13.080736 loss_g: 47.489072 loss_d: -48.766309 time: 8.3 s\n",
      "[89 1250] loss_ae: 13.096526 loss_g: 60.303449 loss_d: -46.437969 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and they have they have they have they have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  have you do you have any children that you like to do or do you </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  <unk> are you kidding </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and they have they have they have you have you have you have you have you have you have you have you have you have you have you </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  no <unk> <unk> i had </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  sure </s>\n",
      "true response:  oh </s>\n",
      "generate response:  are you familiar with that </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  have you have you have you have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  uh - huh </s>\n",
      "generate response:  they sure are the ones that are the ones that are the ones that are the ones that </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  not only a couple of years ago but i don ' t know if you ' ve ever heard of </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  oh really </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  they have you have you have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  but they </s>\n",
      "generate response:  for you there ' s not a whole lot of money that ' s going to be a lot of </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  they just they just they just they just they just they just they just you just have to have a you have to have a </s>\n",
      "true response:  and </s>\n",
      "generate response:  have they have they have a they have a they have a have a you have a they have a have you have you have you have you have you\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  yeah there ' s you don ' t you don ' t really have to go to any place where you can go and see what </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  yeah </s>\n",
      "BLEU1 0.424228, BLEU2 0.348598, BLEU3 0.295344, BLEU4 0.237639, inter_dist1 0.006062, inter_dist2 0.038916 avg_len 16.794198\n",
      " time: 145.6 s\n",
      "Done testing\n",
      "Epoch:  90\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[90 50] loss_ae: 14.236917 loss_g: 51.413763 loss_d: -38.743162 time: 8.3 s\n",
      "[90 100] loss_ae: 13.507351 loss_g: 44.820750 loss_d: -33.784158 time: 8.3 s\n",
      "[90 150] loss_ae: 12.663934 loss_g: 62.386481 loss_d: -52.754854 time: 8.1 s\n",
      "[90 200] loss_ae: 12.095516 loss_g: 57.766913 loss_d: -47.210831 time: 8.1 s\n",
      "[90 250] loss_ae: 14.265453 loss_g: 60.280259 loss_d: -46.188887 time: 8.4 s\n",
      "[90 300] loss_ae: 13.841794 loss_g: 55.261782 loss_d: -50.884497 time: 8.1 s\n",
      "[90 350] loss_ae: 11.726335 loss_g: 58.701368 loss_d: -40.154051 time: 8.3 s\n",
      "[90 400] loss_ae: 12.623581 loss_g: 57.005234 loss_d: -47.157252 time: 8.2 s\n",
      "[90 450] loss_ae: 12.024315 loss_g: 50.730296 loss_d: -39.757673 time: 8.4 s\n",
      "[90 500] loss_ae: 14.916146 loss_g: 52.530922 loss_d: -47.158400 time: 8.1 s\n",
      "[90 550] loss_ae: 12.368717 loss_g: 51.869269 loss_d: -49.555596 time: 8.2 s\n",
      "[90 600] loss_ae: 13.329862 loss_g: 58.147986 loss_d: -45.890403 time: 8.1 s\n",
      "[90 650] loss_ae: 12.717476 loss_g: 59.624146 loss_d: -44.114866 time: 8.2 s\n",
      "[90 700] loss_ae: 13.453353 loss_g: 68.396104 loss_d: -48.345064 time: 8.3 s\n",
      "[90 750] loss_ae: 11.490971 loss_g: 47.147973 loss_d: -39.334274 time: 8.2 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90 800] loss_ae: 13.082747 loss_g: 59.785046 loss_d: -49.617427 time: 8.3 s\n",
      "[90 850] loss_ae: 13.715729 loss_g: 58.853496 loss_d: -50.389934 time: 8.2 s\n",
      "[90 900] loss_ae: 14.287871 loss_g: 58.075284 loss_d: -56.153450 time: 8.3 s\n",
      "[90 950] loss_ae: 12.904881 loss_g: 51.283081 loss_d: -34.622133 time: 8.1 s\n",
      "[90 1000] loss_ae: 12.996835 loss_g: 53.605338 loss_d: -40.897828 time: 8.3 s\n",
      "[90 1050] loss_ae: 13.337683 loss_g: 47.648267 loss_d: -50.086796 time: 8.2 s\n",
      "[90 1100] loss_ae: 12.029847 loss_g: 53.550934 loss_d: -36.111537 time: 8.2 s\n",
      "[90 1150] loss_ae: 12.712329 loss_g: 45.661450 loss_d: -44.266403 time: 8.2 s\n",
      "[90 1200] loss_ae: 13.664434 loss_g: 49.796669 loss_d: -37.036212 time: 8.3 s\n",
      "[90 1250] loss_ae: 12.670691 loss_g: 50.846089 loss_d: -36.054270 time: 8.3 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and you can have they have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  have been <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  they they are they are they are they are they are they are you know what they ' re saying and they ' re not they ' re not </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and you can have you can have a you have a you have a you have a you have a you have a <unk> you have to </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  to be <unk> if you can get a gun and you can ' t get a gun in the </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  to have you have you have you have you have you have you have you have you have you have you have you have you have you have you </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  no it ' s just not as bad as it </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  right </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  to have you have you have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  but they </s>\n",
      "generate response:  but they do they do they do they do a lot of things like that and they ' re not they ' re not </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  i you know like you say you know you can ' t get a lot of <unk> and stuff like that but then again </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  they are they are you know what i ' m saying i ' m not sure if i ' m going to be able to do that </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah that ' s do you do you think that ' s a good idea </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  oh i know we ' ve never been there </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  yeah it ' s </s>\n",
      "BLEU1 0.422375, BLEU2 0.348307, BLEU3 0.296309, BLEU4 0.239028, inter_dist1 0.005592, inter_dist2 0.034083 avg_len 17.391169\n",
      " time: 145.8 s\n",
      "Done testing\n",
      "Epoch:  91\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[91 50] loss_ae: 13.444869 loss_g: 57.312507 loss_d: -47.712972 time: 8.3 s\n",
      "[91 100] loss_ae: 12.995194 loss_g: 47.013979 loss_d: -38.829179 time: 8.2 s\n",
      "[91 150] loss_ae: 11.873674 loss_g: 53.565981 loss_d: -42.589466 time: 8.2 s\n",
      "[91 200] loss_ae: 13.293793 loss_g: 50.735314 loss_d: -42.033276 time: 8.2 s\n",
      "[91 250] loss_ae: 13.598313 loss_g: 59.449159 loss_d: -47.540994 time: 8.3 s\n",
      "[91 300] loss_ae: 12.557753 loss_g: 55.958254 loss_d: -38.189083 time: 8.2 s\n",
      "[91 350] loss_ae: 14.049013 loss_g: 52.545094 loss_d: -45.959035 time: 8.4 s\n",
      "[91 400] loss_ae: 12.907855 loss_g: 53.555093 loss_d: -40.342655 time: 8.3 s\n",
      "[91 450] loss_ae: 14.864852 loss_g: 55.831797 loss_d: -47.233064 time: 8.3 s\n",
      "[91 500] loss_ae: 13.382371 loss_g: 59.827532 loss_d: -48.480555 time: 8.2 s\n",
      "[91 550] loss_ae: 11.656225 loss_g: 51.926031 loss_d: -38.324617 time: 8.3 s\n",
      "[91 600] loss_ae: 12.050986 loss_g: 55.661175 loss_d: -48.600255 time: 8.1 s\n",
      "[91 650] loss_ae: 13.698380 loss_g: 65.781259 loss_d: -56.262927 time: 8.2 s\n",
      "[91 700] loss_ae: 12.759610 loss_g: 48.159130 loss_d: -41.745538 time: 8.2 s\n",
      "[91 750] loss_ae: 12.398517 loss_g: 55.411965 loss_d: -40.986781 time: 8.2 s\n",
      "[91 800] loss_ae: 13.430332 loss_g: 54.706079 loss_d: -36.918768 time: 8.2 s\n",
      "[91 850] loss_ae: 12.927027 loss_g: 46.912151 loss_d: -45.973045 time: 8.3 s\n",
      "[91 900] loss_ae: 13.259121 loss_g: 47.615554 loss_d: -40.655480 time: 8.2 s\n",
      "[91 950] loss_ae: 13.106155 loss_g: 42.030670 loss_d: -39.067068 time: 8.2 s\n",
      "[91 1000] loss_ae: 11.756231 loss_g: 46.098935 loss_d: -31.632824 time: 8.2 s\n",
      "[91 1050] loss_ae: 12.658265 loss_g: 52.507990 loss_d: -47.674549 time: 8.3 s\n",
      "[91 1100] loss_ae: 13.310687 loss_g: 52.578350 loss_d: -50.133631 time: 8.2 s\n",
      "[91 1150] loss_ae: 13.364413 loss_g: 51.223624 loss_d: -45.218972 time: 8.2 s\n",
      "[91 1200] loss_ae: 13.923525 loss_g: 58.738194 loss_d: -50.603888 time: 8.2 s\n",
      "[91 1250] loss_ae: 13.361555 loss_g: 53.930591 loss_d: -47.503437 time: 8.3 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and you have to think that the parents that the parents are the ones that are the ones that are the ones that are the ones that </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  are they really are the ones that are the ones that are the ones </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  oh are you </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  you can do you can you can do a lot of <unk> and <unk> </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  do you do you have a favorite tv or do you have a favorite tv or do you have a favorite tv or do you do you </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  that they ' ve been trying to get out of the house and they ' re all over the place so </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  to have you have you have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  uh - huh </s>\n",
      "generate response:  you know i do </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  yeah so you have to do that for a while </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  i don ' t you don ' t have to have to have a you have to have a have you have you have you have you have you </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and to get out to get out of the house and we ' re not going to do that because we ' re not going to </s>\n",
      "true response:  but they </s>\n",
      "generate response:  they they have they have they have they have they have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know just to have to have a have you have you have you have you have you have you have you have you </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah it ' ll do it for a while and then they ' ll get it back again </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  there ' s always going to be a big place </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.428025, BLEU2 0.349636, BLEU3 0.295135, BLEU4 0.236885, inter_dist1 0.006654, inter_dist2 0.043220 avg_len 15.985222\n",
      " time: 146.0 s\n",
      "Done testing\n",
      "Epoch:  92\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[92 50] loss_ae: 12.023200 loss_g: 54.132193 loss_d: -39.256814 time: 8.3 s\n",
      "[92 100] loss_ae: 12.749739 loss_g: 67.571648 loss_d: -50.091847 time: 8.2 s\n",
      "[92 150] loss_ae: 13.205615 loss_g: 53.116529 loss_d: -43.308498 time: 8.4 s\n",
      "[92 200] loss_ae: 11.748077 loss_g: 55.257765 loss_d: -43.469074 time: 8.1 s\n",
      "[92 250] loss_ae: 13.176337 loss_g: 43.717695 loss_d: -43.807055 time: 8.4 s\n",
      "[92 300] loss_ae: 12.561374 loss_g: 55.944604 loss_d: -45.753173 time: 8.2 s\n",
      "[92 350] loss_ae: 13.779739 loss_g: 47.909948 loss_d: -33.540521 time: 8.3 s\n",
      "[92 400] loss_ae: 13.222088 loss_g: 39.474176 loss_d: -31.047313 time: 8.2 s\n",
      "[92 450] loss_ae: 14.021255 loss_g: 57.759601 loss_d: -44.782368 time: 8.3 s\n",
      "[92 500] loss_ae: 13.464866 loss_g: 48.919531 loss_d: -37.611422 time: 8.3 s\n",
      "[92 550] loss_ae: 15.540322 loss_g: 65.102169 loss_d: -50.883106 time: 8.2 s\n",
      "[92 600] loss_ae: 12.322820 loss_g: 44.463911 loss_d: -39.923296 time: 8.2 s\n",
      "[92 650] loss_ae: 13.111222 loss_g: 50.583793 loss_d: -37.214695 time: 8.1 s\n",
      "[92 700] loss_ae: 11.624567 loss_g: 50.102637 loss_d: -35.441527 time: 8.1 s\n",
      "[92 750] loss_ae: 11.641448 loss_g: 49.775209 loss_d: -35.686125 time: 8.4 s\n",
      "[92 800] loss_ae: 12.835449 loss_g: 55.289019 loss_d: -40.032063 time: 8.2 s\n",
      "[92 850] loss_ae: 15.061761 loss_g: 56.486545 loss_d: -45.769730 time: 8.4 s\n",
      "[92 900] loss_ae: 13.769767 loss_g: 49.560883 loss_d: -35.950749 time: 8.2 s\n",
      "[92 950] loss_ae: 12.625635 loss_g: 43.575745 loss_d: -38.330640 time: 8.3 s\n",
      "[92 1000] loss_ae: 14.305113 loss_g: 49.898581 loss_d: -42.758197 time: 8.2 s\n",
      "[92 1050] loss_ae: 13.995961 loss_g: 53.474510 loss_d: -41.788921 time: 8.3 s\n",
      "[92 1100] loss_ae: 13.446916 loss_g: 55.998649 loss_d: -50.159542 time: 8.1 s\n",
      "[92 1150] loss_ae: 13.125316 loss_g: 43.405378 loss_d: -37.206413 time: 8.2 s\n",
      "[92 1200] loss_ae: 12.801475 loss_g: 50.260623 loss_d: -41.493657 time: 8.1 s\n",
      "[92 1250] loss_ae: 10.285293 loss_g: 51.350676 loss_d: -32.837358 time: 8.3 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and you can they can you can you can you can you can you can have a you can have a you have a <unk> you have to have </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  they did they </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  they have a they have a they have a <unk> they have a they have a <unk> </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  you know they they ' re trying to do it and they ' re not they ' re not they ' re not they ' re not </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  and you have to be careful when you ' re in the house you ' re not going to have to do something like that you know </s>\n",
      "true response:  oh </s>\n",
      "generate response:  oh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  because they do not do anything to do they do they do they do they do they do they do they do it they do they do they do </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and so we have to go to the beach and then we ' ll go to the beach and then we ' ll go </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  or what ' s the name of it or something </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and you can get you can get a little bit of <unk> and you can ' t get a <unk> of a <unk> and you </s>\n",
      "true response:  but they </s>\n",
      "generate response:  i don ' t know they have they have they have they have a they have a they have a they have a they have a </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  oh uh - huh yeah </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know they have they have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  and </s>\n",
      "generate response:  we you have you have you have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  yeah </s>\n",
      "BLEU1 0.416013, BLEU2 0.339970, BLEU3 0.286713, BLEU4 0.229959, inter_dist1 0.006659, inter_dist2 0.043103 avg_len 15.315636\n",
      " time: 145.2 s\n",
      "Done testing\n",
      "Epoch:  93\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[93 50] loss_ae: 13.665412 loss_g: 45.190128 loss_d: -41.751227 time: 8.3 s\n",
      "[93 100] loss_ae: 12.472055 loss_g: 49.357347 loss_d: -34.592035 time: 8.2 s\n",
      "[93 150] loss_ae: 14.178116 loss_g: 47.215170 loss_d: -38.940848 time: 8.2 s\n",
      "[93 200] loss_ae: 12.934866 loss_g: 42.933617 loss_d: -30.380296 time: 8.2 s\n",
      "[93 250] loss_ae: 13.845847 loss_g: 47.884236 loss_d: -45.228212 time: 8.4 s\n",
      "[93 300] loss_ae: 13.260717 loss_g: 50.593638 loss_d: -40.633253 time: 8.1 s\n",
      "[93 350] loss_ae: 13.682681 loss_g: 47.030332 loss_d: -37.446493 time: 8.3 s\n",
      "[93 400] loss_ae: 12.661835 loss_g: 48.422098 loss_d: -41.491714 time: 8.1 s\n",
      "[93 450] loss_ae: 12.676230 loss_g: 54.202842 loss_d: -36.120315 time: 8.3 s\n",
      "[93 500] loss_ae: 12.695053 loss_g: 53.264969 loss_d: -43.005452 time: 8.4 s\n",
      "[93 550] loss_ae: 13.234683 loss_g: 48.512098 loss_d: -36.786761 time: 8.1 s\n",
      "[93 600] loss_ae: 11.810120 loss_g: 48.834926 loss_d: -40.177023 time: 8.0 s\n",
      "[93 650] loss_ae: 13.086818 loss_g: 51.214017 loss_d: -34.384845 time: 8.0 s\n",
      "[93 700] loss_ae: 12.290691 loss_g: 42.294243 loss_d: -29.046345 time: 7.9 s\n",
      "[93 750] loss_ae: 13.755247 loss_g: 50.627788 loss_d: -43.720459 time: 8.0 s\n",
      "[93 800] loss_ae: 12.873364 loss_g: 44.136738 loss_d: -39.783319 time: 7.9 s\n",
      "[93 850] loss_ae: 15.179030 loss_g: 46.797029 loss_d: -34.863009 time: 7.8 s\n",
      "[93 900] loss_ae: 13.455832 loss_g: 41.456135 loss_d: -39.599807 time: 7.9 s\n",
      "[93 950] loss_ae: 14.957437 loss_g: 49.026989 loss_d: -34.860416 time: 8.0 s\n",
      "[93 1000] loss_ae: 13.248387 loss_g: 54.192395 loss_d: -42.342018 time: 7.7 s\n",
      "[93 1050] loss_ae: 12.879216 loss_g: 43.692282 loss_d: -37.591507 time: 8.0 s\n",
      "[93 1100] loss_ae: 12.561299 loss_g: 50.146864 loss_d: -37.350633 time: 8.0 s\n",
      "[93 1150] loss_ae: 12.119085 loss_g: 41.110267 loss_d: -34.854844 time: 8.0 s\n",
      "[93 1200] loss_ae: 13.005178 loss_g: 43.500014 loss_d: -31.021024 time: 7.9 s\n",
      "[93 1250] loss_ae: 15.078805 loss_g: 52.112379 loss_d: -40.199845 time: 8.0 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  teach they have they have they have you know they have a they have a <unk> and they have a <unk> and they have a <unk> and they </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  they have but they have a lot of different foods </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  yeah </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  you know with <unk> you can have a <unk> you know you have to have a <unk> you know </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  yeah what ' s interesting is that you ' ve got to have a lot of <unk> and stuff like that and i ' m not sure </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  oh yeah </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and to get they have to have you know a <unk> or a <unk> or something like that you know i mean i ' ve heard </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  or they are not sure how long it ' s been it ' s been a long time since i ' ve been in </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  right </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  to they they have they have they have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  but they </s>\n",
      "generate response:  and they ' re not the same way here </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you can always if you if you don ' t like you don ' t like you don ' t like you don ' t like to </s>\n",
      "true response:  and </s>\n",
      "generate response:  they have they have they have they have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.431501, BLEU2 0.354420, BLEU3 0.300215, BLEU4 0.241603, inter_dist1 0.006665, inter_dist2 0.041158 avg_len 15.602080\n",
      " time: 150.4 s\n",
      "Done testing\n",
      "Epoch:  94\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[94 50] loss_ae: 13.243767 loss_g: 49.550146 loss_d: -43.486705 time: 8.3 s\n",
      "[94 100] loss_ae: 13.150654 loss_g: 46.533107 loss_d: -35.790742 time: 8.2 s\n",
      "[94 150] loss_ae: 13.153622 loss_g: 42.422799 loss_d: -35.869129 time: 8.2 s\n",
      "[94 200] loss_ae: 12.204491 loss_g: 50.382708 loss_d: -42.009798 time: 8.1 s\n",
      "[94 250] loss_ae: 12.966097 loss_g: 53.045908 loss_d: -37.700955 time: 8.2 s\n",
      "[94 300] loss_ae: 14.282128 loss_g: 56.940031 loss_d: -40.741536 time: 8.1 s\n",
      "[94 350] loss_ae: 12.788376 loss_g: 49.573866 loss_d: -47.314809 time: 8.3 s\n",
      "[94 400] loss_ae: 12.505207 loss_g: 48.631026 loss_d: -35.620051 time: 8.2 s\n",
      "[94 450] loss_ae: 13.729787 loss_g: 46.863953 loss_d: -35.382649 time: 8.4 s\n",
      "[94 500] loss_ae: 11.915376 loss_g: 48.316565 loss_d: -45.103538 time: 8.2 s\n",
      "[94 550] loss_ae: 13.417860 loss_g: 49.527691 loss_d: -36.852037 time: 8.2 s\n",
      "[94 600] loss_ae: 15.518371 loss_g: 52.435492 loss_d: -50.042309 time: 8.1 s\n",
      "[94 650] loss_ae: 12.285804 loss_g: 45.812637 loss_d: -34.268320 time: 8.2 s\n",
      "[94 700] loss_ae: 12.073037 loss_g: 40.032445 loss_d: -31.535124 time: 8.2 s\n",
      "[94 750] loss_ae: 14.339345 loss_g: 48.535797 loss_d: -41.661977 time: 8.1 s\n",
      "[94 800] loss_ae: 14.667251 loss_g: 40.983006 loss_d: -31.244239 time: 8.3 s\n",
      "[94 850] loss_ae: 13.595322 loss_g: 52.048304 loss_d: -44.245786 time: 8.1 s\n",
      "[94 900] loss_ae: 12.950427 loss_g: 48.474621 loss_d: -38.525080 time: 8.2 s\n",
      "[94 950] loss_ae: 13.038982 loss_g: 40.947576 loss_d: -33.263997 time: 8.2 s\n",
      "[94 1000] loss_ae: 12.798998 loss_g: 46.346244 loss_d: -33.212485 time: 8.3 s\n",
      "[94 1050] loss_ae: 14.223211 loss_g: 47.750300 loss_d: -42.977296 time: 8.2 s\n",
      "[94 1100] loss_ae: 13.121552 loss_g: 45.560074 loss_d: -33.312688 time: 8.4 s\n",
      "[94 1150] loss_ae: 12.539119 loss_g: 48.539409 loss_d: -35.783524 time: 8.3 s\n",
      "[94 1200] loss_ae: 14.516877 loss_g: 42.957485 loss_d: -32.954554 time: 8.3 s\n",
      "[94 1250] loss_ae: 13.296460 loss_g: 51.021045 loss_d: -46.568558 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  they they have they have they have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  we do we </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  and there ' s a lot of things that you can do with it you know </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  if they do not do it they don ' t do it they don ' t do it they don ' t do it they don ' t </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  i don ' t have to do </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  oh </s>\n",
      "generate response:  to get out to do you know the <unk> and the <unk> and the <unk> and all that so </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and and they just they just they just don ' t want to do that i mean i think it ' s </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  well yeah they have they have they have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  or they are they are they are they are they are they are they are they are they are they are they are they are they are they are they\n",
      "true response:  but they </s>\n",
      "generate response:  you know they use that for a couple of years </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  of course yeah </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  and have you have you have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  and </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  and they have they have you have you have you have you have you have you have you have you have you have you have you have you </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  yeah </s>\n",
      "BLEU1 0.416465, BLEU2 0.341901, BLEU3 0.289683, BLEU4 0.233121, inter_dist1 0.006232, inter_dist2 0.039975 avg_len 16.453384\n",
      " time: 146.0 s\n",
      "Done testing\n",
      "Epoch:  95\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[95 50] loss_ae: 12.453205 loss_g: 48.503872 loss_d: -38.375802 time: 8.2 s\n",
      "[95 100] loss_ae: 14.280747 loss_g: 48.166006 loss_d: -43.796104 time: 8.1 s\n",
      "[95 150] loss_ae: 14.429789 loss_g: 47.282925 loss_d: -38.900629 time: 8.1 s\n",
      "[95 200] loss_ae: 12.630201 loss_g: 44.498924 loss_d: -34.217503 time: 8.4 s\n",
      "[95 250] loss_ae: 13.928371 loss_g: 51.897075 loss_d: -37.886286 time: 8.2 s\n",
      "[95 300] loss_ae: 12.648220 loss_g: 38.874919 loss_d: -30.364286 time: 8.3 s\n",
      "[95 350] loss_ae: 12.853674 loss_g: 46.016180 loss_d: -38.758909 time: 8.4 s\n",
      "[95 400] loss_ae: 13.192856 loss_g: 43.964400 loss_d: -34.557817 time: 8.2 s\n",
      "[95 450] loss_ae: 11.663925 loss_g: 48.810604 loss_d: -39.046921 time: 8.2 s\n",
      "[95 500] loss_ae: 13.038105 loss_g: 50.968853 loss_d: -42.486969 time: 8.2 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95 550] loss_ae: 11.922449 loss_g: 44.677827 loss_d: -36.697819 time: 8.2 s\n",
      "[95 600] loss_ae: 12.879628 loss_g: 51.886022 loss_d: -39.424097 time: 8.2 s\n",
      "[95 650] loss_ae: 13.483893 loss_g: 44.629009 loss_d: -32.939882 time: 8.3 s\n",
      "[95 700] loss_ae: 14.304925 loss_g: 42.675052 loss_d: -28.517915 time: 8.3 s\n",
      "[95 750] loss_ae: 13.490675 loss_g: 36.880632 loss_d: -27.012731 time: 8.3 s\n",
      "[95 800] loss_ae: 13.237393 loss_g: 38.154242 loss_d: -34.201610 time: 8.3 s\n",
      "[95 850] loss_ae: 12.780832 loss_g: 43.941451 loss_d: -39.361835 time: 8.3 s\n",
      "[95 900] loss_ae: 13.804437 loss_g: 41.996913 loss_d: -33.601454 time: 8.2 s\n",
      "[95 950] loss_ae: 13.599225 loss_g: 52.486835 loss_d: -40.792955 time: 8.3 s\n",
      "[95 1000] loss_ae: 14.553553 loss_g: 48.572036 loss_d: -38.068780 time: 8.2 s\n",
      "[95 1050] loss_ae: 14.294220 loss_g: 47.348842 loss_d: -35.610943 time: 8.4 s\n",
      "[95 1100] loss_ae: 14.020874 loss_g: 47.286647 loss_d: -35.619220 time: 8.1 s\n",
      "[95 1150] loss_ae: 14.100567 loss_g: 45.994283 loss_d: -35.708619 time: 8.2 s\n",
      "[95 1200] loss_ae: 13.438382 loss_g: 48.350479 loss_d: -37.460635 time: 8.2 s\n",
      "[95 1250] loss_ae: 12.417911 loss_g: 49.637887 loss_d: -28.249533 time: 8.1 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  you can get you can get there ' s no way you can get to where you can get a lot of kids and stuff and </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  you have to have you have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  yeah </s>\n",
      "generate response:  you know you can you can you can you can have a you have a you have a you have a you have a <unk> you </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  you can you can do it with the with the <unk> of the <unk> and the <unk> of the <unk> and </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  well i ' ve read a lot of people who are just trying to think of </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  you know when you were talking about the <unk> of the <unk> and the other thing was that you know that ' s what </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  oh yeah they do </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah you know you ' re just </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  we have a lot of <unk> and we have a lot of <unk> and we have a lot of <unk> and we have a </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  and they ' re not they ' re not going to do that they ' re not going to </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  you can you can you can you can have you can you can have a you can have a you can have a you can have a </s>\n",
      "true response:  but they </s>\n",
      "generate response:  they have they have they have they have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  yeah i don ' t </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  you don ' you have you have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  um - hum </s>\n",
      "generate response:  she has a lot of <unk> in a lot of a lot of a lot of people have to have a lot of people in </s>\n",
      "BLEU1 0.414642, BLEU2 0.341692, BLEU3 0.289540, BLEU4 0.232798, inter_dist1 0.005768, inter_dist2 0.035021 avg_len 18.535669\n",
      " time: 144.9 s\n",
      "Done testing\n",
      "Epoch:  96\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[96 50] loss_ae: 13.861538 loss_g: 48.654228 loss_d: -40.330705 time: 8.1 s\n",
      "[96 100] loss_ae: 13.135711 loss_g: 44.266074 loss_d: -32.935054 time: 8.3 s\n",
      "[96 150] loss_ae: 13.868488 loss_g: 44.031878 loss_d: -30.688263 time: 8.1 s\n",
      "[96 200] loss_ae: 12.519646 loss_g: 39.989147 loss_d: -32.488790 time: 8.4 s\n",
      "[96 250] loss_ae: 13.514100 loss_g: 42.044512 loss_d: -33.904995 time: 8.2 s\n",
      "[96 300] loss_ae: 13.598230 loss_g: 39.028220 loss_d: -34.819393 time: 8.4 s\n",
      "[96 350] loss_ae: 13.540200 loss_g: 44.822343 loss_d: -42.273676 time: 8.1 s\n",
      "[96 400] loss_ae: 12.813298 loss_g: 42.007064 loss_d: -37.760422 time: 8.3 s\n",
      "[96 450] loss_ae: 12.844485 loss_g: 40.341481 loss_d: -31.661087 time: 8.2 s\n",
      "[96 500] loss_ae: 14.656782 loss_g: 47.960588 loss_d: -38.565882 time: 8.2 s\n",
      "[96 550] loss_ae: 14.333656 loss_g: 42.256603 loss_d: -36.631509 time: 8.2 s\n",
      "[96 600] loss_ae: 13.809564 loss_g: 48.052095 loss_d: -34.486192 time: 8.3 s\n",
      "[96 650] loss_ae: 13.639323 loss_g: 49.149968 loss_d: -38.159932 time: 8.3 s\n",
      "[96 700] loss_ae: 13.362489 loss_g: 43.245671 loss_d: -43.768769 time: 8.3 s\n",
      "[96 750] loss_ae: 11.988305 loss_g: 46.184446 loss_d: -36.095366 time: 8.2 s\n",
      "[96 800] loss_ae: 12.783821 loss_g: 43.415948 loss_d: -32.301332 time: 8.2 s\n",
      "[96 850] loss_ae: 13.051334 loss_g: 36.578565 loss_d: -38.814451 time: 8.3 s\n",
      "[96 900] loss_ae: 14.370446 loss_g: 47.797384 loss_d: -31.377462 time: 8.2 s\n",
      "[96 950] loss_ae: 14.788310 loss_g: 54.781114 loss_d: -47.905402 time: 8.3 s\n",
      "[96 1000] loss_ae: 12.264059 loss_g: 43.920394 loss_d: -38.481017 time: 8.2 s\n",
      "[96 1050] loss_ae: 14.719379 loss_g: 53.059135 loss_d: -35.745945 time: 8.3 s\n",
      "[96 1100] loss_ae: 13.878913 loss_g: 44.880126 loss_d: -34.593972 time: 8.1 s\n",
      "[96 1150] loss_ae: 13.091990 loss_g: 47.141585 loss_d: -31.930879 time: 8.4 s\n",
      "[96 1200] loss_ae: 15.262038 loss_g: 45.769583 loss_d: -28.777565 time: 8.2 s\n",
      "[96 1250] loss_ae: 13.011311 loss_g: 44.748636 loss_d: -30.039758 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  you know he he ' s been there for a long time and he ' s been there for a long time and he </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh did you </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  they they are they are they are they are they are they are they are they are they are they are they are they are they are </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  it ' ll do not do it if they do it ' s not that they do it they do it they do it </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh do you do you </s>\n",
      "true response:  oh </s>\n",
      "generate response:  and it would be nice if </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  you know they just they just they just they just they just they just <unk> you know they just don ' t have to </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and you can always do it and you don ' t like to do it but you don ' t have to worry about </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  right sure </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  they are they are they are they are they are they are they are they are you know what they ' re doing and they ' re not doing that\n",
      "true response:  but they </s>\n",
      "generate response:  and you can do it with you there and there ' s no way </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you have to have a you have to have a you have a you have a you have a </s>\n",
      "true response:  and </s>\n",
      "generate response:  they are they are they are they are they are they are they are you know a lot of times they are they are they are they are </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  yeah you can do you can ' t get anything like that </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.417832, BLEU2 0.342708, BLEU3 0.290364, BLEU4 0.233675, inter_dist1 0.006277, inter_dist2 0.039266 avg_len 16.421273\n",
      " time: 145.5 s\n",
      "Done testing\n",
      "Epoch:  97\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[97 50] loss_ae: 14.037196 loss_g: 49.571347 loss_d: -40.177725 time: 8.2 s\n",
      "[97 100] loss_ae: 13.756247 loss_g: 46.256645 loss_d: -44.817757 time: 8.2 s\n",
      "[97 150] loss_ae: 14.419084 loss_g: 42.216612 loss_d: -37.051445 time: 8.3 s\n",
      "[97 200] loss_ae: 12.549801 loss_g: 50.281040 loss_d: -35.838866 time: 8.2 s\n",
      "[97 250] loss_ae: 12.767434 loss_g: 44.404250 loss_d: -30.684055 time: 8.1 s\n",
      "[97 300] loss_ae: 12.529663 loss_g: 51.332668 loss_d: -35.061225 time: 8.3 s\n",
      "[97 350] loss_ae: 12.730604 loss_g: 42.512904 loss_d: -39.435215 time: 8.4 s\n",
      "[97 400] loss_ae: 14.133130 loss_g: 42.796466 loss_d: -33.981945 time: 8.3 s\n",
      "[97 450] loss_ae: 15.651160 loss_g: 50.871341 loss_d: -35.516306 time: 8.4 s\n",
      "[97 500] loss_ae: 13.942762 loss_g: 47.096637 loss_d: -39.884758 time: 8.1 s\n",
      "[97 550] loss_ae: 13.549023 loss_g: 38.822951 loss_d: -26.083848 time: 8.3 s\n",
      "[97 600] loss_ae: 14.038144 loss_g: 49.980895 loss_d: -38.779637 time: 8.3 s\n",
      "[97 650] loss_ae: 13.222221 loss_g: 39.259703 loss_d: -23.212313 time: 8.4 s\n",
      "[97 700] loss_ae: 13.413888 loss_g: 44.688374 loss_d: -34.430852 time: 8.2 s\n",
      "[97 750] loss_ae: 13.903417 loss_g: 42.379391 loss_d: -29.296662 time: 8.3 s\n",
      "[97 800] loss_ae: 13.977534 loss_g: 46.889291 loss_d: -34.609868 time: 8.1 s\n",
      "[97 850] loss_ae: 12.223791 loss_g: 47.767374 loss_d: -33.329201 time: 8.3 s\n",
      "[97 900] loss_ae: 12.455048 loss_g: 53.278807 loss_d: -34.910232 time: 8.1 s\n",
      "[97 950] loss_ae: 12.285090 loss_g: 46.647858 loss_d: -30.838835 time: 8.4 s\n",
      "[97 1000] loss_ae: 12.877905 loss_g: 47.674500 loss_d: -35.504431 time: 8.2 s\n",
      "[97 1050] loss_ae: 12.925511 loss_g: 48.506193 loss_d: -40.234891 time: 8.3 s\n",
      "[97 1100] loss_ae: 15.320187 loss_g: 42.468217 loss_d: -39.436651 time: 8.2 s\n",
      "[97 1150] loss_ae: 13.673591 loss_g: 48.250841 loss_d: -47.166003 time: 8.3 s\n",
      "[97 1200] loss_ae: 13.940291 loss_g: 52.133613 loss_d: -44.092758 time: 8.1 s\n",
      "[97 1250] loss_ae: 14.374347 loss_g: 46.146001 loss_d: -32.300646 time: 8.3 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and there ' s a lot of women in the work force and i think that ' s a lot of the <unk> of the </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  are you </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  you know with people who are going to do it you know they ' re not going to do it but they don ' </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  or if you ' re not going to do it you don ' t have to do </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  it can be really nice to have it done too well you know </s>\n",
      "true response:  oh </s>\n",
      "generate response:  you know they were talking about the <unk> or something like that </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  they they have they have they have you have you have you have you have you have you have you have you have you have you </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and they have they have you have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and they have they have you have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  but they </s>\n",
      "generate response:  i really don ' t </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  and </s>\n",
      "generate response:  probably just about ten minutes </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  i ' ll be darned </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.423230, BLEU2 0.348378, BLEU3 0.295919, BLEU4 0.238494, inter_dist1 0.005851, inter_dist2 0.035133 avg_len 17.212735\n",
      " time: 144.9 s\n",
      "Done testing\n",
      "Epoch:  98\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[98 50] loss_ae: 12.830398 loss_g: 42.169614 loss_d: -34.926792 time: 8.3 s\n",
      "[98 100] loss_ae: 12.048044 loss_g: 48.933909 loss_d: -43.707677 time: 8.2 s\n",
      "[98 150] loss_ae: 13.713346 loss_g: 47.576138 loss_d: -29.289276 time: 8.5 s\n",
      "[98 200] loss_ae: 13.224995 loss_g: 45.513329 loss_d: -38.399466 time: 8.4 s\n",
      "[98 250] loss_ae: 14.427649 loss_g: 55.219229 loss_d: -40.580226 time: 7.9 s\n",
      "[98 300] loss_ae: 11.453057 loss_g: 49.889062 loss_d: -41.515501 time: 7.9 s\n",
      "[98 350] loss_ae: 14.209673 loss_g: 42.850549 loss_d: -35.214382 time: 8.0 s\n",
      "[98 400] loss_ae: 13.191998 loss_g: 41.108039 loss_d: -34.224219 time: 7.9 s\n",
      "[98 450] loss_ae: 13.689629 loss_g: 46.367366 loss_d: -37.340073 time: 8.0 s\n",
      "[98 500] loss_ae: 14.569206 loss_g: 45.800285 loss_d: -35.779945 time: 7.9 s\n",
      "[98 550] loss_ae: 13.161681 loss_g: 43.063070 loss_d: -36.750798 time: 7.9 s\n",
      "[98 600] loss_ae: 12.596378 loss_g: 41.366838 loss_d: -28.195362 time: 8.0 s\n",
      "[98 650] loss_ae: 15.062845 loss_g: 36.361562 loss_d: -34.466365 time: 8.0 s\n",
      "[98 700] loss_ae: 13.629710 loss_g: 43.359335 loss_d: -31.848749 time: 8.0 s\n",
      "[98 750] loss_ae: 12.972183 loss_g: 51.417245 loss_d: -34.561806 time: 7.8 s\n",
      "[98 800] loss_ae: 12.009561 loss_g: 39.789588 loss_d: -35.278272 time: 8.0 s\n",
      "[98 850] loss_ae: 12.875183 loss_g: 39.574107 loss_d: -23.064174 time: 7.9 s\n",
      "[98 900] loss_ae: 13.387837 loss_g: 48.405506 loss_d: -40.226880 time: 7.9 s\n",
      "[98 950] loss_ae: 14.175586 loss_g: 44.322336 loss_d: -38.287862 time: 8.0 s\n",
      "[98 1000] loss_ae: 13.316257 loss_g: 46.628669 loss_d: -30.536362 time: 8.0 s\n",
      "[98 1050] loss_ae: 13.933003 loss_g: 46.753642 loss_d: -36.182663 time: 7.9 s\n",
      "[98 1100] loss_ae: 12.973353 loss_g: 44.823865 loss_d: -32.833352 time: 7.8 s\n",
      "[98 1150] loss_ae: 15.436329 loss_g: 46.152949 loss_d: -36.823748 time: 7.9 s\n",
      "[98 1200] loss_ae: 14.683568 loss_g: 38.950820 loss_d: -31.067332 time: 7.9 s\n",
      "[98 1250] loss_ae: 12.920188 loss_g: 37.280113 loss_d: -25.897658 time: 8.0 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and but they don ' t have any of the <unk> of the school system and </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  <unk> </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  you know you can ' t you can ' t you can ' t you can ' t do it </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  to do they do not do it but i don ' t think they ' ll do it or not i don ' t think they should </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  something like that </s>\n",
      "generate response:  they are they are they are they are they are they are they are they are they are they are they are they are they are they are they are\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  and you can always get you can get them out of there and you can ' t get them in the house and </s>\n",
      "true response:  oh </s>\n",
      "generate response:  if they do not do they do they do they do they do they do they do it outside of the house and they do that they </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and have been having to have a lot of <unk> and i have a lot of <unk> and i have a lot of <unk> and i have </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  they are they are they are they are they are they are they are they are they are they are they are they are they are they are they are\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  and there ' s there ' s not there ' s not there ' s not there ' s not there ' s not there ' s </s>\n",
      "true response:  but they </s>\n",
      "generate response:  oh they do have they have a big problem </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  they don ' t they don ' t they don ' t they don ' t they don ' t they don ' t they don ' t they don\n",
      "true response:  and </s>\n",
      "generate response:  they they they they they they they don ' t they don ' t they don ' t they don ' t they don ' t they don ' t\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  because you can go to the lake and it ' s a nice place to go to the beach and the mountains </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum hum </s>\n",
      "BLEU1 0.420835, BLEU2 0.346925, BLEU3 0.294784, BLEU4 0.237612, inter_dist1 0.005697, inter_dist2 0.035992 avg_len 17.292282\n",
      " time: 147.9 s\n",
      "Done testing\n",
      "Epoch:  99\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[99 50] loss_ae: 13.722922 loss_g: 37.576937 loss_d: -29.751738 time: 8.3 s\n",
      "[99 100] loss_ae: 14.403838 loss_g: 38.900196 loss_d: -35.033523 time: 8.2 s\n",
      "[99 150] loss_ae: 13.253735 loss_g: 41.186297 loss_d: -34.225165 time: 8.2 s\n",
      "[99 200] loss_ae: 13.962884 loss_g: 45.764401 loss_d: -36.591498 time: 8.3 s\n",
      "[99 250] loss_ae: 12.297796 loss_g: 43.503153 loss_d: -33.846220 time: 8.1 s\n",
      "[99 300] loss_ae: 13.327763 loss_g: 43.449869 loss_d: -29.857296 time: 8.3 s\n",
      "[99 350] loss_ae: 12.708196 loss_g: 37.583212 loss_d: -35.871593 time: 8.2 s\n",
      "[99 400] loss_ae: 12.639315 loss_g: 39.655715 loss_d: -30.261443 time: 8.4 s\n",
      "[99 450] loss_ae: 13.928290 loss_g: 44.974278 loss_d: -29.650429 time: 8.1 s\n",
      "[99 500] loss_ae: 12.222240 loss_g: 45.302901 loss_d: -32.213882 time: 8.4 s\n",
      "[99 550] loss_ae: 13.500256 loss_g: 38.667065 loss_d: -31.588985 time: 8.1 s\n",
      "[99 600] loss_ae: 14.036475 loss_g: 41.529242 loss_d: -34.320028 time: 8.4 s\n",
      "[99 650] loss_ae: 13.739409 loss_g: 44.228602 loss_d: -35.000885 time: 8.1 s\n",
      "[99 700] loss_ae: 14.015717 loss_g: 39.761647 loss_d: -35.014349 time: 8.3 s\n",
      "[99 750] loss_ae: 11.091466 loss_g: 50.061139 loss_d: -37.798402 time: 8.0 s\n",
      "[99 800] loss_ae: 13.153721 loss_g: 39.744141 loss_d: -31.775320 time: 8.3 s\n",
      "[99 850] loss_ae: 14.411052 loss_g: 38.611667 loss_d: -43.744206 time: 8.1 s\n",
      "[99 900] loss_ae: 12.109140 loss_g: 43.810221 loss_d: -36.208125 time: 8.4 s\n",
      "[99 950] loss_ae: 14.604899 loss_g: 33.580937 loss_d: -22.237652 time: 8.2 s\n",
      "[99 1000] loss_ae: 14.250020 loss_g: 48.490627 loss_d: -33.386972 time: 8.3 s\n",
      "[99 1050] loss_ae: 13.737320 loss_g: 40.738217 loss_d: -25.576605 time: 8.1 s\n",
      "[99 1100] loss_ae: 13.335291 loss_g: 39.996184 loss_d: -38.623601 time: 8.2 s\n",
      "[99 1150] loss_ae: 12.675573 loss_g: 40.351137 loss_d: -31.891791 time: 8.2 s\n",
      "[99 1200] loss_ae: 14.342731 loss_g: 39.601625 loss_d: -28.320829 time: 8.3 s\n",
      "[99 1250] loss_ae: 15.455261 loss_g: 36.634586 loss_d: -30.249845 time: 8.2 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  you know they can ' t do anything else they ' re going to do it and they ' re going to be </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  they are they are they are they are they are they are they are they are they are they are they are they are they </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  and if it ' s not going to be a good idea to have a family reunion </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  they can they can they can they can they can they can they can they can they can they can they can they can they can they can </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  or they can do to make it a little bit </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  i don ' t know we have a lot of <unk> and <unk> and <unk> and </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  you can you can you can have you can you can have you can you have you have you have you have you have you have you have you have\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and we ' ve been trying to get some things to do </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  they have they have they have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  to be able to do it because there are there are there are there are some really nice places where you can go and do it </s>\n",
      "true response:  but they </s>\n",
      "generate response:  you know you can ' t they can ' t they can ' t they can ' t they can ' t they </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  they ' ll have you have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  you ' ve never had to do it i mean i ' ve never been there for a long time </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  but it can ' t be there to do that there ' s a lot of things that she can do </s>\n",
      "BLEU1 0.414773, BLEU2 0.341995, BLEU3 0.290781, BLEU4 0.234438, inter_dist1 0.005803, inter_dist2 0.037889 avg_len 17.608101\n",
      " time: 145.6 s\n",
      "Done testing\n",
      "Epoch:  100\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[100 50] loss_ae: 14.378461 loss_g: 45.317524 loss_d: -43.113633 time: 8.3 s\n",
      "[100 100] loss_ae: 12.106127 loss_g: 43.043635 loss_d: -31.553574 time: 8.1 s\n",
      "[100 150] loss_ae: 13.824179 loss_g: 50.317799 loss_d: -35.438769 time: 8.3 s\n",
      "[100 200] loss_ae: 13.699642 loss_g: 43.362489 loss_d: -36.934854 time: 8.2 s\n",
      "[100 250] loss_ae: 16.310794 loss_g: 46.596096 loss_d: -35.844290 time: 8.3 s\n",
      "[100 300] loss_ae: 12.770677 loss_g: 46.078737 loss_d: -32.156803 time: 8.2 s\n",
      "[100 350] loss_ae: 11.503837 loss_g: 44.541490 loss_d: -30.562015 time: 8.2 s\n",
      "[100 400] loss_ae: 13.907003 loss_g: 46.945399 loss_d: -35.857702 time: 8.2 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100 450] loss_ae: 15.087776 loss_g: 45.159494 loss_d: -34.218481 time: 8.5 s\n",
      "[100 500] loss_ae: 14.233923 loss_g: 43.753378 loss_d: -45.501710 time: 8.4 s\n",
      "[100 550] loss_ae: 13.344636 loss_g: 51.137384 loss_d: -37.833727 time: 8.4 s\n",
      "[100 600] loss_ae: 14.683267 loss_g: 42.967670 loss_d: -32.429834 time: 8.3 s\n",
      "[100 650] loss_ae: 13.145862 loss_g: 40.106173 loss_d: -34.415643 time: 8.4 s\n",
      "[100 700] loss_ae: 13.071185 loss_g: 41.093087 loss_d: -31.705087 time: 8.2 s\n",
      "[100 750] loss_ae: 12.726765 loss_g: 46.139163 loss_d: -36.457487 time: 8.4 s\n",
      "[100 800] loss_ae: 13.525625 loss_g: 39.800800 loss_d: -26.833157 time: 8.2 s\n",
      "[100 850] loss_ae: 13.825599 loss_g: 43.555780 loss_d: -27.418980 time: 8.5 s\n",
      "[100 900] loss_ae: 14.398101 loss_g: 42.999491 loss_d: -26.528606 time: 8.2 s\n",
      "[100 950] loss_ae: 13.477579 loss_g: 37.928825 loss_d: -31.887104 time: 8.4 s\n",
      "[100 1000] loss_ae: 13.719309 loss_g: 34.090738 loss_d: -34.493382 time: 8.3 s\n",
      "[100 1050] loss_ae: 12.965043 loss_g: 42.403228 loss_d: -36.693787 time: 8.3 s\n",
      "[100 1100] loss_ae: 14.205610 loss_g: 39.150735 loss_d: -30.078357 time: 8.3 s\n",
      "[100 1150] loss_ae: 11.774283 loss_g: 36.732069 loss_d: -29.576833 time: 8.3 s\n",
      "[100 1200] loss_ae: 14.224030 loss_g: 39.053129 loss_d: -31.991455 time: 8.3 s\n",
      "[100 1250] loss_ae: 12.544290 loss_g: 46.145739 loss_d: -34.298644 time: 8.3 s\n",
      "Evaluating....\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  there ' s they have to be there for them </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  are they do they do not do they do they do it do they do they do they do it do you do you do you do you do </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  and they have they have they have you have you have you have you have you have you have you have you have you have you have you have you\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  things are <unk> <unk> </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  and they don ' t they don ' t they don ' t they don ' t they don ' t they don ' t </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  do they have that they have to do that for the most part </s>\n",
      "true response:  oh </s>\n",
      "generate response:  yeah you got to have a boat there too </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  because they are they are they are they ' re not so good they ' re not they ' re not they ' re not </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  they have they have they have you have you have you have you have you have you have you have you have you have you have you have you have\n",
      "true response:  uh - huh </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  with their children are more than they are now they ' re not taught as much as they used to be in the same age they </s>\n",
      "true response:  but they </s>\n",
      "generate response:  they just they just they just don ' t have the time </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  oh yeah </s>\n",
      "true response:  and </s>\n",
      "generate response:  doesn ' t do anything to do it but they don ' t do anything they don ' t do it they don ' t do it </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  you can you can you can you can you can have you can you have you have you have you have you have you have you have you have you\n",
      "true response:  um - hum </s>\n",
      "generate response:  and not really </s>\n",
      "BLEU1 0.406111, BLEU2 0.334168, BLEU3 0.283186, BLEU4 0.227818, inter_dist1 0.005829, inter_dist2 0.036221 avg_len 16.902025\n",
      " time: 147.4 s\n",
      "Done testing\n"
     ]
    }
   ],
   "source": [
    "metrics=Metrics(corpus.word2vec)\n",
    "model = DialogWAE(config, n_tokens)\n",
    "if corpus.word2vec is not None:\n",
    "    print(\"Loaded word2vec\")\n",
    "    model.embedder.weight.data.copy_(torch.from_numpy(corpus.word2vec))\n",
    "    model.embedder.weight.data[0].fill_(0)\n",
    "model.to(DEVICE)\n",
    "model.zero_grad()\n",
    "print_every = 50\n",
    "best_state = None\n",
    "max_metric = 0\n",
    "for epoch in range(100):\n",
    "    print('Epoch: ', epoch+1)\n",
    "    train_loader.epoch_init(32, config['diaglen'], 1, shuffle=True)\n",
    "    n_iters=train_loader.num_batch\n",
    "    loss_ae = 0.0\n",
    "    loss_g = 0.0\n",
    "    loss_d = 0.0\n",
    "    epoch_begin = time()\n",
    "    batch_count = 0\n",
    "    batch_begin_time = time()\n",
    "    total_train_batch = 0 # 记录训练的样本数量\n",
    "    total_valid_batch = 0 # 记录测试的样本数量\n",
    "    while True:\n",
    "        model.train()\n",
    "        loss_records=[]\n",
    "        batch = train_loader.next_batch()\n",
    "        total_train_batch += 32\n",
    "        if batch is None:\n",
    "#         if batch is None or total_train_batch >= 1000: # end of epoch\n",
    "            break\n",
    "        context, context_lens, utt_lens, floors,_,_,_,response,res_lens,_ = batch\n",
    "        context, utt_lens = context[:,:,1:], utt_lens-1 # remove the sos token in the context and reduce the context length\n",
    "        context, context_lens, utt_lens, floors, response, res_lens\\\n",
    "                = gVar(context), gVar(context_lens), gVar(utt_lens), gData(floors), gVar(response), gVar(res_lens)\n",
    "        loss_1 = model.train_AE(context, context_lens, utt_lens, floors, response, res_lens)\n",
    "        loss_ae += float(loss_1[0][1])\n",
    "        loss_2 = model.train_G(context, context_lens, utt_lens, floors, response, res_lens)\n",
    "        loss_g += float(loss_2[0][1])\n",
    "        \n",
    "        for i in range(config['n_iters_d']):# train discriminator/critic\n",
    "            loss_3 = model.train_D(context, context_lens, utt_lens, floors, response, res_lens)  \n",
    "            if i==0:\n",
    "                loss_d += float(loss_3[0][1])\n",
    "            if i==config['n_iters_d']-1:\n",
    "                break\n",
    "            batch = train_loader.next_batch()\n",
    "            if batch is None: # end of epoch\n",
    "                break\n",
    "            context, context_lens, utt_lens, floors,_,_,_,response,res_lens,_ = batch\n",
    "            context, utt_lens = context[:,:,1:], utt_lens-1 # remove the sos token in the context and reduce the context length\n",
    "            context, context_lens, utt_lens, floors, response, res_lens\\\n",
    "                = gVar(context), gVar(context_lens), gVar(utt_lens), gData(floors), gVar(response), gVar(res_lens)                      \n",
    "        \n",
    "        batch_count += 1\n",
    "        if batch_count % print_every == 0:\n",
    "            print_flush('[%d %d] loss_ae: %.6f loss_g: %.6f loss_d: %.6f time: %.1f s' %\n",
    "                  (epoch + 1, batch_count, np.exp(loss_ae / print_every), loss_g / print_every,\n",
    "                    loss_d / print_every, time() - batch_begin_time))\n",
    "            loss_ae = 0.0\n",
    "            loss_g = 0.0\n",
    "            loss_d = 0.0\n",
    "            batch_begin_time = time()\n",
    "    print_flush(\"Evaluating....\")\n",
    "#     valid_loader.epoch_init(20, config['diaglen'], 1, shuffle=False)\n",
    "#     loss_valid = valid_small(valid_loader)\n",
    "#     valid_result.append(F1)\n",
    "#     print_flush('*'*60)\n",
    "#     print_flush('[epoch %d]. loss: %.6f time: %.1f s'%(epoch+1, np.exp(loss_valid), time()-epoch_begin))\n",
    "#     print_flush('*'*60)\n",
    "#     print_flush(\"testing....\")\n",
    "#     test_loader.epoch_init(1, config['diaglen'], 1, shuffle=False)\n",
    "#     loss_valid = valid(test_loader)\n",
    "#     print_flush('*'*60)\n",
    "#     print_flush('[epoch %d]. loss: %.6f time: %.1f s'%(epoch+1, np.exp(loss_valid), time()-epoch_begin))\n",
    "#     print_flush('*'*60)\n",
    "    if (epoch+1) > 20:\n",
    "        f_eval = open(\"../result/{}/{}/epoch{}.txt\".format('DialogWAE', 'SWDA', epoch), \"w\")\n",
    "        recall_bleu, bow_extrema, bow_avg, bow_greedy, inter_dist1, inter_dist2, avg_len\\\n",
    "         =evaluate(model, metrics, test_loader, vocab, ivocab, f_eval, repeat=1)\n",
    "    epoch_begin = time()\n",
    "#     if F1 > max_metric:\n",
    "#         best_state = model.state_dict()\n",
    "#         max_metric = F1\n",
    "#         print_flush(\"save model...\")\n",
    "#         torch.save(best_state, '../datasets/models/baseline_LSTM.pth')\n",
    "#     epoch_begin = time()\n",
    "#     if training_termination(valid_result):\n",
    "#         print_flush(\"early stop at [%d] epoch!\" % (epoch+1))\n",
    "#         break\n",
    "    model.adjust_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (liangjiahui)",
   "language": "python",
   "name": "liangjiahui"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
