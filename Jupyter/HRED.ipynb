{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.init as weight_init\n",
    "import numpy as np\n",
    "from time import time\n",
    "import data\n",
    "import sys\n",
    "from metrics import Metrics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "DEVICE = torch.device('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_HRED():\n",
    "    conf = {\n",
    "    'maxlen':30, # maximum utterance length\n",
    "    'diaglen':10, # how many utterance kept in the context window\n",
    "\n",
    "# Model Arguments\n",
    "    'emb_size':200, # size of word embeddings\n",
    "    'n_hidden':300,\n",
    "    'n_hidden_utter_encode':300, # number of hidden units of utterance encoder\n",
    "    'n_hidden_context_encode':300,\n",
    "    'n_hidden_decode':300,\n",
    "    'n_layers':1, # number of layers\n",
    "    'noise_radius':0.2, # stdev of noise for autoencoder (regularizer)\n",
    "    'lambda_gp':10, # Gradient penalty lambda hyperparameter.\n",
    "    'temp':1.0, # softmax temperature (lower --> more discrete)\n",
    "    'dropout':0.5, # dropout applied to layers (0 = no dropout)\n",
    "\n",
    "# Training Arguments\n",
    "    'batch_size':30,\n",
    "    'epochs':100, # maximum number of epochs\n",
    "    'min_epochs':2, # minimum number of epochs to train for\n",
    "\n",
    "    'lr':0.001, # autoencoder learning rate\n",
    "    'beta1':0.9, # beta1 for adam\n",
    "    'clip':1.0,  # gradient clipping, max norm\n",
    "    'gan_clamp':0.01,  # WGAN clamp (Do not use clamp when you apply gradient penelty             \n",
    "    }\n",
    "    return conf \n",
    "\n",
    "config = config_HRED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gData(data):\n",
    "    tensor=data\n",
    "    if isinstance(data, np.ndarray):\n",
    "        tensor = torch.from_numpy(data)\n",
    "    tensor=tensor.to(DEVICE)\n",
    "    return tensor\n",
    "def gVar(data):\n",
    "    return gData(data)\n",
    "def print_flush(data, args=None):\n",
    "    if args == None:\n",
    "        print(data)\n",
    "    else:\n",
    "        print(data, args)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def indexes2sent(indexes, vocab, eos_tok, ignore_tok=0): \n",
    "    '''indexes: numpy array'''\n",
    "    def revert_sent(indexes, ivocab, eos_tok, ignore_tok=0):\n",
    "        toks=[]\n",
    "        length=0\n",
    "        indexes=filter(lambda i: i!=ignore_tok, indexes)\n",
    "        for idx in indexes:\n",
    "            toks.append(ivocab[idx])\n",
    "            length+=1\n",
    "            if idx == eos_tok:\n",
    "                break\n",
    "        return ' '.join(toks), length\n",
    "    \n",
    "    ivocab = {v: k for k, v in vocab.items()}\n",
    "    if indexes.ndim==1:# one sentence\n",
    "        return revert_sent(indexes, ivocab, eos_tok, ignore_tok)\n",
    "    else:# dim>1\n",
    "        sentences=[] # a batch of sentences\n",
    "        lens=[]\n",
    "        for inds in indexes:\n",
    "            sentence, length = revert_sent(inds, ivocab, eos_tok, ignore_tok)\n",
    "            sentences.append(sentence)\n",
    "            lens.append(length)\n",
    "        return sentences, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedder, input_size, hidden_size, bidirectional, n_layers, noise_radius=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.noise_radius=noise_radius\n",
    "        self.n_layers = n_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        assert type(self.bidirectional)==bool\n",
    "        self.embedding = embedder\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, n_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for w in self.rnn.parameters(): \n",
    "            if w.dim()>1:\n",
    "                weight_init.orthogonal_(w)\n",
    "                \n",
    "    def store_grad_norm(self, grad):\n",
    "        norm = torch.norm(grad, 2, 1)\n",
    "        self.grad_norm = norm.detach().data.mean()\n",
    "        return grad\n",
    "    \n",
    "    def forward(self, inputs, input_lens=None, noise=False): \n",
    "        if self.embedding is not None:\n",
    "            inputs=self.embedding(inputs) \n",
    "        \n",
    "        batch_size, seq_len, emb_size=inputs.size()\n",
    "#         inputs=F.dropout(inputs, 0.5, self.training)\n",
    "        \n",
    "        if input_lens is not None:\n",
    "            input_lens_sorted, indices = input_lens.sort(descending=True)\n",
    "            inputs_sorted = inputs.index_select(0, indices)        \n",
    "            inputs = pack_padded_sequence(inputs_sorted, input_lens_sorted.data.tolist(), batch_first=True)\n",
    "            \n",
    "        init_hidden = gVar(torch.zeros(self.n_layers*(1+self.bidirectional), batch_size, self.hidden_size))\n",
    "        hids, h_n = self.rnn(inputs, init_hidden) \n",
    "        if input_lens is not None:\n",
    "            _, inv_indices = indices.sort()\n",
    "            hids, lens = pad_packed_sequence(hids, batch_first=True)     \n",
    "            hids = hids.index_select(0, inv_indices)\n",
    "            h_n = h_n.index_select(1, inv_indices)\n",
    "        h_n = h_n.view(self.n_layers, (1+self.bidirectional), batch_size, self.hidden_size)\n",
    "        h_n = h_n[-1]\n",
    "        enc = h_n.transpose(1,0).contiguous().view(batch_size,-1) \n",
    "#         if noise and self.noise_radius > 0:\n",
    "#             gauss_noise = gVar(torch.normal(means=torch.zeros(enc.size()),std=self.noise_radius))\n",
    "#             enc = enc + gauss_noise\n",
    "            \n",
    "        return enc, hids\n",
    "    \n",
    "class ContextEncoder(nn.Module):\n",
    "    def __init__(self, utt_encoder, input_size, hidden_size, n_layers=1, noise_radius=0.2):\n",
    "        super(ContextEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.noise_radius=noise_radius\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.utt_encoder=utt_encoder\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for w in self.rnn.parameters(): # initialize the gate weights with orthogonal\n",
    "            if w.dim()>1:\n",
    "                weight_init.orthogonal_(w)\n",
    "    \n",
    "    def store_grad_norm(self, grad):\n",
    "        norm = torch.norm(grad, 2, 1)\n",
    "        self.grad_norm = norm.detach().data.mean()\n",
    "        return grad\n",
    "\n",
    "    def forward(self, context, context_lens, utt_lens, floors, noise=False): \n",
    "        batch_size, max_context_len, max_utt_len = context.size()\n",
    "        utts=context.view(-1, max_utt_len) \n",
    "        utt_lens=utt_lens.view(-1)\n",
    "        utt_encs,_ = self.utt_encoder(utts, utt_lens) \n",
    "        utt_encs = utt_encs.view(batch_size, max_context_len, -1)\n",
    "        floor_one_hot = gVar(torch.zeros(floors.numel(), 2))\n",
    "        floor_one_hot.data.scatter_(1, floors.view(-1, 1), 1)\n",
    "        floor_one_hot = floor_one_hot.view(-1, max_context_len, 2)\n",
    "        utt_floor_encs = torch.cat([utt_encs, floor_one_hot], 2) \n",
    "        \n",
    "#         utt_floor_encs=F.dropout(utt_floor_encs, 0.25, self.training)\n",
    "        context_lens_sorted, indices = context_lens.sort(descending=True)\n",
    "        utt_floor_encs = utt_floor_encs.index_select(0, indices)\n",
    "        utt_floor_encs = pack_padded_sequence(utt_floor_encs, context_lens_sorted.data.tolist(), batch_first=True)\n",
    "        \n",
    "        init_hidden=gVar(torch.zeros(1, batch_size, self.hidden_size))\n",
    "        hids, h_n = self.rnn(utt_floor_encs, init_hidden)\n",
    "        \n",
    "        _, inv_indices = indices.sort()\n",
    "        h_n = h_n.index_select(1, inv_indices)  \n",
    "        enc = h_n.transpose(1,0).contiguous().view(batch_size, -1)\n",
    "\n",
    "#         if noise and self.noise_radius > 0:\n",
    "#             gauss_noise = gVar(torch.normal(means=torch.zeros(enc.size()),std=self.noise_radius))\n",
    "#             enc = enc + gauss_noise\n",
    "        return enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedder, input_size, hidden_size, vocab_size, n_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.input_size= input_size \n",
    "        self.hidden_size = hidden_size \n",
    "        self.vocab_size = vocab_size \n",
    "        self.embedding = embedder\n",
    "#         self.linear = nn.Linear(600, hidden_size)\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        for w in self.rnn.parameters():\n",
    "            if w.dim()>1:\n",
    "                weight_init.orthogonal_(w)\n",
    "        self.out.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out.bias.data.fill_(0)\n",
    "    \n",
    "    def forward(self, init_hidden, context=None, inputs=None, lens=None):\n",
    "        batch_size, maxlen = inputs.size()\n",
    "        if self.embedding is not None:\n",
    "            inputs = self.embedding(inputs)\n",
    "        if context is not None:\n",
    "            repeated_context = context.unsqueeze(1).repeat(1, maxlen, 1)\n",
    "            inputs = torch.cat([inputs, repeated_context], 2)\n",
    "#         inputs = F.dropout(inputs, 0.5, self.training)  ß\n",
    "#         init_hidden = self.linear(init_hidden)\n",
    "        hids, h_n = self.rnn(inputs, init_hidden.unsqueeze(0))\n",
    "        decoded = self.out(hids.contiguous().view(-1, self.hidden_size))# reshape before linear over vocab\n",
    "        decoded = decoded.view(batch_size, maxlen, self.vocab_size)\n",
    "        return decoded\n",
    "    \n",
    "    def sampling(self, init_hidden, context, maxlen, SOS_tok, EOS_tok, mode='greedy'):\n",
    "        batch_size=init_hidden.size(0)\n",
    "#         init_hidden = self.linear(init_hidden)\n",
    "        decoded_words = np.zeros((batch_size, maxlen), dtype=np.int)\n",
    "        sample_lens=np.zeros(batch_size, dtype=np.int)         \n",
    "        decoder_input = gVar(torch.LongTensor([[SOS_tok]*batch_size]).view(batch_size,1))\n",
    "        decoder_input = self.embedding(decoder_input) if self.embedding is not None else decoder_input \n",
    "        decoder_input = torch.cat([decoder_input, context.unsqueeze(1)],2) if context is not None else decoder_input\n",
    "        decoder_hidden = init_hidden.unsqueeze(0).contiguous()\n",
    "        for di in range(maxlen):\n",
    "            decoder_output, decoder_hidden = self.rnn(decoder_input, decoder_hidden)\n",
    "            decoder_output=self.out(decoder_output)\n",
    "            if mode=='greedy':\n",
    "                topi = decoder_output[:,-1].max(1, keepdim=True)[1] \n",
    "            elif mode=='sample':\n",
    "                topi = torch.multinomial(F.softmax(decoder_output[:,-1], dim=1), 1)                    \n",
    "            decoder_input = self.embedding(topi) if self.embedding is not None else topi\n",
    "            decoder_input = torch.cat([decoder_input, context.unsqueeze(1)],2) if context is not None else decoder_input\n",
    "            ni = topi.squeeze().data.cpu().numpy() \n",
    "            decoded_words[:,di]=ni\n",
    "                      \n",
    "        for i in range(batch_size):\n",
    "            for word in decoded_words[i]:\n",
    "                if word == EOS_tok:\n",
    "                    break\n",
    "                sample_lens[i]=sample_lens[i]+1\n",
    "        return decoded_words, sample_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HRED(nn.Module):\n",
    "    def __init__(self, config, vocab_size, PAD_token=0):\n",
    "        super(HRED, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.maxlen=config['maxlen']\n",
    "        self.clip = config['clip']\n",
    "        self.lambda_gp = config['lambda_gp']\n",
    "        self.temp=config['temp']\n",
    "        \n",
    "        self.embedder= nn.Embedding(vocab_size, config['emb_size'], padding_idx=PAD_token)\n",
    "        self.utt_encoder = Encoder(self.embedder, config['emb_size'], config['n_hidden_utter_encode'], \n",
    "                                   True, config['n_layers'], config['noise_radius']) \n",
    "        self.context_encoder = ContextEncoder(self.utt_encoder, config['n_hidden_utter_encode']*2+2, config['n_hidden_context_encode'], 1, config['noise_radius']) \n",
    "        self.decoder = Decoder(self.embedder, config['emb_size'], config['n_hidden_decode'], \n",
    "                               vocab_size, n_layers=1) \n",
    "        \n",
    "    def forward(self, context, context_lens, utt_lens, floors, response, res_lens):\n",
    "        c = self.context_encoder(context, context_lens, utt_lens, floors)\n",
    "        x,_ = self.utt_encoder(response[:,1:], res_lens-1)      \n",
    "        output = self.decoder(c, None, response[:,:-1], (res_lens-1))  \n",
    "        flattened_output = output.view(-1, self.vocab_size) \n",
    "        \n",
    "        dec_target = response[:,1:].contiguous().view(-1)\n",
    "        mask = dec_target.gt(0) # [(batch_sz*seq_len)]\n",
    "        masked_target = dec_target.masked_select(mask) # \n",
    "        output_mask = mask.unsqueeze(1).expand(mask.size(0), self.vocab_size)# [(batch_sz*seq_len) x n_tokens]\n",
    "        masked_output = flattened_output.masked_select(output_mask).view(-1, self.vocab_size)\n",
    "\n",
    "        return masked_target, masked_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(context, context_lens, utt_lens, floors, response, res_lens):\n",
    "    model.train()\n",
    "    target, outputs = model(context, context_lens, utt_lens, floors, response, res_lens)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, target)\n",
    "    batch_loss = loss.item()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return batch_loss\n",
    "        \n",
    "def evaluate_batch(context, context_lens, utt_lens, floors, response, res_lens):\n",
    "    model.eval()\n",
    "    target, outputs = model(context, context_lens, utt_lens, floors, response, res_lens)\n",
    "    loss = criterion(outputs, target)\n",
    "    return loss.item()    \n",
    "\n",
    "def valid(valid_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            batch = valid_loader.next_batch()\n",
    "            if batch is None: # end of epoch\n",
    "                break\n",
    "            context, context_lens, utt_lens, floors,_,_,_,response,res_lens,_ = batch\n",
    "            context, utt_lens = context[:,:,1:], utt_lens-1 # remove the sos token in the context and reduce the context length\n",
    "            context, context_lens, utt_lens, floors, response, res_lens\\\n",
    "                    = gVar(context), gVar(context_lens), gVar(utt_lens), gData(floors), gVar(response), gVar(res_lens)\n",
    "            target, outputs = model(context, context_lens, utt_lens, floors, response, res_lens)\n",
    "            loss_batch = criterion(outputs, target)\n",
    "            total_loss += float(loss_batch.item())\n",
    "    return total_loss / valid_loader.num_batch\n",
    "\n",
    "def sample(context, context_lens, utt_lens, floors, repeat, SOS_tok, EOS_tok):    \n",
    "    model.eval()\n",
    "    c = model.context_encoder(context, context_lens, utt_lens, floors)\n",
    "    c_repeated = c.expand(repeat, -1)\n",
    "#     prior_z = self.sample_code_prior(c_repeated)    \n",
    "    sample_words, sample_lens= model.decoder.sampling(c_repeated, \n",
    "                                                     None, config['maxlen'], SOS_tok, EOS_tok, \"sample\") \n",
    "    return sample_words, sample_lens \n",
    "    \n",
    "    \n",
    "def evaluate(model, metrics, test_loader, vocab, ivocab, repeat):\n",
    "    \n",
    "    recall_bleus, prec_bleus, bows_extrema, bows_avg, bows_greedy, intra_dist1s, intra_dist2s, avg_lens, inter_dist1s, inter_dist2s\\\n",
    "        = [], [], [], [], [], [], [], [], [], []\n",
    "    local_t = 0\n",
    "    while True:\n",
    "        batch = test_loader.next_batch()\n",
    "        if batch is None:\n",
    "            break\n",
    "        local_t += 1 \n",
    "        context, context_lens, utt_lens, floors,_,_,_,response,res_lens,_ = batch   \n",
    "        context, utt_lens = context[:,:,1:], utt_lens-1 # remove the sos token in the context and reduce the context length\n",
    "#         f_eval.write(\"Batch %d \\n\" % (local_t))# print the context\n",
    "        start = np.maximum(0, context_lens[0]-5)\n",
    "        for t_id in range(start, context.shape[1], 1):\n",
    "            context_str = indexes2sent(context[0, t_id], vocab, vocab[\"</s>\"], 0)\n",
    "#             f_eval.write(\"Context %d-%d: %s\\n\" % (t_id, floors[0, t_id], context_str))\n",
    "        # print the true outputs    \n",
    "        ref_str, _ = indexes2sent(response[0], vocab, vocab[\"</s>\"], vocab[\"<s>\"])\n",
    "        ref_tokens = ref_str.split(' ')\n",
    "#         f_eval.write(\"Target >> %s\\n\" % (ref_str.replace(\" ' \", \"'\")))\n",
    "        \n",
    "        context, context_lens, utt_lens, floors = gVar(context), gVar(context_lens), gVar(utt_lens), gData(floors)\n",
    "        sample_words, sample_lens = sample(context, context_lens, utt_lens, floors, repeat, vocab[\"<s>\"], vocab[\"</s>\"])\n",
    "        # nparray: [repeat x seq_len]\n",
    "        pred_sents, _ = indexes2sent(sample_words, vocab, vocab[\"</s>\"], 0)\n",
    "        pred_tokens = [sent.split(' ') for sent in pred_sents]\n",
    "#         for r_id, pred_sent in enumerate(pred_sents):\n",
    "#             f_eval.write(\"Sample %d >> %s\\n\" % (r_id, pred_sent.replace(\" ' \", \"'\")))\n",
    "        max_bleu, avg_bleu = metrics.sim_bleu(pred_tokens, ref_tokens)\n",
    "        recall_bleus.append(max_bleu)\n",
    "        prec_bleus.append(avg_bleu)\n",
    "        \n",
    "        bow_extrema, bow_avg, bow_greedy = metrics.sim_bow(sample_words, sample_lens, response[:,1:], res_lens-2)\n",
    "        bows_extrema.append(bow_extrema)\n",
    "        bows_avg.append(bow_avg)\n",
    "        bows_greedy.append(bow_greedy)\n",
    "        \n",
    "        intra_dist1, intra_dist2, inter_dist1, inter_dist2 = metrics.div_distinct(sample_words, sample_lens)\n",
    "        intra_dist1s.append(intra_dist1)\n",
    "        intra_dist2s.append(intra_dist2)\n",
    "        avg_lens.append(np.mean(sample_lens))\n",
    "        inter_dist1s.append(inter_dist1)\n",
    "        inter_dist2s.append(inter_dist2)\n",
    "        break\n",
    "    recall_bleu = float(np.mean(recall_bleus))\n",
    "    prec_bleu = float(np.mean(prec_bleus))\n",
    "    f1 = 2*(prec_bleu*recall_bleu) / (prec_bleu+recall_bleu+10e-12)\n",
    "    bow_extrema = float(np.mean(bows_extrema))\n",
    "    bow_avg = float(np.mean(bows_avg))\n",
    "    bow_greedy=float(np.mean(bows_greedy))\n",
    "    intra_dist1=float(np.mean(intra_dist1s))\n",
    "    intra_dist2=float(np.mean(intra_dist2s))\n",
    "    avg_len=float(np.mean(avg_lens))\n",
    "    inter_dist1=float(np.mean(inter_dist1s))\n",
    "    inter_dist2=float(np.mean(inter_dist2s))\n",
    "    report = \"Avg recall BLEU %f, avg precision BLEU %f, F1 %f, bow_extrema %f, bow_avg %f, bow_greedy %f,\\\n",
    "    intra_dist1 %f, intra_dist2 %f, avg_len %f, inter_dist1 %f, inter_dist2 %f (only 1 ref, not final results)\" \\\n",
    "    % (recall_bleu, prec_bleu, f1, bow_extrema, bow_avg, bow_greedy, intra_dist1, intra_dist2, avg_len, inter_dist1, inter_dist2)\n",
    "    print(report)\n",
    "    print(' time: %.1f s'%(time()-epoch_begin))\n",
    "#     f_eval.write(report + \"\\n\")\n",
    "    print(\"Done testing\")\n",
    "    return recall_bleu, prec_bleu, bow_extrema, bow_avg, bow_greedy, intra_dist1, intra_dist2, avg_len, inter_dist1, inter_dist2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max utt len 96, mean utt len 14.69\n",
      "Max utt len 75, mean utt len 15.06\n",
      "Max utt len 74, mean utt len 15.39\n",
      "Load corpus with train size 3, valid size 3, test size 3 raw vocab size 24497 vocab size 10000 at cut_off 4 OOV rate 0.008035\n",
      "<d> index 143\n",
      "<sil> index -1\n",
      "67 topics in train data\n",
      "['statement-non-opinion', 'acknowledge_(backchannel)', 'statement-opinion', 'abandoned_or_turn-exit/uninterpretable', 'yes-no-question', 'agree/accept', 'appreciation', 'wh-question', 'backchannel_in_question_form', 'yes_answers', 'conventional-closing', 'response_acknowledgement', 'open-question', 'no_answers', 'affirmative_non-yes_answers', 'declarative_yes-no-question', 'summarize/reformulate', 'other', 'action-directive', 'rhetorical-questions', 'conventional-opening', 'collaborative_completion', 'signal-non-understanding', 'or-clause', 'hold_before_answer/agreement', 'quotation', 'negative_non-no_answers', 'self-talk', 'apology', 'dispreferred_answers', 'offers,_options_commits', 'other_answers', 'reject', 'repeat-phrase', 'non-verbal', 'declarative_wh-question', 'thanking', 'hedge', 'maybe/accept-part', '3rd-party-talk', 'downplayer', 'tag-question']\n",
      "42 dialog acts in train data\n",
      "word2vec cannot cover 0.006599 vocab\n",
      "Done loading corpus\n",
      "Max len 265 and min len 10 and avg len 90.737910\n",
      "Max len 191 and min len 34 and avg len 88.083333\n",
      "Max len 207 and min len 14 and avg len 90.403226\n"
     ]
    }
   ],
   "source": [
    "corpus = getattr(data, 'SWDA'+'Corpus')('../datasets/SWDA/', wordvec_path='../datasets/'+'glove.twitter.27B.200d.txt', wordvec_dim=config['emb_size'])\n",
    "dials = corpus.get_dialogs()\n",
    "metas = corpus.get_metas()\n",
    "vocab = corpus.ivocab\n",
    "ivocab = corpus.vocab\n",
    "n_tokens = len(ivocab)\n",
    "train_dial, valid_dial, test_dial = dials.get(\"train\"), dials.get(\"valid\"), dials.get(\"test\")\n",
    "train_meta, valid_meta, test_meta = metas.get(\"train\"), metas.get(\"valid\"), metas.get(\"test\")\n",
    "train_loader = getattr(data, 'SWDA'+'DataLoader')(\"Train\", train_dial, train_meta, config['maxlen'])\n",
    "valid_loader = getattr(data, 'SWDA'+'DataLoader')(\"Valid\", valid_dial, valid_meta, config['maxlen'])\n",
    "test_loader = getattr(data, 'SWDA'+'DataLoader')(\"Test\", test_dial, test_meta, config['maxlen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max utt len 296, mean utt len 16.48\n",
      "Max utt len 174, mean utt len 16.37\n",
      "Max utt len 214, mean utt len 16.68\n",
      "Load corpus with train size 2, valid size 2, test size 2 raw vocab size 17716 vocab size 10000 at cut_off 2 OOV rate 0.006757\n",
      "<d> index 21\n",
      "<sil> index -1\n",
      "word2vec cannot cover 0.032194 vocab\n",
      "Done loading corpus\n",
      "Max len 36 and min len 3 and avg len 8.840439\n",
      "Max len 32 and min len 3 and avg len 9.069000\n",
      "Max len 27 and min len 3 and avg len 8.740000\n"
     ]
    }
   ],
   "source": [
    "corpus = getattr(data, 'DailyDial'+'Corpus')('../datasets/DailyDial/', wordvec_path='../datasets/'+'glove.twitter.27B.200d.txt', wordvec_dim=config['emb_size'])\n",
    "dials = corpus.get_dialogs()\n",
    "metas = corpus.get_metas()\n",
    "vocab = corpus.ivocab\n",
    "ivocab = corpus.vocab\n",
    "n_tokens = len(ivocab)\n",
    "train_dial, valid_dial, test_dial = dials.get(\"train\"), dials.get(\"valid\"), dials.get(\"test\")\n",
    "train_meta, valid_meta, test_meta = metas.get(\"train\"), metas.get(\"valid\"), metas.get(\"test\")\n",
    "train_loader = getattr(data, 'DailyDial'+'DataLoader')(\"Train\", train_dial, train_meta, config['maxlen'])\n",
    "valid_loader = getattr(data, 'DailyDial'+'DataLoader')(\"Valid\", valid_dial, valid_meta, config['maxlen'])\n",
    "test_loader = getattr(data, 'DailyDial'+'DataLoader')(\"Test\", test_dial, test_meta, config['maxlen'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word2vec\n"
     ]
    }
   ],
   "source": [
    "metrics=Metrics(corpus.word2vec)\n",
    "model = HRED(config, n_tokens)\n",
    "if corpus.word2vec is not None:\n",
    "    print(\"Loaded word2vec\")\n",
    "    model.embedder.weight.data.copy_(torch.from_numpy(corpus.word2vec))\n",
    "    model.embedder.weight.data[0].fill_(0)\n",
    "model.to(DEVICE)\n",
    "parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(parameters, lr=config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "print_every = 100\n",
    "best_state = None\n",
    "max_metric = 0\n",
    "for epoch in range(config['epochs']):\n",
    "    print('Epoch: ', epoch+1)\n",
    "    train_loader.epoch_init(128, config['diaglen'], 1, shuffle=True)\n",
    "    n_iters=train_loader.num_batch\n",
    "    total_loss = 0.0\n",
    "    epoch_begin = time()\n",
    "    batch_count = 0\n",
    "    batch_begin_time = time()\n",
    "    while True:\n",
    "        loss_records=[]\n",
    "        batch = train_loader.next_batch()\n",
    "        if batch is None: # end of epoch\n",
    "            break\n",
    "        context, context_lens, utt_lens, floors,_,_,_,response,res_lens,_ = batch\n",
    "        context, utt_lens = context[:,:,1:], utt_lens-1 # remove the sos token in the context and reduce the context length\n",
    "        context, context_lens, utt_lens, floors, response, res_lens\\\n",
    "                = gVar(context), gVar(context_lens), gVar(utt_lens), gData(floors), gVar(response), gVar(res_lens)\n",
    "        loss_batch = train(context, context_lens, utt_lens, floors, response, res_lens)\n",
    "        total_loss += float(loss_batch)\n",
    "        batch_count += 1\n",
    "        if batch_count % print_every == 0:\n",
    "            print_flush('[%d %d] loss: %.6f time: %.1f s' %\n",
    "                  (epoch + 1, batch_count, np.exp(total_loss / print_every), time() - batch_begin_time))\n",
    "            total_loss = 0.0\n",
    "            batch_begin_time = time()\n",
    "#     scheduler.step()\n",
    "#     print_flush(\"Evaluating....\")\n",
    "#     valid_loader.epoch_init(32, config['diaglen'], 1, shuffle=False)\n",
    "#     loss_valid = valid(valid_loader)\n",
    "#     valid_result.append(F1)\n",
    "#     print_flush('*'*60)\n",
    "#     print_flush('[epoch %d]. loss: %.6f time: %.1f s'%(epoch+1, np.exp(loss_valid), time()-epoch_begin))\n",
    "#     print_flush('*'*60)\n",
    "    print_flush(\"testing....\")\n",
    "    test_loader.epoch_init(1, config['diaglen'], 1, shuffle=False)\n",
    "#     loss_valid = valid(test_loader)\n",
    "#     print_flush('*'*60)\n",
    "#     print_flush('[epoch %d]. loss: %.6f time: %.1f s'%(epoch+1, np.exp(loss_valid), time()-epoch_begin))\n",
    "#     print_flush('*'*60)\n",
    "    recall_bleu, prec_bleu, bow_extrema, bow_avg, bow_greedy, intra_dist1, intra_dist2, avg_len, inter_dist1, inter_dist2\\\n",
    "     =evaluate(model, metrics, test_loader, vocab, ivocab, repeat=10)\n",
    "    epoch_begin = time()\n",
    "#     if F1 > max_metric:\n",
    "#         best_state = model.state_dict()\n",
    "#         max_metric = F1\n",
    "#         print_flush(\"save model...\")\n",
    "#         torch.save(best_state, '../datasets/models/baseline_LSTM.pth')\n",
    "#     epoch_begin = time()\n",
    "#     if training_termination(valid_result):\n",
    "#         print_flush(\"early stop at [%d] epoch!\" % (epoch+1))\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与dialog_doublegan 采用相同的数据集大小\n",
    "def valid_small(valid_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_valid_batch = 0\n",
    "    valid_count = 0\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            batch = valid_loader.next_batch()\n",
    "            if batch is None or total_valid_batch >= 1500: # end of epoch\n",
    "                break\n",
    "            total_valid_batch += 20\n",
    "            valid_count += 1\n",
    "            context, context_lens, utt_lens, floors,_,_,_,response,res_lens,_ = batch\n",
    "            context, utt_lens = context[:,:,1:], utt_lens-1 # remove the sos token in the context and reduce the context length\n",
    "            context, context_lens, utt_lens, floors, response, res_lens\\\n",
    "                    = gVar(context), gVar(context_lens), gVar(utt_lens), gData(floors), gVar(response), gVar(res_lens)\n",
    "            target, outputs = model(context, context_lens, utt_lens, floors, response, res_lens)\n",
    "            loss_batch = criterion(outputs, target)\n",
    "            total_loss += float(loss_batch.item())\n",
    "        return total_loss / valid_count    \n",
    "    \n",
    "def sample(context, context_lens, utt_lens, floors, repeat, SOS_tok, EOS_tok):    \n",
    "    model.eval()\n",
    "    c = model.context_encoder(context, context_lens, utt_lens, floors)\n",
    "#     c_repeated = c.expand(repeat, -1)\n",
    "    sample_words, sample_lens= model.decoder.sampling(c, None, config['maxlen'], SOS_tok, EOS_tok, \"greedy\")\n",
    "    return sample_words, sample_lens \n",
    "\n",
    "def evaluate(model, metrics, test_loader, vocab, ivocab, f_eval, repeat):\n",
    "    recall_bleus, prec_bleus, bows_extrema, bows_avg, bows_greedy, intra_dist1s, intra_dist2s, avg_lens, inter_dist1s, inter_dist2s\\\n",
    "        = [], [], [], [], [], [], [], [], [], []\n",
    "    bleu1_4s = []\n",
    "    local_t = 0\n",
    "    test_loader.epoch_init(1, config['diaglen'], 1, shuffle=False)\n",
    "    valid_count = 0\n",
    "    begin_time = time()\n",
    "    all_generated_sentences = []\n",
    "    all_generated_lens = []\n",
    "    while True:\n",
    "        batch = test_loader.next_batch()\n",
    "        if batch is None:\n",
    "#         if batch is None or valid_count >= 400:\n",
    "            break\n",
    "        valid_count += 1\n",
    "        local_t += 1 \n",
    "        context, context_lens, utt_lens, floors,_,_,_,response,res_lens,_ = batch   \n",
    "        context, utt_lens = context[:,:,1:], utt_lens-1 # remove the sos token in the context and reduce the context length\n",
    "#         f_eval.write(\"Batch %d \\n\" % (local_t))# print the context\n",
    "        f_eval.write(\"Batch %d \\n\" % (local_t))\n",
    "        start = np.maximum(0, context_lens[0]-5)\n",
    "        for t_id in range(start, context.shape[1], 1):\n",
    "            context_str = indexes2sent(context[0, t_id], vocab, vocab[\"</s>\"], 0)\n",
    "            f_eval.write(\"Context %d-%d: %s\\n\" % (t_id, floors[0, t_id], context_str))\n",
    "        # print the true outputs    \n",
    "        ref_str, _ = indexes2sent(response[0], vocab, vocab[\"</s>\"], vocab[\"<s>\"])\n",
    "        ref_tokens = ref_str.split(' ')\n",
    "        f_eval.write(\"Target >> %s\\n\" % (ref_str.replace(\" ' \", \"'\")))\n",
    "        context, context_lens, utt_lens, floors = gVar(context), gVar(context_lens), gVar(utt_lens), gData(floors)\n",
    "        sample_words, sample_lens = sample(context, context_lens, utt_lens, floors, repeat, vocab[\"<s>\"], vocab[\"</s>\"])\n",
    "        # 存储所有生成的回复，用来计算div\n",
    "        all_generated_sentences.append(sample_words[0].tolist())\n",
    "        all_generated_lens.append(sample_lens[0].tolist())\n",
    "        # nparray: [repeat x seq_len]\n",
    "        pred_sents, _ = indexes2sent(sample_words, vocab, vocab[\"</s>\"], 0)\n",
    "        if valid_count % 300 == 0:\n",
    "            print('true response: ', ref_str)\n",
    "            print('generate response: ', pred_sents[0])\n",
    "        pred_tokens = [sent.split(' ') for sent in pred_sents]\n",
    "        for r_id, pred_sent in enumerate(pred_sents):\n",
    "            f_eval.write(\"Generate >> %s\\n\" % (pred_sent.replace(\" ' \", \"'\")))\n",
    "        max_bleu, avg_bleu = metrics.sim_bleu(pred_tokens, ref_tokens)\n",
    "        recall_bleus.append(max_bleu)\n",
    "        prec_bleus.append(avg_bleu)\n",
    "        bleu1_4s.append(metrics.sim_bleu1_4(pred_tokens[0], ref_tokens))\n",
    "        bow_extrema, bow_avg, bow_greedy = metrics.sim_bow(sample_words, sample_lens, response[:,1:], res_lens-2)\n",
    "        bows_extrema.append(bow_extrema)\n",
    "        bows_avg.append(bow_avg)\n",
    "        bows_greedy.append(bow_greedy)\n",
    "#         intra_dist1, intra_dist2, inter_dist1, inter_dist2 = metrics.div_distinct(sample_words, sample_lens-1)\n",
    "#         intra_dist1s.append(intra_dist1)\n",
    "#         intra_dist2s.append(intra_dist2)\n",
    "        avg_lens.append(np.mean(sample_lens))\n",
    "#         inter_dist1s.append(inter_dist1)\n",
    "#         inter_dist2s.append(inter_dist2)\n",
    "        f_eval.write(\"\\n\")\n",
    "    recall_bleu = float(np.mean(recall_bleus))\n",
    "    prec_bleu = float(np.mean(prec_bleus))\n",
    "    f1 = 2*(prec_bleu*recall_bleu) / (prec_bleu+recall_bleu+10e-12)\n",
    "    bleu1_4 = np.mean(bleu1_4s, 0)\n",
    "    bow_extrema = float(np.mean(bows_extrema))\n",
    "    bow_avg = float(np.mean(bows_avg))\n",
    "    bow_greedy=float(np.mean(bows_greedy))\n",
    "#     intra_dist1=float(np.mean(intra_dist1s))\n",
    "#     intra_dist2=float(np.mean(intra_dist2s))\n",
    "    avg_len=float(np.mean(avg_lens))\n",
    "    all_generated_sentences = np.array(all_generated_sentences)\n",
    "    all_generated_lens = np.array(all_generated_lens)\n",
    "#     print(all_generated_sentences[:5])\n",
    "#     print(all_generated_lens[:5])\n",
    "    intra_dist1, intra_dist2, inter_dist1, inter_dist2 = metrics.div_distinct(all_generated_sentences, all_generated_lens)\n",
    "#     inter_dist1=float(np.mean(inter_dist1s))\n",
    "#     inter_dist2=float(np.mean(inter_dist2s))\n",
    "#     report = \"Avg recall BLEU %f, bow_extrema %f, bow_avg %f, bow_greedy %f, inter_dist1 %f, inter_dist2 %f avg_len %f\" \\\n",
    "#     % (recall_bleu, bow_extrema, bow_avg, bow_greedy, inter_dist1, inter_dist2, avg_len)\n",
    "    report = \"BLEU1 %f, BLEU2 %f, BLEU3 %f, BLEU4 %f, inter_dist1 %f, inter_dist2 %f avg_len %f\" % (bleu1_4[0], bleu1_4[1], bleu1_4[2], bleu1_4[3], inter_dist1, inter_dist2, avg_len)\n",
    "    f_eval.write(report + \"\\n\")\n",
    "    print(report)\n",
    "    print(' time: %.1f s'%(time()-begin_time))\n",
    "#     f_eval.write(report + \"\\n\")\n",
    "    print(\"Done testing\")\n",
    "    return recall_bleu, bow_extrema, bow_avg, bow_greedy, inter_dist1, inter_dist2, avg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word2vec\n",
      "Epoch:  1\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[1 50] loss: 452.534842 time: 2.8 s\n",
      "[1 100] loss: 161.236787 time: 2.7 s\n",
      "[1 150] loss: 116.181163 time: 2.8 s\n",
      "[1 200] loss: 106.455213 time: 2.7 s\n",
      "[1 250] loss: 99.108374 time: 2.8 s\n",
      "[1 300] loss: 92.354235 time: 2.8 s\n",
      "[1 350] loss: 79.116544 time: 2.8 s\n",
      "[1 400] loss: 73.616033 time: 2.8 s\n",
      "[1 450] loss: 72.990612 time: 2.8 s\n",
      "[1 500] loss: 79.281011 time: 2.8 s\n",
      "[1 550] loss: 69.700417 time: 2.8 s\n",
      "[1 600] loss: 65.322941 time: 2.8 s\n",
      "[1 650] loss: 65.763151 time: 2.7 s\n",
      "[1 700] loss: 63.126728 time: 2.8 s\n",
      "[1 750] loss: 62.846444 time: 2.8 s\n",
      "[1 800] loss: 59.764124 time: 2.7 s\n",
      "[1 850] loss: 58.628815 time: 2.7 s\n",
      "[1 900] loss: 56.008655 time: 2.8 s\n",
      "[1 950] loss: 58.555644 time: 2.8 s\n",
      "[1 1000] loss: 57.143383 time: 2.8 s\n",
      "[1 1050] loss: 58.728644 time: 2.7 s\n",
      "[1 1100] loss: 64.541011 time: 2.7 s\n",
      "[1 1150] loss: 52.586404 time: 2.8 s\n",
      "[1 1200] loss: 51.025980 time: 2.7 s\n",
      "[1 1250] loss: 51.379712 time: 2.8 s\n",
      "[1 1300] loss: 59.617546 time: 2.8 s\n",
      "[1 1350] loss: 60.161998 time: 2.7 s\n",
      "[1 1400] loss: 53.415211 time: 2.7 s\n",
      "[1 1450] loss: 52.505051 time: 2.8 s\n",
      "[1 1500] loss: 50.627097 time: 2.8 s\n",
      "[1 1550] loss: 52.076499 time: 2.8 s\n",
      "[1 1600] loss: 50.685676 time: 2.8 s\n",
      "[1 1650] loss: 50.904362 time: 2.8 s\n",
      "[1 1700] loss: 62.615667 time: 2.8 s\n",
      "[1 1750] loss: 48.296269 time: 2.8 s\n",
      "[1 1800] loss: 46.023190 time: 2.8 s\n",
      "[1 1850] loss: 49.605169 time: 2.8 s\n",
      "[1 1900] loss: 52.503063 time: 2.7 s\n",
      "[1 1950] loss: 50.392266 time: 2.7 s\n",
      "[1 2000] loss: 48.642123 time: 2.8 s\n",
      "[1 2050] loss: 44.024770 time: 2.8 s\n",
      "[1 2100] loss: 46.035520 time: 2.8 s\n",
      "[1 2150] loss: 47.458763 time: 2.7 s\n",
      "[1 2200] loss: 50.861046 time: 2.7 s\n",
      "[1 2250] loss: 51.662565 time: 2.7 s\n",
      "[1 2300] loss: 48.265555 time: 2.8 s\n",
      "[1 2350] loss: 44.600910 time: 2.8 s\n",
      "[1 2400] loss: 44.144547 time: 2.8 s\n",
      "[1 2450] loss: 41.617730 time: 2.8 s\n",
      "[1 2500] loss: 49.910776 time: 2.7 s\n",
      "[1 2550] loss: 50.693567 time: 2.8 s\n",
      "[1 2600] loss: 47.453306 time: 2.8 s\n",
      "[1 2650] loss: 46.077652 time: 2.7 s\n",
      "[1 2700] loss: 45.265803 time: 2.7 s\n",
      "[1 2750] loss: 50.348746 time: 2.7 s\n",
      "[1 2800] loss: 47.236891 time: 2.8 s\n",
      "[1 2850] loss: 43.672017 time: 2.7 s\n",
      "[1 2900] loss: 45.001545 time: 2.7 s\n",
      "[1 2950] loss: 46.249821 time: 2.7 s\n",
      "[1 3000] loss: 56.197787 time: 2.8 s\n",
      "[1 3050] loss: 47.511827 time: 2.7 s\n",
      "[1 3100] loss: 44.515911 time: 2.8 s\n",
      "[1 3150] loss: 43.275894 time: 2.7 s\n",
      "[1 3200] loss: 44.583129 time: 2.7 s\n",
      "[1 3250] loss: 43.964717 time: 2.8 s\n",
      "[1 3300] loss: 47.752685 time: 2.8 s\n",
      "[1 3350] loss: 49.827986 time: 2.8 s\n",
      "[1 3400] loss: 45.330982 time: 2.8 s\n",
      "[1 3450] loss: 40.368693 time: 2.8 s\n",
      "[1 3500] loss: 38.904806 time: 2.8 s\n",
      "[1 3550] loss: 41.702281 time: 2.8 s\n",
      "[1 3600] loss: 43.578646 time: 2.7 s\n",
      "[1 3650] loss: 43.873554 time: 2.8 s\n",
      "[1 3700] loss: 43.834525 time: 2.7 s\n",
      "[1 3750] loss: 43.194243 time: 2.8 s\n",
      "[1 3800] loss: 39.739116 time: 2.8 s\n",
      "[1 3850] loss: 44.257788 time: 2.7 s\n",
      "[1 3900] loss: 43.512949 time: 2.8 s\n",
      "[1 3950] loss: 42.948050 time: 2.8 s\n",
      "[1 4000] loss: 45.478861 time: 2.7 s\n",
      "[1 4050] loss: 42.008574 time: 2.8 s\n",
      "[1 4100] loss: 43.595726 time: 2.8 s\n",
      "[1 4150] loss: 41.501529 time: 2.8 s\n",
      "[1 4200] loss: 42.526667 time: 2.8 s\n",
      "[1 4250] loss: 43.731187 time: 2.8 s\n",
      "[1 4300] loss: 40.084357 time: 2.7 s\n",
      "[1 4350] loss: 38.764665 time: 2.7 s\n",
      "[1 4400] loss: 39.005253 time: 2.8 s\n",
      "[1 4450] loss: 41.817196 time: 2.8 s\n",
      "[1 4500] loss: 41.205024 time: 2.8 s\n",
      "[1 4550] loss: 43.425395 time: 2.8 s\n",
      "[1 4600] loss: 42.459214 time: 2.7 s\n",
      "[1 4650] loss: 41.904748 time: 2.8 s\n",
      "[1 4700] loss: 41.571440 time: 2.8 s\n",
      "[1 4750] loss: 42.398288 time: 2.7 s\n",
      "[1 4800] loss: 39.975927 time: 2.8 s\n",
      "[1 4850] loss: 38.811228 time: 2.7 s\n",
      "[1 4900] loss: 36.312736 time: 2.8 s\n",
      "[1 4950] loss: 36.772624 time: 2.8 s\n",
      "[1 5000] loss: 41.384758 time: 2.7 s\n",
      "[1 5050] loss: 41.308551 time: 2.8 s\n",
      "[1 5100] loss: 39.655594 time: 2.8 s\n",
      "[1 5150] loss: 37.822655 time: 2.7 s\n",
      "[1 5200] loss: 35.697816 time: 2.7 s\n",
      "[1 5250] loss: 36.322065 time: 2.8 s\n",
      "[1 5300] loss: 37.228648 time: 2.7 s\n",
      "[1 5350] loss: 38.065089 time: 2.8 s\n",
      "[1 5400] loss: 36.044542 time: 2.7 s\n",
      "[1 5450] loss: 35.207440 time: 2.8 s\n",
      "[1 5500] loss: 39.550435 time: 2.7 s\n",
      "[1 5550] loss: 38.509450 time: 2.8 s\n",
      "[1 5600] loss: 39.364871 time: 2.8 s\n",
      "[1 5650] loss: 39.273614 time: 2.8 s\n",
      "[1 5700] loss: 36.562976 time: 2.8 s\n",
      "[1 5750] loss: 34.873572 time: 2.7 s\n",
      "[1 5800] loss: 37.735509 time: 2.8 s\n",
      "[1 5850] loss: 38.506605 time: 2.8 s\n",
      "[1 5900] loss: 37.350667 time: 2.7 s\n",
      "[1 5950] loss: 40.571859 time: 2.8 s\n",
      "[1 6000] loss: 39.898977 time: 2.8 s\n",
      "[1 6050] loss: 38.969293 time: 2.8 s\n",
      "[1 6100] loss: 36.101695 time: 2.8 s\n",
      "[1 6150] loss: 35.736254 time: 2.7 s\n",
      "[1 6200] loss: 40.275803 time: 2.8 s\n",
      "[1 6250] loss: 39.392956 time: 2.7 s\n",
      "[1 6300] loss: 39.135141 time: 2.8 s\n",
      "[1 6350] loss: 38.553424 time: 2.8 s\n",
      "Evaluating....\n",
      "Valid begins with 221 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 1]. loss: 39.715136 time: 355.6 s\n",
      "************************************************************\n",
      "Epoch:  2\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[2 50] loss: 36.956576 time: 2.8 s\n",
      "[2 100] loss: 34.748248 time: 2.8 s\n",
      "[2 150] loss: 38.241332 time: 2.8 s\n",
      "[2 200] loss: 38.789745 time: 2.8 s\n",
      "[2 250] loss: 36.086128 time: 2.8 s\n",
      "[2 300] loss: 36.152263 time: 2.8 s\n",
      "[2 350] loss: 31.464385 time: 2.8 s\n",
      "[2 400] loss: 29.513283 time: 2.8 s\n",
      "[2 450] loss: 36.788376 time: 2.8 s\n",
      "[2 500] loss: 37.681456 time: 2.8 s\n",
      "[2 550] loss: 36.878819 time: 2.7 s\n",
      "[2 600] loss: 39.768156 time: 2.8 s\n",
      "[2 650] loss: 34.709253 time: 2.8 s\n",
      "[2 700] loss: 29.931212 time: 2.8 s\n",
      "[2 750] loss: 29.131457 time: 2.7 s\n",
      "[2 800] loss: 28.849807 time: 2.8 s\n",
      "[2 850] loss: 28.414115 time: 2.8 s\n",
      "[2 900] loss: 27.347178 time: 2.8 s\n",
      "[2 950] loss: 29.589838 time: 2.7 s\n",
      "[2 1000] loss: 29.250630 time: 2.8 s\n",
      "[2 1050] loss: 36.065382 time: 2.7 s\n",
      "[2 1100] loss: 33.309545 time: 2.8 s\n",
      "[2 1150] loss: 33.595979 time: 2.7 s\n",
      "[2 1200] loss: 30.740542 time: 2.7 s\n",
      "[2 1250] loss: 36.048158 time: 2.8 s\n",
      "[2 1300] loss: 33.120549 time: 2.7 s\n",
      "[2 1350] loss: 29.017086 time: 2.8 s\n",
      "[2 1400] loss: 27.763678 time: 2.7 s\n",
      "[2 1450] loss: 28.582155 time: 2.8 s\n",
      "[2 1500] loss: 31.845640 time: 2.7 s\n",
      "[2 1550] loss: 30.976533 time: 2.7 s\n",
      "[2 1600] loss: 40.182626 time: 2.8 s\n",
      "[2 1650] loss: 34.152599 time: 2.7 s\n",
      "[2 1700] loss: 34.545784 time: 2.7 s\n",
      "[2 1750] loss: 32.347040 time: 2.7 s\n",
      "[2 1800] loss: 41.151609 time: 2.8 s\n",
      "[2 1850] loss: 40.848870 time: 2.7 s\n",
      "[2 1900] loss: 32.473896 time: 2.8 s\n",
      "[2 1950] loss: 29.777656 time: 2.8 s\n",
      "[2 2000] loss: 29.224795 time: 2.7 s\n",
      "[2 2050] loss: 35.432065 time: 2.7 s\n",
      "[2 2100] loss: 32.270684 time: 2.8 s\n",
      "[2 2150] loss: 33.432411 time: 2.7 s\n",
      "[2 2200] loss: 32.244444 time: 2.7 s\n",
      "[2 2250] loss: 31.837573 time: 2.7 s\n",
      "[2 2300] loss: 33.145349 time: 2.7 s\n",
      "[2 2350] loss: 31.264185 time: 2.8 s\n",
      "[2 2400] loss: 31.848133 time: 2.7 s\n",
      "[2 2450] loss: 33.976485 time: 2.7 s\n",
      "[2 2500] loss: 32.175048 time: 2.7 s\n",
      "[2 2550] loss: 31.762175 time: 2.7 s\n",
      "[2 2600] loss: 39.838569 time: 2.7 s\n",
      "[2 2650] loss: 34.924576 time: 2.7 s\n",
      "[2 2700] loss: 35.146761 time: 2.8 s\n",
      "[2 2750] loss: 32.631173 time: 2.8 s\n",
      "[2 2800] loss: 33.759819 time: 2.7 s\n",
      "[2 2850] loss: 37.004964 time: 2.7 s\n",
      "[2 2900] loss: 34.383807 time: 2.7 s\n",
      "[2 2950] loss: 32.080899 time: 2.7 s\n",
      "[2 3000] loss: 29.873708 time: 2.8 s\n",
      "[2 3050] loss: 29.443105 time: 2.8 s\n",
      "[2 3100] loss: 35.759925 time: 2.8 s\n",
      "[2 3150] loss: 31.031418 time: 2.8 s\n",
      "[2 3200] loss: 29.092982 time: 2.7 s\n",
      "[2 3250] loss: 33.140528 time: 2.8 s\n",
      "[2 3300] loss: 32.990773 time: 2.7 s\n",
      "[2 3350] loss: 31.556929 time: 2.7 s\n",
      "[2 3400] loss: 33.281489 time: 2.7 s\n",
      "[2 3450] loss: 39.692404 time: 2.7 s\n",
      "[2 3500] loss: 31.886450 time: 2.7 s\n",
      "[2 3550] loss: 31.410056 time: 2.8 s\n",
      "[2 3600] loss: 31.665375 time: 2.7 s\n",
      "[2 3650] loss: 39.220673 time: 2.8 s\n",
      "[2 3700] loss: 31.832496 time: 2.7 s\n",
      "[2 3750] loss: 33.966738 time: 2.8 s\n",
      "[2 3800] loss: 32.445810 time: 2.8 s\n",
      "[2 3850] loss: 34.499370 time: 2.7 s\n",
      "[2 3900] loss: 34.030788 time: 2.8 s\n",
      "[2 3950] loss: 31.349381 time: 2.8 s\n",
      "[2 4000] loss: 27.944514 time: 2.8 s\n",
      "[2 4050] loss: 29.778203 time: 2.7 s\n",
      "[2 4100] loss: 27.902189 time: 2.7 s\n",
      "[2 4150] loss: 33.280101 time: 2.8 s\n",
      "[2 4200] loss: 33.011296 time: 2.7 s\n",
      "[2 4250] loss: 32.673241 time: 2.8 s\n",
      "[2 4300] loss: 31.404541 time: 2.8 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4350] loss: 31.904695 time: 2.8 s\n",
      "[2 4400] loss: 30.326734 time: 2.8 s\n",
      "[2 4450] loss: 31.145038 time: 2.8 s\n",
      "[2 4500] loss: 33.319513 time: 2.8 s\n",
      "[2 4550] loss: 33.140694 time: 2.8 s\n",
      "[2 4600] loss: 33.418467 time: 2.8 s\n",
      "[2 4650] loss: 32.633628 time: 2.7 s\n",
      "[2 4700] loss: 31.822360 time: 2.8 s\n",
      "[2 4750] loss: 32.708029 time: 2.8 s\n",
      "[2 4800] loss: 30.888971 time: 2.7 s\n",
      "[2 4850] loss: 30.637003 time: 2.8 s\n",
      "[2 4900] loss: 36.109482 time: 2.7 s\n",
      "[2 4950] loss: 31.585760 time: 2.7 s\n",
      "[2 5000] loss: 35.341996 time: 2.8 s\n",
      "[2 5050] loss: 32.848950 time: 2.8 s\n",
      "[2 5100] loss: 28.462257 time: 2.7 s\n",
      "[2 5150] loss: 29.766338 time: 2.8 s\n",
      "[2 5200] loss: 32.406430 time: 2.7 s\n",
      "[2 5250] loss: 32.849897 time: 2.7 s\n",
      "[2 5300] loss: 33.453780 time: 2.8 s\n",
      "[2 5350] loss: 32.110377 time: 2.7 s\n",
      "[2 5400] loss: 31.941167 time: 2.7 s\n",
      "[2 5450] loss: 30.539737 time: 2.8 s\n",
      "[2 5500] loss: 33.609455 time: 2.8 s\n",
      "[2 5550] loss: 34.403154 time: 2.7 s\n",
      "[2 5600] loss: 32.703543 time: 2.8 s\n",
      "[2 5650] loss: 31.661435 time: 2.7 s\n",
      "[2 5700] loss: 31.679893 time: 2.8 s\n",
      "[2 5750] loss: 32.104409 time: 2.8 s\n",
      "[2 5800] loss: 31.266938 time: 2.8 s\n",
      "[2 5850] loss: 31.475458 time: 2.7 s\n",
      "[2 5900] loss: 37.090804 time: 2.7 s\n",
      "[2 5950] loss: 31.547170 time: 2.8 s\n",
      "[2 6000] loss: 32.413766 time: 2.7 s\n",
      "[2 6050] loss: 31.219068 time: 2.8 s\n",
      "[2 6100] loss: 38.484446 time: 2.7 s\n",
      "[2 6150] loss: 32.575148 time: 2.8 s\n",
      "[2 6200] loss: 33.096958 time: 2.8 s\n",
      "[2 6250] loss: 31.487889 time: 1.9 s\n",
      "[2 6300] loss: 33.349947 time: 1.4 s\n",
      "[2 6350] loss: 33.388928 time: 1.4 s\n",
      "Evaluating....\n",
      "Valid begins with 221 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 2]. loss: 36.099091 time: 348.3 s\n",
      "************************************************************\n",
      "Epoch:  3\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[3 50] loss: 27.214020 time: 1.4 s\n",
      "[3 100] loss: 26.333968 time: 1.4 s\n",
      "[3 150] loss: 25.037457 time: 1.4 s\n",
      "[3 200] loss: 25.473090 time: 1.4 s\n",
      "[3 250] loss: 24.064052 time: 1.4 s\n",
      "[3 300] loss: 26.269749 time: 1.4 s\n",
      "[3 350] loss: 26.613993 time: 1.4 s\n",
      "[3 400] loss: 25.686072 time: 1.4 s\n",
      "[3 450] loss: 27.369032 time: 1.4 s\n",
      "[3 500] loss: 26.801489 time: 1.4 s\n",
      "[3 550] loss: 25.097472 time: 1.4 s\n",
      "[3 600] loss: 24.670447 time: 1.4 s\n",
      "[3 650] loss: 22.063011 time: 1.4 s\n",
      "[3 700] loss: 29.769289 time: 1.4 s\n",
      "[3 750] loss: 32.934443 time: 1.4 s\n",
      "[3 800] loss: 24.820473 time: 1.4 s\n",
      "[3 850] loss: 26.681930 time: 1.4 s\n",
      "[3 900] loss: 26.933723 time: 1.4 s\n",
      "[3 950] loss: 26.351743 time: 1.4 s\n",
      "[3 1000] loss: 28.872204 time: 1.4 s\n",
      "[3 1050] loss: 29.979286 time: 1.4 s\n",
      "[3 1100] loss: 24.222264 time: 1.4 s\n",
      "[3 1150] loss: 28.810809 time: 1.4 s\n",
      "[3 1200] loss: 27.119727 time: 1.4 s\n",
      "[3 1250] loss: 26.376047 time: 1.4 s\n",
      "[3 1300] loss: 25.332701 time: 1.4 s\n",
      "[3 1350] loss: 28.112958 time: 1.4 s\n",
      "[3 1400] loss: 30.153203 time: 1.4 s\n",
      "[3 1450] loss: 29.248296 time: 1.4 s\n",
      "[3 1500] loss: 25.037935 time: 1.4 s\n",
      "[3 1550] loss: 27.764586 time: 1.4 s\n",
      "[3 1600] loss: 28.016838 time: 1.4 s\n",
      "[3 1650] loss: 29.764686 time: 1.4 s\n",
      "[3 1700] loss: 30.611597 time: 1.4 s\n",
      "[3 1750] loss: 26.716127 time: 1.4 s\n",
      "[3 1800] loss: 27.730613 time: 1.4 s\n",
      "[3 1850] loss: 33.489312 time: 1.4 s\n",
      "[3 1900] loss: 28.408206 time: 1.4 s\n",
      "[3 1950] loss: 32.524204 time: 1.4 s\n",
      "[3 2000] loss: 32.205342 time: 1.4 s\n",
      "[3 2050] loss: 29.654858 time: 1.4 s\n",
      "[3 2100] loss: 23.991666 time: 1.4 s\n",
      "[3 2150] loss: 26.631802 time: 1.4 s\n",
      "[3 2200] loss: 26.625822 time: 1.4 s\n",
      "[3 2250] loss: 26.118055 time: 1.4 s\n",
      "[3 2300] loss: 26.976133 time: 1.4 s\n",
      "[3 2350] loss: 27.672508 time: 1.4 s\n",
      "[3 2400] loss: 26.587759 time: 1.4 s\n",
      "[3 2450] loss: 23.313359 time: 1.3 s\n",
      "[3 2500] loss: 24.699883 time: 1.4 s\n",
      "[3 2550] loss: 30.351250 time: 1.4 s\n",
      "[3 2600] loss: 27.800776 time: 1.4 s\n",
      "[3 2650] loss: 25.718100 time: 1.4 s\n",
      "[3 2700] loss: 25.678865 time: 1.4 s\n",
      "[3 2750] loss: 27.864059 time: 1.4 s\n",
      "[3 2800] loss: 30.358123 time: 1.4 s\n",
      "[3 2850] loss: 31.529260 time: 1.4 s\n",
      "[3 2900] loss: 31.400983 time: 1.4 s\n",
      "[3 2950] loss: 30.735866 time: 1.4 s\n",
      "[3 3000] loss: 29.414712 time: 1.4 s\n",
      "[3 3050] loss: 26.919197 time: 1.4 s\n",
      "[3 3100] loss: 27.704193 time: 1.4 s\n",
      "[3 3150] loss: 28.790119 time: 1.4 s\n",
      "[3 3200] loss: 30.769404 time: 1.4 s\n",
      "[3 3250] loss: 28.196015 time: 1.4 s\n",
      "[3 3300] loss: 28.178363 time: 1.4 s\n",
      "[3 3350] loss: 28.874469 time: 1.4 s\n",
      "[3 3400] loss: 32.490303 time: 1.4 s\n",
      "[3 3450] loss: 27.382378 time: 1.4 s\n",
      "[3 3500] loss: 27.028386 time: 1.4 s\n",
      "[3 3550] loss: 25.763565 time: 1.4 s\n",
      "[3 3600] loss: 24.259812 time: 1.4 s\n",
      "[3 3650] loss: 35.432409 time: 1.4 s\n",
      "[3 3700] loss: 26.760894 time: 1.4 s\n",
      "[3 3750] loss: 27.641276 time: 1.4 s\n",
      "[3 3800] loss: 27.130439 time: 1.4 s\n",
      "[3 3850] loss: 26.736575 time: 1.4 s\n",
      "[3 3900] loss: 33.018106 time: 1.4 s\n",
      "[3 3950] loss: 33.860963 time: 1.4 s\n",
      "[3 4000] loss: 29.103441 time: 1.4 s\n",
      "[3 4050] loss: 28.264292 time: 1.4 s\n",
      "[3 4100] loss: 26.709815 time: 1.4 s\n",
      "[3 4150] loss: 26.211480 time: 1.4 s\n",
      "[3 4200] loss: 24.821500 time: 1.4 s\n",
      "[3 4250] loss: 26.303959 time: 1.4 s\n",
      "[3 4300] loss: 27.444323 time: 1.4 s\n",
      "[3 4350] loss: 26.890838 time: 1.4 s\n",
      "[3 4400] loss: 29.048180 time: 1.4 s\n",
      "[3 4450] loss: 29.235198 time: 1.4 s\n",
      "[3 4500] loss: 28.638180 time: 1.4 s\n",
      "[3 4550] loss: 32.225806 time: 1.4 s\n",
      "[3 4600] loss: 30.423774 time: 1.4 s\n",
      "[3 4650] loss: 28.887562 time: 1.4 s\n",
      "[3 4700] loss: 29.208677 time: 1.4 s\n",
      "[3 4750] loss: 26.459739 time: 1.4 s\n",
      "[3 4800] loss: 31.740809 time: 1.4 s\n",
      "[3 4850] loss: 30.915384 time: 1.4 s\n",
      "[3 4900] loss: 28.429942 time: 1.4 s\n",
      "[3 4950] loss: 27.765987 time: 1.4 s\n",
      "[3 5000] loss: 27.352329 time: 1.4 s\n",
      "[3 5050] loss: 28.307712 time: 1.4 s\n",
      "[3 5100] loss: 27.678425 time: 1.4 s\n",
      "[3 5150] loss: 27.925032 time: 1.4 s\n",
      "[3 5200] loss: 28.725523 time: 1.4 s\n",
      "[3 5250] loss: 26.439146 time: 1.4 s\n",
      "[3 5300] loss: 31.275822 time: 1.4 s\n",
      "[3 5350] loss: 29.052496 time: 1.4 s\n",
      "[3 5400] loss: 28.804629 time: 1.4 s\n",
      "[3 5450] loss: 29.033409 time: 1.4 s\n",
      "[3 5500] loss: 26.994037 time: 1.4 s\n",
      "[3 5550] loss: 26.112177 time: 1.4 s\n",
      "[3 5600] loss: 32.283105 time: 1.4 s\n",
      "[3 5650] loss: 29.483326 time: 1.4 s\n",
      "[3 5700] loss: 29.108084 time: 1.4 s\n",
      "[3 5750] loss: 30.445398 time: 1.4 s\n",
      "[3 5800] loss: 28.398720 time: 1.4 s\n",
      "[3 5850] loss: 28.726030 time: 1.4 s\n",
      "[3 5900] loss: 29.532140 time: 1.4 s\n",
      "[3 5950] loss: 30.350137 time: 1.4 s\n",
      "[3 6000] loss: 27.393753 time: 1.4 s\n",
      "[3 6050] loss: 28.728305 time: 1.4 s\n",
      "[3 6100] loss: 29.138461 time: 1.4 s\n",
      "[3 6150] loss: 30.532614 time: 1.4 s\n",
      "[3 6200] loss: 29.290853 time: 1.4 s\n",
      "[3 6250] loss: 26.872791 time: 1.4 s\n",
      "[3 6300] loss: 26.018275 time: 1.4 s\n",
      "[3 6350] loss: 26.582594 time: 1.4 s\n",
      "Evaluating....\n",
      "Valid begins with 221 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 3]. loss: 36.205637 time: 178.9 s\n",
      "************************************************************\n",
      "Epoch:  4\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[4 50] loss: 31.425381 time: 1.4 s\n",
      "[4 100] loss: 28.404762 time: 1.4 s\n",
      "[4 150] loss: 23.928081 time: 1.4 s\n",
      "[4 200] loss: 21.651349 time: 1.4 s\n",
      "[4 250] loss: 22.744459 time: 1.4 s\n",
      "[4 300] loss: 24.309405 time: 1.4 s\n",
      "[4 350] loss: 23.208503 time: 1.4 s\n",
      "[4 400] loss: 22.593129 time: 1.4 s\n",
      "[4 450] loss: 29.318032 time: 1.4 s\n",
      "[4 500] loss: 23.022177 time: 1.4 s\n",
      "[4 550] loss: 23.855584 time: 1.4 s\n",
      "[4 600] loss: 26.106267 time: 1.4 s\n",
      "[4 650] loss: 23.102098 time: 1.4 s\n",
      "[4 700] loss: 23.261364 time: 1.4 s\n",
      "[4 750] loss: 23.385774 time: 1.4 s\n",
      "[4 800] loss: 24.638940 time: 1.4 s\n",
      "[4 850] loss: 23.478688 time: 1.4 s\n",
      "[4 900] loss: 22.051022 time: 1.4 s\n",
      "[4 950] loss: 24.114606 time: 1.4 s\n",
      "[4 1000] loss: 22.750946 time: 1.4 s\n",
      "[4 1050] loss: 20.929191 time: 1.4 s\n",
      "[4 1100] loss: 20.135319 time: 1.4 s\n",
      "[4 1150] loss: 18.912351 time: 1.4 s\n",
      "[4 1200] loss: 20.151855 time: 1.4 s\n",
      "[4 1250] loss: 23.058548 time: 1.4 s\n",
      "[4 1300] loss: 22.691286 time: 1.4 s\n",
      "[4 1350] loss: 23.878386 time: 1.4 s\n",
      "[4 1400] loss: 23.783747 time: 1.4 s\n",
      "[4 1450] loss: 25.624276 time: 1.4 s\n",
      "[4 1500] loss: 30.691879 time: 1.4 s\n",
      "[4 1550] loss: 25.181274 time: 1.4 s\n",
      "[4 1600] loss: 26.730615 time: 1.4 s\n",
      "[4 1650] loss: 26.090115 time: 1.4 s\n",
      "[4 1700] loss: 26.256000 time: 1.4 s\n",
      "[4 1750] loss: 24.804868 time: 1.4 s\n",
      "[4 1800] loss: 26.673189 time: 1.4 s\n",
      "[4 1850] loss: 24.840750 time: 1.4 s\n",
      "[4 1900] loss: 25.739068 time: 1.4 s\n",
      "[4 1950] loss: 28.684185 time: 1.4 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 2000] loss: 24.075985 time: 1.4 s\n",
      "[4 2050] loss: 22.353302 time: 1.4 s\n",
      "[4 2100] loss: 22.302833 time: 1.4 s\n",
      "[4 2150] loss: 22.885731 time: 1.4 s\n",
      "[4 2200] loss: 26.067880 time: 1.4 s\n",
      "[4 2250] loss: 25.981470 time: 1.4 s\n",
      "[4 2300] loss: 26.263950 time: 1.4 s\n",
      "[4 2350] loss: 25.008963 time: 1.4 s\n",
      "[4 2400] loss: 24.483451 time: 1.4 s\n",
      "[4 2450] loss: 25.030412 time: 1.4 s\n",
      "[4 2500] loss: 26.683007 time: 1.4 s\n",
      "[4 2550] loss: 23.777622 time: 2.0 s\n",
      "[4 2600] loss: 28.691539 time: 2.5 s\n",
      "[4 2650] loss: 25.081116 time: 1.4 s\n",
      "[4 2700] loss: 25.138328 time: 1.4 s\n",
      "[4 2750] loss: 24.279999 time: 1.4 s\n",
      "[4 2800] loss: 24.863160 time: 1.7 s\n",
      "[4 2850] loss: 24.120879 time: 2.8 s\n",
      "[4 2900] loss: 28.106052 time: 2.8 s\n",
      "[4 2950] loss: 27.076857 time: 2.8 s\n",
      "[4 3000] loss: 25.362134 time: 2.8 s\n",
      "[4 3050] loss: 23.012313 time: 2.8 s\n",
      "[4 3100] loss: 23.404455 time: 2.8 s\n",
      "[4 3150] loss: 25.473163 time: 2.8 s\n",
      "[4 3200] loss: 28.828741 time: 2.8 s\n",
      "[4 3250] loss: 28.703936 time: 2.8 s\n",
      "[4 3300] loss: 23.449074 time: 2.8 s\n",
      "[4 3350] loss: 23.095135 time: 2.8 s\n",
      "[4 3400] loss: 26.571088 time: 2.8 s\n",
      "[4 3450] loss: 26.159639 time: 2.8 s\n",
      "[4 3500] loss: 27.440241 time: 2.8 s\n",
      "[4 3550] loss: 26.147252 time: 2.7 s\n",
      "[4 3600] loss: 25.261518 time: 2.8 s\n",
      "[4 3650] loss: 25.198720 time: 2.8 s\n",
      "[4 3700] loss: 24.838121 time: 2.7 s\n",
      "[4 3750] loss: 24.998758 time: 2.7 s\n",
      "[4 3800] loss: 27.864924 time: 2.7 s\n",
      "[4 3850] loss: 30.847729 time: 2.8 s\n",
      "[4 3900] loss: 27.501113 time: 2.8 s\n",
      "[4 3950] loss: 28.532464 time: 2.7 s\n",
      "[4 4000] loss: 25.554406 time: 2.7 s\n",
      "[4 4050] loss: 24.880597 time: 2.8 s\n",
      "[4 4100] loss: 24.243259 time: 2.7 s\n",
      "[4 4150] loss: 27.698363 time: 2.8 s\n",
      "[4 4200] loss: 25.822693 time: 2.8 s\n",
      "[4 4250] loss: 25.202209 time: 2.8 s\n",
      "[4 4300] loss: 31.539365 time: 2.7 s\n",
      "[4 4350] loss: 28.050703 time: 2.7 s\n",
      "[4 4400] loss: 25.234561 time: 2.7 s\n",
      "[4 4450] loss: 24.568255 time: 2.8 s\n",
      "[4 4500] loss: 29.775596 time: 2.8 s\n",
      "[4 4550] loss: 24.998903 time: 2.7 s\n",
      "[4 4600] loss: 23.621959 time: 2.7 s\n",
      "[4 4650] loss: 27.755105 time: 2.8 s\n",
      "[4 4700] loss: 24.910587 time: 2.8 s\n",
      "[4 4750] loss: 24.582221 time: 2.7 s\n",
      "[4 4800] loss: 24.358411 time: 2.7 s\n",
      "[4 4850] loss: 24.480144 time: 2.8 s\n",
      "[4 4900] loss: 23.931583 time: 2.7 s\n",
      "[4 4950] loss: 25.076677 time: 2.7 s\n",
      "[4 5000] loss: 24.398035 time: 2.8 s\n",
      "[4 5050] loss: 26.301185 time: 2.7 s\n",
      "[4 5100] loss: 24.945040 time: 2.8 s\n",
      "[4 5150] loss: 25.863190 time: 2.7 s\n",
      "[4 5200] loss: 24.505027 time: 2.7 s\n",
      "[4 5250] loss: 24.232256 time: 2.7 s\n",
      "[4 5300] loss: 22.619583 time: 2.7 s\n",
      "[4 5350] loss: 24.038533 time: 2.7 s\n",
      "[4 5400] loss: 23.185611 time: 2.8 s\n",
      "[4 5450] loss: 26.226110 time: 2.7 s\n",
      "[4 5500] loss: 24.827600 time: 2.7 s\n",
      "[4 5550] loss: 24.011309 time: 2.8 s\n",
      "[4 5600] loss: 24.348748 time: 2.7 s\n",
      "[4 5650] loss: 22.548308 time: 2.7 s\n",
      "[4 5700] loss: 24.172049 time: 2.8 s\n",
      "[4 5750] loss: 24.129099 time: 2.8 s\n",
      "[4 5800] loss: 22.687425 time: 2.8 s\n",
      "[4 5850] loss: 25.741737 time: 2.7 s\n",
      "[4 5900] loss: 26.522968 time: 2.8 s\n",
      "[4 5950] loss: 25.783761 time: 2.7 s\n",
      "[4 6000] loss: 25.549707 time: 2.8 s\n",
      "[4 6050] loss: 26.543125 time: 2.7 s\n",
      "[4 6100] loss: 26.019936 time: 2.8 s\n",
      "[4 6150] loss: 27.285790 time: 2.8 s\n",
      "[4 6200] loss: 26.924255 time: 2.8 s\n",
      "[4 6250] loss: 25.279869 time: 2.7 s\n",
      "[4 6300] loss: 23.295201 time: 2.7 s\n",
      "[4 6350] loss: 25.882549 time: 2.7 s\n",
      "Evaluating....\n",
      "Valid begins with 221 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 4]. loss: 35.729752 time: 280.3 s\n",
      "************************************************************\n",
      "Epoch:  5\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[5 50] loss: 26.168552 time: 2.7 s\n",
      "[5 100] loss: 16.614767 time: 2.7 s\n",
      "[5 150] loss: 15.969958 time: 2.8 s\n",
      "[5 200] loss: 16.518172 time: 2.7 s\n",
      "[5 250] loss: 17.964098 time: 2.7 s\n",
      "[5 300] loss: 22.587652 time: 2.8 s\n",
      "[5 350] loss: 20.909608 time: 2.8 s\n",
      "[5 400] loss: 18.658228 time: 2.8 s\n",
      "[5 450] loss: 21.925802 time: 2.8 s\n",
      "[5 500] loss: 21.511765 time: 2.7 s\n",
      "[5 550] loss: 22.225200 time: 2.7 s\n",
      "[5 600] loss: 25.864410 time: 2.8 s\n",
      "[5 650] loss: 24.784535 time: 2.8 s\n",
      "[5 700] loss: 22.244908 time: 2.7 s\n",
      "[5 750] loss: 21.706945 time: 2.8 s\n",
      "[5 800] loss: 22.320880 time: 2.8 s\n",
      "[5 850] loss: 21.949237 time: 2.8 s\n",
      "[5 900] loss: 21.715371 time: 2.7 s\n",
      "[5 950] loss: 19.785019 time: 2.8 s\n",
      "[5 1000] loss: 20.420240 time: 2.8 s\n",
      "[5 1050] loss: 20.947792 time: 2.7 s\n",
      "[5 1100] loss: 21.767030 time: 2.7 s\n",
      "[5 1150] loss: 22.410751 time: 2.8 s\n",
      "[5 1200] loss: 21.656826 time: 2.8 s\n",
      "[5 1250] loss: 21.742692 time: 2.8 s\n",
      "[5 1300] loss: 20.885829 time: 2.8 s\n",
      "[5 1350] loss: 21.125354 time: 2.7 s\n",
      "[5 1400] loss: 22.991901 time: 2.8 s\n",
      "[5 1450] loss: 21.814234 time: 2.7 s\n",
      "[5 1500] loss: 22.788206 time: 2.8 s\n",
      "[5 1550] loss: 21.970910 time: 2.8 s\n",
      "[5 1600] loss: 22.901143 time: 2.8 s\n",
      "[5 1650] loss: 21.198703 time: 2.7 s\n",
      "[5 1700] loss: 21.478093 time: 2.8 s\n",
      "[5 1750] loss: 21.063627 time: 2.8 s\n",
      "[5 1800] loss: 20.319092 time: 2.7 s\n",
      "[5 1850] loss: 20.476291 time: 2.8 s\n",
      "[5 1900] loss: 21.143137 time: 2.8 s\n",
      "[5 1950] loss: 20.494132 time: 2.7 s\n",
      "[5 2000] loss: 22.716736 time: 2.8 s\n",
      "[5 2050] loss: 23.424916 time: 2.7 s\n",
      "[5 2100] loss: 22.840930 time: 2.7 s\n",
      "[5 2150] loss: 22.521732 time: 2.8 s\n",
      "[5 2200] loss: 22.269710 time: 2.8 s\n",
      "[5 2250] loss: 21.318499 time: 2.8 s\n",
      "[5 2300] loss: 18.348445 time: 2.8 s\n",
      "[5 2350] loss: 20.726771 time: 2.8 s\n",
      "[5 2400] loss: 21.366024 time: 2.8 s\n",
      "[5 2450] loss: 22.509527 time: 2.7 s\n",
      "[5 2500] loss: 22.577117 time: 2.8 s\n",
      "[5 2550] loss: 24.796264 time: 2.8 s\n",
      "[5 2600] loss: 28.410819 time: 2.7 s\n",
      "[5 2650] loss: 24.156720 time: 2.8 s\n",
      "[5 2700] loss: 21.707194 time: 2.8 s\n",
      "[5 2750] loss: 21.163855 time: 2.8 s\n",
      "[5 2800] loss: 22.090878 time: 2.8 s\n",
      "[5 2850] loss: 21.098063 time: 2.8 s\n",
      "[5 2900] loss: 19.727022 time: 2.8 s\n",
      "[5 2950] loss: 22.911590 time: 2.8 s\n",
      "[5 3000] loss: 22.200469 time: 2.8 s\n",
      "[5 3050] loss: 22.131477 time: 2.8 s\n",
      "[5 3100] loss: 21.125272 time: 2.8 s\n",
      "[5 3150] loss: 21.335043 time: 2.8 s\n",
      "[5 3200] loss: 24.098703 time: 2.8 s\n",
      "[5 3250] loss: 24.411765 time: 2.8 s\n",
      "[5 3300] loss: 25.388524 time: 2.8 s\n",
      "[5 3350] loss: 25.541444 time: 2.8 s\n",
      "[5 3400] loss: 24.461534 time: 2.8 s\n",
      "[5 3450] loss: 23.454198 time: 2.7 s\n",
      "[5 3500] loss: 25.508927 time: 2.7 s\n",
      "[5 3550] loss: 23.626417 time: 2.8 s\n",
      "[5 3600] loss: 22.935311 time: 2.8 s\n",
      "[5 3650] loss: 26.947971 time: 2.8 s\n",
      "[5 3700] loss: 21.467368 time: 2.7 s\n",
      "[5 3750] loss: 20.645184 time: 2.8 s\n",
      "[5 3800] loss: 21.593129 time: 2.8 s\n",
      "[5 3850] loss: 24.134728 time: 2.8 s\n",
      "[5 3900] loss: 24.311974 time: 2.8 s\n",
      "[5 3950] loss: 23.880269 time: 2.8 s\n",
      "[5 4000] loss: 23.252297 time: 2.8 s\n",
      "[5 4050] loss: 21.531693 time: 2.7 s\n",
      "[5 4100] loss: 24.347634 time: 2.8 s\n",
      "[5 4150] loss: 24.674484 time: 2.7 s\n",
      "[5 4200] loss: 22.189428 time: 2.8 s\n",
      "[5 4250] loss: 25.360941 time: 2.8 s\n",
      "[5 4300] loss: 25.882856 time: 2.7 s\n",
      "[5 4350] loss: 22.384401 time: 2.8 s\n",
      "[5 4400] loss: 24.428241 time: 2.7 s\n",
      "[5 4450] loss: 27.503605 time: 2.8 s\n",
      "[5 4500] loss: 21.608328 time: 2.7 s\n",
      "[5 4550] loss: 22.192707 time: 2.7 s\n",
      "[5 4600] loss: 26.364944 time: 2.7 s\n",
      "[5 4650] loss: 26.805333 time: 2.8 s\n",
      "[5 4700] loss: 25.055370 time: 2.8 s\n",
      "[5 4750] loss: 23.291581 time: 2.8 s\n",
      "[5 4800] loss: 20.976861 time: 2.8 s\n",
      "[5 4850] loss: 21.430433 time: 2.7 s\n",
      "[5 4900] loss: 27.640688 time: 2.8 s\n",
      "[5 4950] loss: 25.667029 time: 2.8 s\n",
      "[5 5000] loss: 20.726816 time: 2.8 s\n",
      "[5 5050] loss: 26.576972 time: 2.8 s\n",
      "[5 5100] loss: 26.641886 time: 2.7 s\n",
      "[5 5150] loss: 23.055985 time: 2.8 s\n",
      "[5 5200] loss: 25.446159 time: 2.7 s\n",
      "[5 5250] loss: 26.345264 time: 2.8 s\n",
      "[5 5300] loss: 26.198047 time: 2.8 s\n",
      "[5 5350] loss: 24.555525 time: 2.8 s\n",
      "[5 5400] loss: 24.272040 time: 2.8 s\n",
      "[5 5450] loss: 23.765151 time: 2.8 s\n",
      "[5 5500] loss: 23.676623 time: 2.8 s\n",
      "[5 5550] loss: 24.023544 time: 2.7 s\n",
      "[5 5600] loss: 24.361090 time: 2.9 s\n",
      "[5 5650] loss: 22.671747 time: 2.7 s\n",
      "[5 5700] loss: 22.935374 time: 2.8 s\n",
      "[5 5750] loss: 22.218214 time: 2.8 s\n",
      "[5 5800] loss: 24.208509 time: 2.7 s\n",
      "[5 5850] loss: 23.699912 time: 2.7 s\n",
      "[5 5900] loss: 22.922087 time: 2.8 s\n",
      "[5 5950] loss: 23.021080 time: 2.7 s\n",
      "[5 6000] loss: 24.456563 time: 2.8 s\n",
      "[5 6050] loss: 23.517613 time: 2.7 s\n",
      "[5 6100] loss: 23.559830 time: 2.7 s\n",
      "[5 6150] loss: 23.110070 time: 2.8 s\n",
      "[5 6200] loss: 21.524766 time: 2.8 s\n",
      "[5 6250] loss: 21.262553 time: 2.8 s\n",
      "[5 6300] loss: 22.843336 time: 2.7 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 6350] loss: 27.974790 time: 2.8 s\n",
      "Evaluating....\n",
      "Valid begins with 221 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 5]. loss: 36.687643 time: 355.7 s\n",
      "************************************************************\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and i think that ' s the key thing to do with the kids and the kids are in school and </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  well i ' m a little older than mine </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and i think that the jury may have been a little bit more difficult to do with the </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and i ' m not sure that it ' s a lot of fun but i don ' t think we ' re gonna do it </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  and it ' s a lot of fun </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  you know we ' re not gonna have to go to the area of the country and we ' re not gonna be able to do it </s>\n",
      "true response:  but they </s>\n",
      "generate response:  oh </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  well i ' ve been in the south of texas and i ' ve been in texas for a long time </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.482655, BLEU2 0.384982, BLEU3 0.318910, BLEU4 0.254138, inter_dist1 0.012581, inter_dist2 0.044352 avg_len 10.136471\n",
      " time: 320.1 s\n",
      "Done testing\n",
      "Epoch:  6\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[6 50] loss: 17.774139 time: 2.7 s\n",
      "[6 100] loss: 16.958499 time: 2.7 s\n",
      "[6 150] loss: 17.517983 time: 2.8 s\n",
      "[6 200] loss: 17.750148 time: 2.7 s\n",
      "[6 250] loss: 20.217731 time: 2.7 s\n",
      "[6 300] loss: 18.700189 time: 2.8 s\n",
      "[6 350] loss: 16.453624 time: 2.8 s\n",
      "[6 400] loss: 19.177879 time: 2.8 s\n",
      "[6 450] loss: 21.474549 time: 2.7 s\n",
      "[6 500] loss: 21.403832 time: 2.8 s\n",
      "[6 550] loss: 20.670245 time: 2.8 s\n",
      "[6 600] loss: 20.942322 time: 2.7 s\n",
      "[6 650] loss: 20.814865 time: 2.8 s\n",
      "[6 700] loss: 19.673374 time: 2.8 s\n",
      "[6 750] loss: 19.858355 time: 2.8 s\n",
      "[6 800] loss: 23.439975 time: 2.7 s\n",
      "[6 850] loss: 21.714012 time: 2.7 s\n",
      "[6 900] loss: 18.827962 time: 2.7 s\n",
      "[6 950] loss: 17.373728 time: 2.8 s\n",
      "[6 1000] loss: 17.937760 time: 2.7 s\n",
      "[6 1050] loss: 18.076476 time: 2.8 s\n",
      "[6 1100] loss: 19.032588 time: 2.8 s\n",
      "[6 1150] loss: 18.107000 time: 2.8 s\n",
      "[6 1200] loss: 18.628376 time: 2.8 s\n",
      "[6 1250] loss: 22.720128 time: 2.8 s\n",
      "[6 1300] loss: 22.025829 time: 2.8 s\n",
      "[6 1350] loss: 20.956204 time: 2.8 s\n",
      "[6 1400] loss: 20.623976 time: 2.8 s\n",
      "[6 1450] loss: 20.547537 time: 2.7 s\n",
      "[6 1500] loss: 18.953934 time: 2.8 s\n",
      "[6 1550] loss: 20.874933 time: 2.7 s\n",
      "[6 1600] loss: 20.010679 time: 2.8 s\n",
      "[6 1650] loss: 18.329814 time: 2.8 s\n",
      "[6 1700] loss: 20.397934 time: 2.7 s\n",
      "[6 1750] loss: 21.410080 time: 2.8 s\n",
      "[6 1800] loss: 19.256942 time: 2.8 s\n",
      "[6 1850] loss: 21.468963 time: 2.8 s\n",
      "[6 1900] loss: 23.027611 time: 2.7 s\n",
      "[6 1950] loss: 23.132166 time: 2.8 s\n",
      "[6 2000] loss: 21.493892 time: 2.7 s\n",
      "[6 2050] loss: 21.399379 time: 2.8 s\n",
      "[6 2100] loss: 20.851499 time: 2.8 s\n",
      "[6 2150] loss: 21.196330 time: 2.8 s\n",
      "[6 2200] loss: 22.463643 time: 2.8 s\n",
      "[6 2250] loss: 20.615960 time: 2.8 s\n",
      "[6 2300] loss: 19.111450 time: 2.7 s\n",
      "[6 2350] loss: 18.479010 time: 2.8 s\n",
      "[6 2400] loss: 18.435879 time: 2.7 s\n",
      "[6 2450] loss: 21.075865 time: 2.8 s\n",
      "[6 2500] loss: 20.997753 time: 2.8 s\n",
      "[6 2550] loss: 21.732976 time: 2.8 s\n",
      "[6 2600] loss: 21.259168 time: 2.7 s\n",
      "[6 2650] loss: 20.630070 time: 2.8 s\n",
      "[6 2700] loss: 22.301295 time: 2.8 s\n",
      "[6 2750] loss: 25.871017 time: 2.8 s\n",
      "[6 2800] loss: 24.394105 time: 2.8 s\n",
      "[6 2850] loss: 21.481243 time: 2.8 s\n",
      "[6 2900] loss: 22.463840 time: 2.8 s\n",
      "[6 2950] loss: 22.564685 time: 2.7 s\n",
      "[6 3000] loss: 22.943144 time: 2.8 s\n",
      "[6 3050] loss: 23.141364 time: 2.7 s\n",
      "[6 3100] loss: 21.278736 time: 2.8 s\n",
      "[6 3150] loss: 21.906689 time: 2.8 s\n",
      "[6 3200] loss: 23.048762 time: 2.8 s\n",
      "[6 3250] loss: 23.506932 time: 2.8 s\n",
      "[6 3300] loss: 26.348016 time: 2.8 s\n",
      "[6 3350] loss: 22.286602 time: 2.8 s\n",
      "[6 3400] loss: 22.236414 time: 2.8 s\n",
      "[6 3450] loss: 19.771718 time: 2.8 s\n",
      "[6 3500] loss: 19.395537 time: 2.8 s\n",
      "[6 3550] loss: 26.044106 time: 2.7 s\n",
      "[6 3600] loss: 19.604265 time: 2.8 s\n",
      "[6 3650] loss: 21.009207 time: 2.7 s\n",
      "[6 3700] loss: 21.368638 time: 2.8 s\n",
      "[6 3750] loss: 22.326385 time: 2.8 s\n",
      "[6 3800] loss: 23.570874 time: 2.8 s\n",
      "[6 3850] loss: 20.434686 time: 2.8 s\n",
      "[6 3900] loss: 20.607260 time: 2.8 s\n",
      "[6 3950] loss: 21.445915 time: 2.8 s\n",
      "[6 4000] loss: 20.952540 time: 2.8 s\n",
      "[6 4050] loss: 19.686936 time: 2.8 s\n",
      "[6 4100] loss: 20.224924 time: 2.8 s\n",
      "[6 4150] loss: 20.894921 time: 2.8 s\n",
      "[6 4200] loss: 21.927105 time: 2.7 s\n",
      "[6 4250] loss: 20.036558 time: 2.7 s\n",
      "[6 4300] loss: 19.208691 time: 2.8 s\n",
      "[6 4350] loss: 22.249926 time: 2.8 s\n",
      "[6 4400] loss: 21.948039 time: 2.8 s\n",
      "[6 4450] loss: 23.221688 time: 2.7 s\n",
      "[6 4500] loss: 24.063254 time: 2.8 s\n",
      "[6 4550] loss: 20.574397 time: 2.8 s\n",
      "[6 4600] loss: 20.047909 time: 2.8 s\n",
      "[6 4650] loss: 20.856715 time: 2.8 s\n",
      "[6 4700] loss: 21.446225 time: 2.8 s\n",
      "[6 4750] loss: 22.122859 time: 2.7 s\n",
      "[6 4800] loss: 23.833840 time: 2.8 s\n",
      "[6 4850] loss: 21.694800 time: 2.8 s\n",
      "[6 4900] loss: 21.339788 time: 2.8 s\n",
      "[6 4950] loss: 20.044443 time: 2.7 s\n",
      "[6 5000] loss: 20.123899 time: 2.8 s\n",
      "[6 5050] loss: 21.271117 time: 2.8 s\n",
      "[6 5100] loss: 19.078973 time: 2.8 s\n",
      "[6 5150] loss: 20.694915 time: 2.8 s\n",
      "[6 5200] loss: 20.260837 time: 2.8 s\n",
      "[6 5250] loss: 21.743673 time: 2.8 s\n",
      "[6 5300] loss: 22.256353 time: 2.8 s\n",
      "[6 5350] loss: 22.562192 time: 2.9 s\n",
      "[6 5400] loss: 23.937253 time: 2.8 s\n",
      "[6 5450] loss: 24.283067 time: 2.8 s\n",
      "[6 5500] loss: 22.641506 time: 2.8 s\n",
      "[6 5550] loss: 20.737903 time: 2.8 s\n",
      "[6 5600] loss: 18.738982 time: 2.8 s\n",
      "[6 5650] loss: 21.288223 time: 2.8 s\n",
      "[6 5700] loss: 21.751745 time: 2.8 s\n",
      "[6 5750] loss: 21.572797 time: 2.8 s\n",
      "[6 5800] loss: 23.126657 time: 2.8 s\n",
      "[6 5850] loss: 22.303578 time: 2.8 s\n",
      "[6 5900] loss: 20.951495 time: 2.8 s\n",
      "[6 5950] loss: 20.072712 time: 2.7 s\n",
      "[6 6000] loss: 20.813632 time: 2.8 s\n",
      "[6 6050] loss: 22.409915 time: 2.8 s\n",
      "[6 6100] loss: 22.175381 time: 2.8 s\n",
      "[6 6150] loss: 22.677262 time: 2.8 s\n",
      "[6 6200] loss: 22.929058 time: 2.8 s\n",
      "[6 6250] loss: 21.237163 time: 2.7 s\n",
      "[6 6300] loss: 20.264070 time: 2.8 s\n",
      "[6 6350] loss: 18.997464 time: 2.8 s\n",
      "Evaluating....\n",
      "Valid begins with 221 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 6]. loss: 38.954675 time: 356.5 s\n",
      "************************************************************\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and he ' s a he ' s a he ' s a big barry <unk> and he ' s got a lot of attention to the kids </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  well i ' m a little bit too </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and i think that ' s the way it is </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  something like that </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  i ' m not going to be a <unk> </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  we ' re not really into the old age but i ' m not sure that it ' s not the same as it is </s>\n",
      "true response:  but they </s>\n",
      "generate response:  oh i know </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  i ' m not sure that it ' s not that much of a choice but </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.447554, BLEU2 0.350079, BLEU3 0.281872, BLEU4 0.219774, inter_dist1 0.013170, inter_dist2 0.044362 avg_len 9.143222\n",
      " time: 243.5 s\n",
      "Done testing\n",
      "Epoch:  7\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[7 50] loss: 18.765230 time: 2.0 s\n",
      "[7 100] loss: 18.346266 time: 1.9 s\n",
      "[7 150] loss: 16.941255 time: 1.9 s\n",
      "[7 200] loss: 15.852223 time: 2.0 s\n",
      "[7 250] loss: 14.684546 time: 2.0 s\n",
      "[7 300] loss: 15.852279 time: 1.9 s\n",
      "[7 350] loss: 19.306086 time: 2.0 s\n",
      "[7 400] loss: 18.373401 time: 1.9 s\n",
      "[7 450] loss: 17.396336 time: 2.0 s\n",
      "[7 500] loss: 16.406814 time: 1.9 s\n",
      "[7 550] loss: 15.261255 time: 1.9 s\n",
      "[7 600] loss: 17.228179 time: 2.0 s\n",
      "[7 650] loss: 20.479521 time: 2.0 s\n",
      "[7 700] loss: 21.456930 time: 2.0 s\n",
      "[7 750] loss: 20.009793 time: 2.0 s\n",
      "[7 800] loss: 19.292825 time: 1.9 s\n",
      "[7 850] loss: 17.511127 time: 2.0 s\n",
      "[7 900] loss: 15.918182 time: 2.0 s\n",
      "[7 950] loss: 15.609322 time: 1.9 s\n",
      "[7 1000] loss: 16.982460 time: 2.0 s\n",
      "[7 1050] loss: 17.690570 time: 2.0 s\n",
      "[7 1100] loss: 15.908559 time: 1.9 s\n",
      "[7 1150] loss: 20.297678 time: 2.0 s\n",
      "[7 1200] loss: 20.005115 time: 1.9 s\n",
      "[7 1250] loss: 19.408095 time: 1.9 s\n",
      "[7 1300] loss: 17.929369 time: 1.9 s\n",
      "[7 1350] loss: 17.741465 time: 2.0 s\n",
      "[7 1400] loss: 16.434724 time: 2.0 s\n",
      "[7 1450] loss: 17.589500 time: 2.0 s\n",
      "[7 1500] loss: 15.879924 time: 2.0 s\n",
      "[7 1550] loss: 18.230308 time: 2.0 s\n",
      "[7 1600] loss: 18.584100 time: 1.9 s\n",
      "[7 1650] loss: 19.557249 time: 1.9 s\n",
      "[7 1700] loss: 18.902726 time: 1.9 s\n",
      "[7 1750] loss: 18.204676 time: 2.0 s\n",
      "[7 1800] loss: 16.778163 time: 1.9 s\n",
      "[7 1850] loss: 17.038431 time: 2.0 s\n",
      "[7 1900] loss: 18.442797 time: 2.0 s\n",
      "[7 1950] loss: 17.250899 time: 2.0 s\n",
      "[7 2000] loss: 18.670339 time: 2.0 s\n",
      "[7 2050] loss: 18.683971 time: 2.0 s\n",
      "[7 2100] loss: 18.526275 time: 1.9 s\n",
      "[7 2150] loss: 18.995494 time: 2.0 s\n",
      "[7 2200] loss: 17.810210 time: 2.0 s\n",
      "[7 2250] loss: 18.180982 time: 2.0 s\n",
      "[7 2300] loss: 19.018452 time: 2.0 s\n",
      "[7 2350] loss: 19.508034 time: 1.9 s\n",
      "[7 2400] loss: 22.189232 time: 2.0 s\n",
      "[7 2450] loss: 20.252089 time: 2.0 s\n",
      "[7 2500] loss: 18.819084 time: 1.9 s\n",
      "[7 2550] loss: 17.336343 time: 2.0 s\n",
      "[7 2600] loss: 17.847214 time: 2.0 s\n",
      "[7 2650] loss: 20.397079 time: 2.0 s\n",
      "[7 2700] loss: 21.622789 time: 1.9 s\n",
      "[7 2750] loss: 21.993870 time: 2.0 s\n",
      "[7 2800] loss: 18.904699 time: 2.0 s\n",
      "[7 2850] loss: 21.732567 time: 1.9 s\n",
      "[7 2900] loss: 22.287678 time: 2.0 s\n",
      "[7 2950] loss: 22.849521 time: 1.9 s\n",
      "[7 3000] loss: 19.661050 time: 2.0 s\n",
      "[7 3050] loss: 20.142693 time: 2.0 s\n",
      "[7 3100] loss: 19.139051 time: 2.0 s\n",
      "[7 3150] loss: 16.929319 time: 2.0 s\n",
      "[7 3200] loss: 18.175684 time: 2.0 s\n",
      "[7 3250] loss: 19.798715 time: 2.0 s\n",
      "[7 3300] loss: 19.147271 time: 1.9 s\n",
      "[7 3350] loss: 19.526789 time: 2.0 s\n",
      "[7 3400] loss: 19.393100 time: 2.0 s\n",
      "[7 3450] loss: 19.759882 time: 2.0 s\n",
      "[7 3500] loss: 18.838777 time: 2.0 s\n",
      "[7 3550] loss: 19.380120 time: 2.0 s\n",
      "[7 3600] loss: 20.850264 time: 2.0 s\n",
      "[7 3650] loss: 20.457249 time: 2.0 s\n",
      "[7 3700] loss: 22.086963 time: 1.9 s\n",
      "[7 3750] loss: 23.457655 time: 1.9 s\n",
      "[7 3800] loss: 19.962668 time: 2.0 s\n",
      "[7 3850] loss: 19.823326 time: 2.0 s\n",
      "[7 3900] loss: 21.900600 time: 2.0 s\n",
      "[7 3950] loss: 20.517884 time: 2.0 s\n",
      "[7 4000] loss: 24.345172 time: 2.0 s\n",
      "[7 4050] loss: 20.863286 time: 2.0 s\n",
      "[7 4100] loss: 21.778754 time: 2.0 s\n",
      "[7 4150] loss: 23.751817 time: 2.0 s\n",
      "[7 4200] loss: 21.829096 time: 2.0 s\n",
      "[7 4250] loss: 20.747390 time: 2.0 s\n",
      "[7 4300] loss: 20.241014 time: 2.0 s\n",
      "[7 4350] loss: 26.424003 time: 2.0 s\n",
      "[7 4400] loss: 22.765038 time: 2.0 s\n",
      "[7 4450] loss: 23.202087 time: 2.0 s\n",
      "[7 4500] loss: 22.573272 time: 2.0 s\n",
      "[7 4550] loss: 20.800529 time: 2.0 s\n",
      "[7 4600] loss: 18.477748 time: 2.0 s\n",
      "[7 4650] loss: 18.579534 time: 1.9 s\n",
      "[7 4700] loss: 18.661247 time: 2.0 s\n",
      "[7 4750] loss: 18.045543 time: 2.0 s\n",
      "[7 4800] loss: 18.985795 time: 2.0 s\n",
      "[7 4850] loss: 20.159317 time: 2.0 s\n",
      "[7 4900] loss: 22.942978 time: 2.0 s\n",
      "[7 4950] loss: 22.264121 time: 2.0 s\n",
      "[7 5000] loss: 22.163114 time: 2.0 s\n",
      "[7 5050] loss: 20.721134 time: 2.0 s\n",
      "[7 5100] loss: 21.361329 time: 2.0 s\n",
      "[7 5150] loss: 21.002895 time: 2.0 s\n",
      "[7 5200] loss: 19.375072 time: 2.0 s\n",
      "[7 5250] loss: 20.517286 time: 1.9 s\n",
      "[7 5300] loss: 21.982891 time: 2.0 s\n",
      "[7 5350] loss: 18.685253 time: 2.0 s\n",
      "[7 5400] loss: 21.128726 time: 2.0 s\n",
      "[7 5450] loss: 22.187644 time: 2.0 s\n",
      "[7 5500] loss: 21.619242 time: 2.0 s\n",
      "[7 5550] loss: 21.225114 time: 2.0 s\n",
      "[7 5600] loss: 17.900161 time: 1.9 s\n",
      "[7 5650] loss: 18.363507 time: 2.0 s\n",
      "[7 5700] loss: 18.349872 time: 2.0 s\n",
      "[7 5750] loss: 21.439398 time: 2.0 s\n",
      "[7 5800] loss: 21.840022 time: 2.0 s\n",
      "[7 5850] loss: 22.079716 time: 2.0 s\n",
      "[7 5900] loss: 24.026668 time: 2.0 s\n",
      "[7 5950] loss: 21.094820 time: 2.0 s\n",
      "[7 6000] loss: 18.966319 time: 2.0 s\n",
      "[7 6050] loss: 17.897224 time: 2.0 s\n",
      "[7 6100] loss: 19.957621 time: 2.0 s\n",
      "[7 6150] loss: 18.865831 time: 2.0 s\n",
      "[7 6200] loss: 17.816556 time: 2.0 s\n",
      "[7 6250] loss: 19.908413 time: 2.0 s\n",
      "[7 6300] loss: 20.386722 time: 2.0 s\n",
      "[7 6350] loss: 21.206459 time: 2.0 s\n",
      "Evaluating....\n",
      "Valid begins with 221 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 7]. loss: 39.626095 time: 253.4 s\n",
      "************************************************************\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and i think that ' s a neat idea </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  well we have a cat and we have a cat and we have a cat and we have a cat and we have a cat and we have a cat\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and i think that ' s the way it is to me </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  oh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  and i just i just don ' t know how to do it </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  we have a lot of kids in school and we have a lot of kids and we have a lot of kids and we have a lot of kids </s>\n",
      "true response:  but they </s>\n",
      "generate response:  oh </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah </s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  um - hum </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.450906, BLEU2 0.355227, BLEU3 0.289403, BLEU4 0.227789, inter_dist1 0.017134, inter_dist2 0.061858 avg_len 8.934136\n",
      " time: 163.6 s\n",
      "Done testing\n",
      "Epoch:  8\n",
      "Train begins with 6398 batches with 12 left over samples\n",
      "[8 50] loss: 16.530627 time: 1.9 s\n",
      "[8 100] loss: 15.899976 time: 1.9 s\n",
      "[8 150] loss: 16.644875 time: 1.9 s\n",
      "[8 200] loss: 17.350121 time: 1.9 s\n",
      "[8 250] loss: 16.172146 time: 2.0 s\n",
      "[8 300] loss: 16.353236 time: 2.0 s\n",
      "[8 350] loss: 15.261588 time: 2.0 s\n",
      "[8 400] loss: 14.569209 time: 2.0 s\n",
      "[8 450] loss: 14.957734 time: 1.9 s\n",
      "[8 500] loss: 14.550823 time: 1.9 s\n",
      "[8 550] loss: 17.860043 time: 2.0 s\n",
      "[8 600] loss: 17.723930 time: 2.0 s\n",
      "[8 650] loss: 16.618515 time: 2.0 s\n",
      "[8 700] loss: 16.353326 time: 1.9 s\n",
      "[8 750] loss: 17.019297 time: 1.9 s\n",
      "[8 800] loss: 18.400938 time: 1.9 s\n",
      "[8 850] loss: 18.657190 time: 1.9 s\n",
      "[8 900] loss: 20.487953 time: 1.9 s\n",
      "[8 950] loss: 17.495048 time: 1.9 s\n",
      "[8 1000] loss: 20.346311 time: 1.9 s\n",
      "[8 1050] loss: 17.108858 time: 2.0 s\n",
      "[8 1100] loss: 15.063936 time: 2.0 s\n",
      "[8 1150] loss: 13.595588 time: 1.9 s\n",
      "[8 1200] loss: 13.207581 time: 2.0 s\n",
      "[8 1250] loss: 14.238384 time: 1.9 s\n",
      "[8 1300] loss: 14.306139 time: 1.9 s\n",
      "[8 1350] loss: 13.220169 time: 2.0 s\n",
      "[8 1400] loss: 18.548498 time: 1.9 s\n",
      "[8 1450] loss: 18.716108 time: 1.9 s\n",
      "[8 1500] loss: 18.656601 time: 1.9 s\n",
      "[8 1550] loss: 16.866070 time: 2.0 s\n",
      "[8 1600] loss: 19.300932 time: 2.0 s\n",
      "[8 1650] loss: 20.951917 time: 1.9 s\n",
      "[8 1700] loss: 18.688523 time: 2.0 s\n",
      "[8 1750] loss: 18.416637 time: 1.9 s\n",
      "[8 1800] loss: 17.748541 time: 1.9 s\n",
      "[8 1850] loss: 17.878017 time: 2.0 s\n",
      "[8 1900] loss: 18.324159 time: 2.0 s\n",
      "[8 1950] loss: 18.849273 time: 2.0 s\n",
      "[8 2000] loss: 18.903995 time: 2.0 s\n",
      "[8 2050] loss: 19.787546 time: 2.0 s\n",
      "[8 2100] loss: 20.640998 time: 1.9 s\n",
      "[8 2150] loss: 19.498562 time: 1.9 s\n",
      "[8 2200] loss: 17.627486 time: 1.9 s\n",
      "[8 2250] loss: 15.939998 time: 1.9 s\n",
      "[8 2300] loss: 17.712490 time: 2.0 s\n",
      "[8 2350] loss: 21.927527 time: 2.0 s\n",
      "[8 2400] loss: 22.710455 time: 2.0 s\n",
      "[8 2450] loss: 17.940981 time: 2.0 s\n",
      "[8 2500] loss: 18.152514 time: 2.0 s\n",
      "[8 2550] loss: 17.727607 time: 2.0 s\n",
      "[8 2600] loss: 17.024309 time: 2.0 s\n",
      "[8 2650] loss: 15.245526 time: 2.0 s\n",
      "[8 2700] loss: 16.703399 time: 2.0 s\n",
      "[8 2750] loss: 18.909536 time: 2.0 s\n",
      "[8 2800] loss: 18.986999 time: 2.0 s\n",
      "[8 2850] loss: 19.039219 time: 2.1 s\n",
      "[8 2900] loss: 19.232069 time: 2.0 s\n",
      "[8 2950] loss: 15.966512 time: 2.0 s\n",
      "[8 3000] loss: 16.787082 time: 2.0 s\n",
      "[8 3050] loss: 15.445821 time: 2.0 s\n",
      "[8 3100] loss: 17.579984 time: 2.0 s\n",
      "[8 3150] loss: 18.073731 time: 2.0 s\n",
      "[8 3200] loss: 18.051259 time: 2.0 s\n",
      "[8 3250] loss: 19.270413 time: 2.0 s\n",
      "[8 3300] loss: 19.733927 time: 2.0 s\n",
      "[8 3350] loss: 18.489063 time: 2.0 s\n",
      "[8 3400] loss: 18.367200 time: 2.0 s\n",
      "[8 3450] loss: 17.260376 time: 2.0 s\n",
      "[8 3500] loss: 18.067012 time: 2.0 s\n",
      "[8 3550] loss: 18.700450 time: 1.9 s\n",
      "[8 3600] loss: 20.574346 time: 2.0 s\n",
      "[8 3650] loss: 20.799918 time: 2.0 s\n",
      "[8 3700] loss: 21.828537 time: 2.0 s\n",
      "[8 3750] loss: 21.067375 time: 2.0 s\n",
      "[8 3800] loss: 19.597398 time: 2.0 s\n",
      "[8 3850] loss: 18.854797 time: 2.0 s\n",
      "[8 3900] loss: 20.054952 time: 2.0 s\n",
      "[8 3950] loss: 18.634885 time: 2.0 s\n",
      "[8 4000] loss: 17.574376 time: 2.0 s\n",
      "[8 4050] loss: 18.289584 time: 2.0 s\n",
      "[8 4100] loss: 19.330115 time: 2.0 s\n",
      "[8 4150] loss: 19.697256 time: 2.0 s\n",
      "[8 4200] loss: 19.730113 time: 2.0 s\n",
      "[8 4250] loss: 18.411713 time: 2.0 s\n",
      "[8 4300] loss: 21.270982 time: 2.0 s\n",
      "[8 4350] loss: 19.711309 time: 2.0 s\n",
      "[8 4400] loss: 18.946292 time: 2.0 s\n",
      "[8 4450] loss: 17.700138 time: 2.0 s\n",
      "[8 4500] loss: 17.348772 time: 2.0 s\n",
      "[8 4550] loss: 15.040986 time: 1.9 s\n",
      "[8 4600] loss: 18.782261 time: 2.0 s\n",
      "[8 4650] loss: 19.537327 time: 1.9 s\n",
      "[8 4700] loss: 23.233081 time: 1.9 s\n",
      "[8 4750] loss: 21.470113 time: 1.9 s\n",
      "[8 4800] loss: 18.423004 time: 1.9 s\n",
      "[8 4850] loss: 18.523403 time: 2.0 s\n",
      "[8 4900] loss: 17.633261 time: 2.0 s\n",
      "[8 4950] loss: 18.986830 time: 2.0 s\n",
      "[8 5000] loss: 18.480899 time: 2.0 s\n",
      "[8 5050] loss: 18.719787 time: 2.0 s\n",
      "[8 5100] loss: 18.810251 time: 2.0 s\n",
      "[8 5150] loss: 20.659275 time: 2.0 s\n",
      "[8 5200] loss: 19.623047 time: 2.0 s\n",
      "[8 5250] loss: 19.612513 time: 2.0 s\n",
      "[8 5300] loss: 18.212726 time: 2.0 s\n",
      "[8 5350] loss: 18.411520 time: 2.0 s\n",
      "[8 5400] loss: 17.486629 time: 2.0 s\n",
      "[8 5450] loss: 20.227758 time: 2.1 s\n",
      "[8 5500] loss: 18.504576 time: 2.0 s\n",
      "[8 5550] loss: 18.763021 time: 2.0 s\n",
      "[8 5600] loss: 18.922828 time: 2.0 s\n",
      "[8 5650] loss: 19.293898 time: 2.0 s\n",
      "[8 5700] loss: 19.329816 time: 2.0 s\n",
      "[8 5750] loss: 21.709052 time: 2.0 s\n",
      "[8 5800] loss: 18.946124 time: 2.0 s\n",
      "[8 5850] loss: 19.072852 time: 2.0 s\n",
      "[8 5900] loss: 22.785811 time: 2.0 s\n",
      "[8 5950] loss: 20.886636 time: 2.0 s\n",
      "[8 6000] loss: 19.801530 time: 2.0 s\n",
      "[8 6050] loss: 19.980154 time: 2.0 s\n",
      "[8 6100] loss: 18.256564 time: 2.0 s\n",
      "[8 6150] loss: 20.667924 time: 2.0 s\n",
      "[8 6200] loss: 18.451729 time: 2.0 s\n",
      "[8 6250] loss: 18.659873 time: 2.0 s\n",
      "[8 6300] loss: 21.611950 time: 2.0 s\n",
      "[8 6350] loss: 21.646741 time: 2.0 s\n",
      "Evaluating....\n",
      "Valid begins with 221 batches with 0 left over samples\n",
      "************************************************************\n",
      "[epoch 8]. loss: 41.504991 time: 253.9 s\n",
      "************************************************************\n",
      "Test begins with 5481 batches with 0 left over samples\n",
      "true response:  and at this particular point you know taking appropriate course work but really nothing that ' s in out of the ordinary </s>\n",
      "generate response:  and i ' m not quite sure that that ' s a good thing to do with it but i ' m not sure that </s>\n",
      "true response:  well i have two cats </s>\n",
      "generate response:  well we have a dog and we ' re kind of a labrador puppy </s>\n",
      "true response:  yeah </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  <unk> of people ' s heart strings and making them cry and feel like they ' re doing somebody good by giving them </s>\n",
      "generate response:  and i think that ' s the way that ' s going to be the only way that i ' m sure </s>\n",
      "true response:  something like that </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah that ' s fun </s>\n",
      "generate response:  oh that ' s good </s>\n",
      "true response:  oh </s>\n",
      "generate response:  oh </s>\n",
      "true response:  lay on those beaches over there </s>\n",
      "generate response:  but i ' m not a big person i ' m not a big fan of that </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  uh - huh </s>\n",
      "true response:  they before we got her everybody said oh they ' re just like a cat you know </s>\n",
      "generate response:  yeah it ' s just it ' s just awful it ' s just awful it ' s just awful it ' s just </s>\n",
      "true response:  uh - huh </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  we sort of like our roots and like to get back and have family reunions and all that </s>\n",
      "generate response:  it ' s not a lot of the things that we ' re not allowed to do that but i think it ' s really sad </s>\n",
      "true response:  but they </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah well see that ' s the reason that we couldn ' t make really make them at first we were going to get a pick up truck </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  you ' d think i ' d have a lot time but </s>\n",
      "generate response:  you know </s>\n",
      "true response:  and </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  yeah i ' ve got a well my roses are on the west side of the house i asked my neighbors what they wanted to see outside their </s>\n",
      "generate response:  yeah </s>\n",
      "true response:  um - hum </s>\n",
      "generate response:  um - hum </s>\n",
      "BLEU1 0.442182, BLEU2 0.345311, BLEU3 0.277501, BLEU4 0.215971, inter_dist1 0.018007, inter_dist2 0.065754 avg_len 9.179894\n",
      " time: 163.7 s\n",
      "Done testing\n"
     ]
    }
   ],
   "source": [
    "metrics=Metrics(corpus.word2vec)\n",
    "model = HRED(config, n_tokens)\n",
    "if corpus.word2vec is not None:\n",
    "    print(\"Loaded word2vec\")\n",
    "    model.embedder.weight.data.copy_(torch.from_numpy(corpus.word2vec))\n",
    "    model.embedder.weight.data[0].fill_(0)\n",
    "model.to(DEVICE)\n",
    "parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(parameters, lr=config['lr'])\n",
    "\n",
    "model.zero_grad()\n",
    "print_every = 50\n",
    "best_state = None\n",
    "max_metric = 0\n",
    "for epoch in range(8):\n",
    "    print('Epoch: ', epoch+1)\n",
    "    train_loader.epoch_init(32, config['diaglen'], 1, shuffle=True)\n",
    "    n_iters=train_loader.num_batch\n",
    "    total_loss = 0.0\n",
    "    epoch_begin = time()\n",
    "    batch_count = 0\n",
    "    batch_begin_time = time()\n",
    "    total_train_batch = 0 # 记录训练的样本数量\n",
    "    total_valid_batch = 0 # 记录测试的样本数量\n",
    "    # 分别用来记录训练时候，生成器最顶层的梯度，最底层的梯度以及判别器最顶层的梯度\n",
    "    train_grad_G_top_layer = []\n",
    "    train_grad_G_bottom_layer = []\n",
    "    train_grad_D_top_layer = []\n",
    "    while True:\n",
    "        loss_records=[]\n",
    "        batch = train_loader.next_batch()\n",
    "        total_train_batch += 32\n",
    "        if batch is None:\n",
    "#         if batch is None or total_train_batch >= 1000: # end of epoch\n",
    "            break\n",
    "        context, context_lens, utt_lens, floors,_,_,_,response,res_lens,_ = batch\n",
    "        context, utt_lens = context[:,:,1:], utt_lens-1 # remove the sos token in the context and reduce the context length\n",
    "        context, context_lens, utt_lens, floors, response, res_lens\\\n",
    "                = gVar(context), gVar(context_lens), gVar(utt_lens), gData(floors), gVar(response), gVar(res_lens)\n",
    "        loss_batch = train(context, context_lens, utt_lens, floors, response, res_lens)\n",
    "        total_loss += float(loss_batch)\n",
    "        batch_count += 1\n",
    "        if batch_count % print_every == 0:\n",
    "            print_flush('[%d %d] loss: %.6f time: %.1f s' %\n",
    "                  (epoch + 1, batch_count, np.exp(total_loss / print_every), time() - batch_begin_time))\n",
    "            total_loss = 0.0\n",
    "            batch_begin_time = time()\n",
    "#         train_grad_G_top_layer.append(torch.mean(model.decoder.rnn.weight_hh_l0.grad))\n",
    "#         train_grad_G_bottom_layer.append(torch.mean(model.generator.embedding.weight.grad))\n",
    "#         train_grad_G_bottom_layer.append(torch.mean(model.decoder.rnn.weight_hh_l0.grad))\n",
    "#     plot_gradient(train_grad_G_top_layer, 'G top layer')\n",
    "#     plot_gradient(train_grad_G_bottom_layer, 'G bottom layer')\n",
    "    print_flush(\"Evaluating....\")\n",
    "    valid_loader.epoch_init(20, config['diaglen'], 1, shuffle=False)\n",
    "    loss_valid = valid_small(valid_loader)\n",
    "#     valid_result.append(F1)\n",
    "    print_flush('*'*60)\n",
    "    print_flush('[epoch %d]. loss: %.6f time: %.1f s'%(epoch+1, np.exp(loss_valid), time()-epoch_begin))\n",
    "    print_flush('*'*60)\n",
    "#     print_flush(\"testing....\")\n",
    "#     test_loader.epoch_init(1, config['diaglen'], 1, shuffle=False)\n",
    "#     loss_valid = valid(test_loader)\n",
    "#     print_flush('*'*60)\n",
    "#     print_flush('[epoch %d]. loss: %.6f time: %.1f s'%(epoch+1, np.exp(loss_valid), time()-epoch_begin))\n",
    "#     print_flush('*'*60)\n",
    "    if (epoch+1) > 4:\n",
    "        f_eval = open(\"../result/{}/{}/epoch{}.txt\".format('HRED', 'SWDA', epoch), \"w\")\n",
    "        recall_bleu, bow_extrema, bow_avg, bow_greedy, inter_dist1, inter_dist2, avg_len\\\n",
    "         =evaluate(model, metrics, test_loader, vocab, ivocab, f_eval, repeat=10)\n",
    "    epoch_begin = time()\n",
    "#     if F1 > max_metric:\n",
    "#         best_state = model.state_dict()\n",
    "#         max_metric = F1\n",
    "#         print_flush(\"save model...\")\n",
    "#         torch.save(best_state, '../datasets/models/baseline_LSTM.pth')\n",
    "#     epoch_begin = time()\n",
    "#     if training_termination(valid_result):\n",
    "#         print_flush(\"early stop at [%d] epoch!\" % (epoch+1))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (liangjiahui)",
   "language": "python",
   "name": "liangjiahui"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
